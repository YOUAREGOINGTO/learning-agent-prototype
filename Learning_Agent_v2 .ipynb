{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ZGxjx8JAOB",
        "outputId": "69cbb143-0db4-4fc8-a953-b984c9f49a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI Configured Successfully.\n",
            "Rate Limiter Initialized: Max 8 calls per 60 seconds.\n",
            "llm_call function defined. Default model: gemini-2.0-flash-thinking-exp-1219\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict, Callable,Optional\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Optional, Any,Tuple\n",
        "\n",
        "try:\n",
        "    API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"Google Generative AI Configured Successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: Secret 'GEMINI_API_KEY' not found.\")\n",
        "    print(\"Please add your Gemini API Key to Colab Secrets.\")\n",
        "    # Optionally, raise an error or exit if the key is critical\n",
        "    API_KEY = None # Set API_KEY to None to indicate failure\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during genai configuration: {e}\")\n",
        "    API_KEY = None\n",
        "\n",
        "DEFAULT_MODEL_NAME= \"gemini-2.0-flash-thinking-exp-1219\"\n",
        "\n",
        "\n",
        "DEFAULT_SAFETY_SETTINGS = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "\n",
        "# --- Rate Limiter Class ---\n",
        "class RateLimiter:\n",
        "    \"\"\"Handles rate limiting for API calls\"\"\"\n",
        "    def __init__(self, max_calls=8, time_period=60):\n",
        "        self.max_calls = max_calls\n",
        "        self.time_period = time_period\n",
        "        self.call_timestamps = []\n",
        "        self.total_calls = 0\n",
        "        print(f\"Rate Limiter Initialized: Max {self.max_calls} calls per {self.time_period} seconds.\")\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait if we're approaching rate limits\"\"\"\n",
        "        if not API_KEY:\n",
        "             raise ValueError(\"Cannot make API calls: Gemini API Key not configured.\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "\n",
        "\n",
        "        self.call_timestamps = [ts for ts in self.call_timestamps\n",
        "                               if current_time - ts < timedelta(seconds=self.time_period)]\n",
        "\n",
        "        # If we've reached max calls within the time period, wait\n",
        "        if len(self.call_timestamps) >= self.max_calls:\n",
        "            oldest_call = min(self.call_timestamps)\n",
        "            wait_time = (oldest_call + timedelta(seconds=self.time_period) - current_time).total_seconds()\n",
        "\n",
        "            if wait_time > 0:\n",
        "                print(f\"\\n[Rate Limiter]: Limit reached ({len(self.call_timestamps)}/{self.max_calls} calls in last {self.time_period}s). Waiting {wait_time:.1f} seconds...\")\n",
        "                time.sleep(wait_time + 0.5) # Add a small buffer\n",
        "\n",
        "            # Update timestamp list after waiting\n",
        "            current_time = datetime.now()\n",
        "            self.call_timestamps = [ts for ts in self.call_timestamps\n",
        "                                   if current_time - ts < timedelta(seconds=self.time_period)]\n",
        "\n",
        "        # Record this call\n",
        "        self.call_timestamps.append(datetime.now())\n",
        "        self.total_calls += 1\n",
        "\n",
        "        return self.total_calls\n",
        "\n",
        "# --- Instantiate the Rate Limiter (global for this script/cell)\n",
        "limiter = RateLimiter()\n",
        "\n",
        "\n",
        "def llm_call(\n",
        "    prompt: str,\n",
        "    chat_history: Optional[List[Dict[str, Any]]] = None,\n",
        "    system_prompt: str = \"\",\n",
        "    model: Optional[str] = None,\n",
        "    temperature: float = 0.1,\n",
        "    safety_settings: Optional[List[Dict]] = None\n",
        ") -> str:\n",
        "\n",
        "    global limiter # Use the global limiter instance\n",
        "    call_number = limiter.wait_if_needed() # Wait if necessary and get call number\n",
        "\n",
        "\n",
        "    model_to_use = model if model else DEFAULT_MODEL_NAME\n",
        "\n",
        "    effective_safety_settings = safety_settings if safety_settings else DEFAULT_SAFETY_SETTINGS\n",
        "\n",
        "    print(f\"\\n--- LLM Call #{call_number} ---\")\n",
        "    print(f\"Model: {model_to_use}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        model_instance = genai.GenerativeModel(\n",
        "            model_name=model_to_use,\n",
        "            system_instruction=system_prompt if system_prompt else None,\n",
        "            safety_settings=effective_safety_settings\n",
        "        )\n",
        "\n",
        "        # Configure generation\n",
        "        generation_config = genai.types.GenerationConfig(\n",
        "            temperature=temperature\n",
        "            # max_output_tokens=4096 # You can set max tokens if needed\n",
        "        )\n",
        "\n",
        "        # --- MODIFICATION START: Prepare input for generate_content ---\n",
        "        generation_input: Any # Type hint for clarity\n",
        "        if chat_history:\n",
        "\n",
        "            if not isinstance(chat_history, list) or not all(isinstance(item, dict) and 'role' in item and 'parts' in item for item in chat_history):\n",
        "                 # Raise an error or log a warning, returning an error message\n",
        "                 print(f\"ERROR (Call #{call_number}): Invalid chat_history format. Expected List[Dict[str, Any]] with 'role' and 'parts'.\")\n",
        "                 return \"[ERROR: Invalid chat_history format]\"\n",
        "\n",
        "\n",
        "            generation_input = chat_history + [{\"role\": \"user\", \"parts\": [prompt]}]\n",
        "\n",
        "            print(f\"Using chat history with {len(chat_history)} previous turns.\")\n",
        "        else:\n",
        "            # Single turn prompt (original behavior)\n",
        "            generation_input = prompt\n",
        "            # print(f\"User Prompt: {prompt[:100]}...\") # Optional: Print snippet\n",
        "            print(\"Using single prompt (no chat history).\")\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "        # Call the API with either the prompt string or the list of content dicts\n",
        "        response = model_instance.generate_content(\n",
        "            generation_input, # Pass the prepared input here\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        print(f\"--- Response Received (Call #{call_number}) ---\")\n",
        "        # --- Token Usage Snippet (Unchanged) ---\n",
        "        try:\n",
        "            # Check if usage_metadata exists and is not None\n",
        "            if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
        "                prompt_tokens = response.usage_metadata.prompt_token_count\n",
        "                completion_tokens = response.usage_metadata.candidates_token_count\n",
        "                total_tokens = response.usage_metadata.total_token_count\n",
        "                print(f\"Token Usage (Call #{call_number}): Prompt={prompt_tokens}, Completion={completion_tokens}, Total={total_tokens}\")\n",
        "            else:\n",
        "                print(f\"Token Usage (Call #{call_number}): Not available in response.\")\n",
        "        except AttributeError:\n",
        "             # This might catch cases where usage_metadata attribute doesn't exist at all\n",
        "             print(f\"Token Usage (Call #{call_number}): 'usage_metadata' attribute not found in response object.\")\n",
        "        # --- END OF SNIPPET ---\n",
        "\n",
        "        # --- Response Handling (Improved Robustness) ---\n",
        "        response_text = \"\"\n",
        "        block_reason = None\n",
        "        finish_reason = None\n",
        "\n",
        "        # 1. Check for explicit blocking via prompt_feedback\n",
        "        if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
        "             block_reason = response.prompt_feedback.block_reason.name # Get enum name\n",
        "             print(f\"WARN (Call #{call_number}): Call blocked by API based on prompt. Reason: {block_reason}\")\n",
        "\n",
        "             return f\"[BLOCKED DUE TO PROMPT: {block_reason}]\"\n",
        "\n",
        "        # 2. Check candidate content and finish reason\n",
        "        if hasattr(response, 'candidates') and response.candidates:\n",
        "            candidate = response.candidates[0] # Usually only one candidate for non-streaming\n",
        "\n",
        "            # Get finish reason\n",
        "            if hasattr(candidate, 'finish_reason'):\n",
        "                 finish_reason = candidate.finish_reason.name # Get enum name\n",
        "\n",
        "            # Check if generation stopped due to safety or other reasons\n",
        "            if finish_reason not in ['STOP', 'UNSPECIFIED', None]: # STOP is normal completion\n",
        "                print(f\"WARN (Call #{call_number}): Generation stopped prematurely. Reason: {finish_reason}\")\n",
        "                # Check safety ratings on the candidate if available\n",
        "                if hasattr(candidate, 'safety_ratings') and candidate.safety_ratings:\n",
        "                    safety_ratings_str = \", \".join([f\"{r.category.name}: {r.probability.name}\" for r in candidate.safety_ratings])\n",
        "                    print(f\"Candidate Safety Ratings: {safety_ratings_str}\")\n",
        "                    # Decide if this specific finish reason constitutes a block message\n",
        "                    if finish_reason == 'SAFETY':\n",
        "                         return f\"[BLOCKED CONTENT DUE TO: {finish_reason}. Ratings: {safety_ratings_str}]\"\n",
        "                    else:\n",
        "                         return f\"[GENERATION STOPPED: {finish_reason}. Ratings: {safety_ratings_str}]\"\n",
        "                else:\n",
        "                     return f\"[GENERATION STOPPED: {finish_reason}]\" # Return stop reason even without ratings\n",
        "\n",
        "            # 3. Try to extract text content if available\n",
        "            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts') and candidate.content.parts:\n",
        "                 # Assuming the first part contains the text response\n",
        "                 response_text = \"\".join(part.text for part in candidate.content.parts if hasattr(part, 'text'))\n",
        "                 # Fallback via response.text if parts extraction failed but .text exists\n",
        "                 if not response_text and hasattr(response, 'text'):\n",
        "                      response_text = response.text\n",
        "\n",
        "        # Fallback: If candidate/parts structure isn't as expected, try the top-level response.text\n",
        "        if not response_text and hasattr(response, 'text'):\n",
        "            response_text = response.text\n",
        "\n",
        "        # 4. Final check and return\n",
        "        if response_text:\n",
        "            return response_text\n",
        "        else:\n",
        "             # If we got here, no content was found and no explicit block/stop reason was returned above\n",
        "             print(f\"WARN (Call #{call_number}): Received no text content from API, and no explicit block/stop reason identified.\")\n",
        "             # print(\"Full response object:\", response) # Uncomment for debugging\n",
        "             return \"[EMPTY RESPONSE]\"\n",
        "        # --- END OF Response Handling ---\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR (Call #{call_number}) during Gemini API call: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for better debugging\n",
        "        return f\"[ERROR: {type(e).__name__} - {e}]\"\n",
        "\n",
        "\n",
        "print(f\"llm_call function defined. Default model: {DEFAULT_MODEL_NAME}\")\n",
        "def extract_xml(text: str, tag: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the content of the specified XML tag from the given text.\n",
        "    Made case-insensitive and strips whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the XML.\n",
        "        tag (str): The XML tag to extract content from.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the specified XML tag, or an empty string if not found.\n",
        "    \"\"\"\n",
        "    # Use re.IGNORECASE for case-insensitivity and re.DOTALL to match across newlines\n",
        "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL | re.IGNORECASE)\n",
        "    # Use .strip() to remove leading/trailing whitespace from the extracted content\n",
        "    return match.group(1).strip() if match else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d2rg1ayfuIaY",
        "outputId": "67b1d31b-1c48-42ee-a036-a64c99a4a2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Listing Available Gemini Models ---\n",
            "Found 45 models supporting 'generateContent':\n",
            "  1. Name: models/gemini-1.0-pro-vision-latest\n",
            "     Display Name: Gemini 1.0 Pro Vision\n",
            "     Input Tokens: 12288, Output Tokens: 4096\n",
            "  2. Name: models/gemini-pro-vision\n",
            "     Display Name: Gemini 1.0 Pro Vision\n",
            "     Input Tokens: 12288, Output Tokens: 4096\n",
            "  3. Name: models/gemini-1.5-pro-latest\n",
            "     Display Name: Gemini 1.5 Pro Latest\n",
            "     Input Tokens: 2000000, Output Tokens: 8192\n",
            "  4. Name: models/gemini-1.5-pro-001\n",
            "     Display Name: Gemini 1.5 Pro 001\n",
            "     Input Tokens: 2000000, Output Tokens: 8192\n",
            "  5. Name: models/gemini-1.5-pro-002\n",
            "     Display Name: Gemini 1.5 Pro 002\n",
            "     Input Tokens: 2000000, Output Tokens: 8192\n",
            "  6. Name: models/gemini-1.5-pro\n",
            "     Display Name: Gemini 1.5 Pro\n",
            "     Input Tokens: 2000000, Output Tokens: 8192\n",
            "  7. Name: models/gemini-1.5-flash-latest\n",
            "     Display Name: Gemini 1.5 Flash Latest\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  8. Name: models/gemini-1.5-flash-001\n",
            "     Display Name: Gemini 1.5 Flash 001\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  9. Name: models/gemini-1.5-flash-001-tuning\n",
            "     Display Name: Gemini 1.5 Flash 001 Tuning\n",
            "     Input Tokens: 16384, Output Tokens: 8192\n",
            "  10. Name: models/gemini-1.5-flash\n",
            "     Display Name: Gemini 1.5 Flash\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  11. Name: models/gemini-1.5-flash-002\n",
            "     Display Name: Gemini 1.5 Flash 002\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  12. Name: models/gemini-1.5-flash-8b\n",
            "     Display Name: Gemini 1.5 Flash-8B\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  13. Name: models/gemini-1.5-flash-8b-001\n",
            "     Display Name: Gemini 1.5 Flash-8B 001\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  14. Name: models/gemini-1.5-flash-8b-latest\n",
            "     Display Name: Gemini 1.5 Flash-8B Latest\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  15. Name: models/gemini-1.5-flash-8b-exp-0827\n",
            "     Display Name: Gemini 1.5 Flash 8B Experimental 0827\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  16. Name: models/gemini-1.5-flash-8b-exp-0924\n",
            "     Display Name: Gemini 1.5 Flash 8B Experimental 0924\n",
            "     Input Tokens: 1000000, Output Tokens: 8192\n",
            "  17. Name: models/gemini-2.5-pro-exp-03-25\n",
            "     Display Name: Gemini 2.5 Pro Experimental 03-25\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  18. Name: models/gemini-2.5-pro-preview-03-25\n",
            "     Display Name: Gemini 2.5 Pro Preview 03-25\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  19. Name: models/gemini-2.5-flash-preview-04-17\n",
            "     Display Name: Gemini 2.5 Flash Preview 04-17\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  20. Name: models/gemini-2.5-flash-preview-05-20\n",
            "     Display Name: Gemini 2.5 Flash Preview 05-20\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  21. Name: models/gemini-2.5-flash-preview-04-17-thinking\n",
            "     Display Name: Gemini 2.5 Flash Preview 04-17 for cursor testing\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  22. Name: models/gemini-2.5-pro-preview-05-06\n",
            "     Display Name: Gemini 2.5 Pro Preview 05-06\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  23. Name: models/gemini-2.0-flash-exp\n",
            "     Display Name: Gemini 2.0 Flash Experimental\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  24. Name: models/gemini-2.0-flash\n",
            "     Display Name: Gemini 2.0 Flash\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  25. Name: models/gemini-2.0-flash-001\n",
            "     Display Name: Gemini 2.0 Flash 001\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  26. Name: models/gemini-2.0-flash-exp-image-generation\n",
            "     Display Name: Gemini 2.0 Flash (Image Generation) Experimental\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  27. Name: models/gemini-2.0-flash-lite-001\n",
            "     Display Name: Gemini 2.0 Flash-Lite 001\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  28. Name: models/gemini-2.0-flash-lite\n",
            "     Display Name: Gemini 2.0 Flash-Lite\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  29. Name: models/gemini-2.0-flash-preview-image-generation\n",
            "     Display Name: Gemini 2.0 Flash Preview Image Generation\n",
            "     Input Tokens: 32768, Output Tokens: 8192\n",
            "  30. Name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "     Display Name: Gemini 2.0 Flash-Lite Preview 02-05\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  31. Name: models/gemini-2.0-flash-lite-preview\n",
            "     Display Name: Gemini 2.0 Flash-Lite Preview\n",
            "     Input Tokens: 1048576, Output Tokens: 8192\n",
            "  32. Name: models/gemini-2.0-pro-exp\n",
            "     Display Name: Gemini 2.0 Pro Experimental\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  33. Name: models/gemini-2.0-pro-exp-02-05\n",
            "     Display Name: Gemini 2.0 Pro Experimental 02-05\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  34. Name: models/gemini-exp-1206\n",
            "     Display Name: Gemini Experimental 1206\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  35. Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "     Display Name: Gemini 2.5 Flash Preview 04-17\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  36. Name: models/gemini-2.0-flash-thinking-exp\n",
            "     Display Name: Gemini 2.5 Flash Preview 04-17\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  37. Name: models/gemini-2.0-flash-thinking-exp-1219\n",
            "     Display Name: Gemini 2.5 Flash Preview 04-17\n",
            "     Input Tokens: 1048576, Output Tokens: 65536\n",
            "  38. Name: models/gemini-2.5-flash-preview-tts\n",
            "     Display Name: Gemini 2.5 Flash Preview TTS\n",
            "     Input Tokens: 32768, Output Tokens: 8192\n",
            "  39. Name: models/gemini-2.5-pro-preview-tts\n",
            "     Display Name: Gemini 2.5 Pro Preview TTS\n",
            "     Input Tokens: 65536, Output Tokens: 65536\n",
            "  40. Name: models/learnlm-2.0-flash-experimental\n",
            "     Display Name: LearnLM 2.0 Flash Experimental\n",
            "     Input Tokens: 1048576, Output Tokens: 32768\n",
            "  41. Name: models/gemma-3-1b-it\n",
            "     Display Name: Gemma 3 1B\n",
            "     Input Tokens: 32768, Output Tokens: 8192\n",
            "  42. Name: models/gemma-3-4b-it\n",
            "     Display Name: Gemma 3 4B\n",
            "     Input Tokens: 32768, Output Tokens: 8192\n",
            "  43. Name: models/gemma-3-12b-it\n",
            "     Display Name: Gemma 3 12B\n",
            "     Input Tokens: 32768, Output Tokens: 8192\n",
            "  44. Name: models/gemma-3-27b-it\n",
            "     Display Name: Gemma 3 27B\n",
            "     Input Tokens: 131072, Output Tokens: 8192\n",
            "  45. Name: models/gemma-3n-e4b-it\n",
            "     Display Name: Gemma 3n E4B\n",
            "     Input Tokens: 8192, Output Tokens: 2048\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from typing import List, Dict, Optional, Any # Assuming these are already imported\n",
        "\n",
        "\n",
        "def list_available_models() -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Lists available Gemini models that support 'generateContent'.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n",
        "                               contains information about a model.\n",
        "                               Returns an empty list if an error occurs or API key is not set.\n",
        "    \"\"\"\n",
        "    if not API_KEY: # Check if API key was successfully configured\n",
        "        print(\"ERROR: Cannot list models. Google API Key not configured.\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n--- Listing Available Gemini Models ---\")\n",
        "    models_info = []\n",
        "    try:\n",
        "        for m in genai.list_models():\n",
        "            # Models that support 'generateContent' are typically the ones you can use with your llm_call\n",
        "            if 'generateContent' in m.supported_generation_methods:\n",
        "                model_details = {\n",
        "                    \"name\": m.name,\n",
        "                    \"display_name\": m.display_name,\n",
        "                    \"description\": m.description,\n",
        "                    \"version\": m.version,\n",
        "                    \"input_token_limit\": m.input_token_limit if hasattr(m, 'input_token_limit') else \"N/A\",\n",
        "                    \"output_token_limit\": m.output_token_limit if hasattr(m, 'output_token_limit') else \"N/A\",\n",
        "                    \"supported_generation_methods\": m.supported_generation_methods,\n",
        "                }\n",
        "                models_info.append(model_details)\n",
        "\n",
        "        if models_info:\n",
        "            print(f\"Found {len(models_info)} models supporting 'generateContent':\")\n",
        "            for i, model_data in enumerate(models_info):\n",
        "                print(f\"  {i+1}. Name: {model_data['name']}\")\n",
        "                print(f\"     Display Name: {model_data['display_name']}\")\n",
        "                # print(f\"     Description: {model_data['description'][:100]}...\") # Optional: print snippet\n",
        "                print(f\"     Input Tokens: {model_data['input_token_limit']}, Output Tokens: {model_data['output_token_limit']}\")\n",
        "        else:\n",
        "            print(\"No models found supporting 'generateContent' or an issue occurred.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not list models. Reason: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return [] # Return empty list on error\n",
        "\n",
        "    return models_info\n",
        "\n",
        "# --- Example of how to call this function ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure API key is configured before calling\n",
        "    if API_KEY: # Make sure API_KEY is set from your setup\n",
        "        available_models = list_available_models()\n",
        "        # if available_models:\n",
        "        #     print(\"\\n--- Full Model Details (Python List) ---\")\n",
        "        #     for model_data in available_models:\n",
        "        #         print(model_data)\n",
        "    else:\n",
        "        print(\"Please ensure your GOOGLE_API_KEY is set up in Colab Secrets or your environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjc_ajKl6BkP"
      },
      "source": [
        "## Uploaing Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUpJ3mMiDG71",
        "outputId": "e674b461-3608-479a-a622-c428f5af84c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "import PyPDF2\n",
        "import docx\n",
        "import csv\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time # Import the time module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWTiPAovCOth",
        "outputId": "1cbb4d1f-6124-4cf9-de58-c3fc896a807a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory: '/content/drive/MyDrive/txt_files'\n",
            "\n",
            "Scanning directory: '/content/drive/MyDrive/Research Paper - Learning Agent'...\n",
            "Found 1 potential files to process.\n",
            "\n",
            "Processing file 1/1: 2104.08691v2 (1).pdf\n",
            "    Attempting to write to: 2104.08691v2 (1).txt\n",
            "    -> Extracted and saved to: 2104.08691v2 (1).txt (Write operation completed)\n",
            "\n",
            "--- Processing Complete ---\n",
            "Successfully processed files reported by script: 1\n",
            "Skipped/Failed files:       0\n",
            "---------------------------\n",
            "INFO: Waiting a few seconds for potential Drive sync...\n",
            "INFO: Wait finished.\n",
            "\n",
            "List of created/processed TXT files reported by script:\n",
            "Verifying file existence in output directory:\n",
            "- 2104.08691v2 (1).txt (Exists: True)\n",
            "\n",
            "Verification complete: 1 files found in output directory.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    import PyPDF2\n",
        "except ImportError:\n",
        "    print(\"WARNING: PyPDF2 library not found. PDF extraction will be disabled.\")\n",
        "    print(\"         Install it using: pip install PyPDF2\")\n",
        "    PyPDF2 = None\n",
        "\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    print(\"WARNING: python-docx library not found. DOCX extraction will be disabled.\")\n",
        "    print(\"         Install it using: pip install python-docx\")\n",
        "    docx = None\n",
        "\n",
        "# --- Helper Functions (keep as they are) ---\n",
        "\n",
        "def _extract_text_from_pdf(file_content: bytes) -> Optional[str]:\n",
        "    # (Your existing PDF extraction code - no changes needed here)\n",
        "    if not PyPDF2: return None\n",
        "    try:\n",
        "        pdf_file = io.BytesIO(file_content); reader = PyPDF2.PdfReader(pdf_file); text = \"\"\n",
        "        if reader.is_encrypted:\n",
        "            try:\n",
        "                if reader.decrypt('') == PyPDF2.PasswordType.NOT_DECRYPTED: return None\n",
        "            except Exception: return None\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                page_text = page.extract_text();\n",
        "                if page_text: text += page_text + \"\\n\"\n",
        "            except Exception: continue\n",
        "        return text.strip() if text else None\n",
        "    except PyPDF2.errors.PdfReadError: return None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "def _extract_text_from_docx(file_content: bytes) -> Optional[str]:\n",
        "    # (Your existing DOCX extraction code - no changes needed here)\n",
        "    if not docx: return None\n",
        "    try:\n",
        "        doc_file = io.BytesIO(file_content); document = docx.Document(doc_file)\n",
        "        text = \"\\n\".join([para.text for para in document.paragraphs if para.text])\n",
        "        return text.strip() if text else None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "def _read_text_from_txt(file_content: bytes, filename_for_log: str) -> Optional[str]:\n",
        "    # (Your existing TXT reading code - no changes needed here)\n",
        "    try: text = file_content.decode('utf-8'); return text\n",
        "    except UnicodeDecodeError:\n",
        "        try: text = file_content.decode('latin-1'); return text\n",
        "        except Exception: return None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def extract_and_copy_text_files(input_dir: str, output_dir: str) -> List[str]:\n",
        "    if not os.path.isdir(input_dir):\n",
        "        print(f\"Error: Input directory not found or is not a directory: '{input_dir}'\")\n",
        "        return []\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Output directory: '{os.path.abspath(output_dir)}'\")\n",
        "\n",
        "    successfully_processed_files = []\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    print(f\"\\nScanning directory: '{os.path.abspath(input_dir)}'...\")\n",
        "\n",
        "    files_to_process = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for filename in files:\n",
        "             files_to_process.append(os.path.join(root, filename))\n",
        "\n",
        "    total_files = len(files_to_process)\n",
        "    print(f\"Found {total_files} potential files to process.\")\n",
        "\n",
        "    # --- Process each file ---\n",
        "    for i, input_file_path in enumerate(files_to_process):\n",
        "        filename = os.path.basename(input_file_path)\n",
        "        base_name, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        supported_extensions = [\".pdf\", \".docx\", \".txt\"]\n",
        "        if file_extension not in supported_extensions:\n",
        "            continue # Skip unsupported types early\n",
        "\n",
        "        # Use relative path for potentially deeply nested files for cleaner logs\n",
        "        try:\n",
        "            relative_path = os.path.relpath(input_file_path, input_dir)\n",
        "        except ValueError:\n",
        "            relative_path = input_file_path\n",
        "        print(f\"\\nProcessing file {i+1}/{total_files}: {relative_path}\")\n",
        "\n",
        "\n",
        "        if file_extension == \".pdf\" and not PyPDF2:\n",
        "            print(\"    - Skipping PDF: PyPDF2 library not available.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        if file_extension == \".docx\" and not docx:\n",
        "            print(\"    - Skipping DOCX: python-docx library not available.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(input_file_path, 'rb') as f:\n",
        "                file_content = f.read()\n",
        "        except IOError as e:\n",
        "            print(f\"    - Error reading file: {e}\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        except Exception as e:\n",
        "             print(f\"    - Unexpected error reading file: {e}\")\n",
        "             skipped_count += 1\n",
        "             continue\n",
        "\n",
        "        full_text = None\n",
        "        if file_extension == \".pdf\":\n",
        "            full_text = _extract_text_from_pdf(file_content)\n",
        "        elif file_extension == \".docx\":\n",
        "            full_text = _extract_text_from_docx(file_content)\n",
        "        elif file_extension == \".txt\":\n",
        "            full_text = _read_text_from_txt(file_content, filename)\n",
        "\n",
        "        if full_text is not None:\n",
        "            output_filename = base_name + \".txt\"\n",
        "            output_file_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            try:\n",
        "                print(f\"    Attempting to write to: {output_filename}\") # Add debug print\n",
        "                with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
        "                    f_out.write(full_text.strip())\n",
        "                    # --- Add these lines to force flush ---\n",
        "                    f_out.flush() # Flush Python's internal buffer\n",
        "                    os.fsync(f_out.fileno()) # Ask OS to sync to disk (Drive mount)\n",
        "                    # ----------------------------------------\n",
        "                action = \"Extracted and saved\" if file_extension != \".txt\" else \"Processed\"\n",
        "                print(f\"    -> {action} to: {output_filename} (Write operation completed)\")\n",
        "                successfully_processed_files.append(os.path.abspath(output_file_path))\n",
        "                processed_count += 1\n",
        "                # --- Optional: Add a small delay after each file ---\n",
        "                # time.sleep(0.5) # Pause for 0.5 seconds\n",
        "                # -------------------------------------------------\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"    - Error writing output file '{output_filename}': {e}\")\n",
        "                skipped_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"    - Unexpected error writing output file '{output_filename}': {e}\")\n",
        "                skipped_count += 1\n",
        "        else:\n",
        "            print(f\"    - Text extraction or reading failed for: {filename}\")\n",
        "            skipped_count += 1\n",
        "\n",
        "    print(f\"\\n--- Processing Complete ---\")\n",
        "    print(f\"Successfully processed files reported by script: {processed_count}\")\n",
        "    print(f\"Skipped/Failed files:       {skipped_count}\")\n",
        "    print(f\"---------------------------\")\n",
        "    print(\"INFO: Waiting a few seconds for potential Drive sync...\")\n",
        "    time.sleep(5) # Add a final delay to allow background sync\n",
        "    print(\"INFO: Wait finished.\")\n",
        "\n",
        "    return successfully_processed_files\n",
        "\n",
        "# --- Example Usage (Keep as is) ---\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_DIRECTORY = r\"/content/drive/MyDrive/Research Paper - Learning Agent\"\n",
        "    OUTPUT_DIRECTORY = r\"/content/drive/MyDrive/txt_files\"\n",
        "\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: Input directory not found: '{INPUT_DIRECTORY}'\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    created_files = extract_and_copy_text_files(INPUT_DIRECTORY, OUTPUT_DIRECTORY)\n",
        "\n",
        "    if created_files:\n",
        "        print(\"\\nList of created/processed TXT files reported by script:\")\n",
        "        # Check existence on disk *after* the final delay\n",
        "        print(\"Verifying file existence in output directory:\")\n",
        "        actual_files_found = 0\n",
        "        for f_path in created_files:\n",
        "             exists = os.path.exists(f_path)\n",
        "             print(f\"- {os.path.relpath(f_path, OUTPUT_DIRECTORY)} (Exists: {exists})\")\n",
        "             if exists:\n",
        "                 actual_files_found += 1\n",
        "        print(f\"\\nVerification complete: {actual_files_found} files found in output directory.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo TXT files were created or processed according to script logs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvqpMTNRJOCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i2U6ZCEFcWs"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_txt_file(file_path: str) -> Optional[str]:\n",
        "\n",
        "    try:\n",
        "        # Ensure the file exists before trying to open it\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: File not found at path: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        # Open the file in read mode ('r') with UTF-8 encoding (common and robust)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except FileNotFoundError: # This is redundant if os.path.exists is used, but good practice\n",
        "        print(f\"ERROR: File not found: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not read file {file_path}. Reason: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oql4lPhwFIhx"
      },
      "outputs": [],
      "source": [
        "resource_text = (extract_text_from_txt_file(created_files[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBSsnYSILtdk",
        "outputId": "f9c84644-0de9-4d62-8f51-f1a0c404fbcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63856"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "len(resource_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuO2cGsuEJqE",
        "outputId": "a8bc7a94-ce42-4f1e-cc09-4485ee1f27cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/txt_files/2104.08691v2 (1).txt']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "created_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwDgevWwE3Py"
      },
      "outputs": [],
      "source": [
        "\n",
        "book_name = (created_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtUxN_eCKxZW",
        "outputId": "75ffa674-96aa-42a5-e72f-6690ac2d7c0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/txt_files/2104.08691v2 (1).txt']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "created_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrTuII_XCQxY"
      },
      "source": [
        "## Syllabus Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__1qxs9VahkR"
      },
      "outputs": [],
      "source": [
        "syllabus_generator_system_prompt = \"\"\"\n",
        "You are an AI Syllabus Creator. Your input is a conversation history detailing a user's request for a learning plan. Your task is to analyze this entire conversation and generate or modify a syllabus based on the discussion.\n",
        "\n",
        "**Core Instructions:**\n",
        "\n",
        "1.  **Analyze Conversation History:**\n",
        "    *   Carefully read the entire conversation history provided as context.\n",
        "    *   Identify the primary learning topic, the user's stated experience level (e.g., beginner, intermediate, advanced in the subject or prerequisites), their explicit learning goals, preferred learning style (if mentioned), and any constraints.\n",
        "    *   If a previous syllabus version (enclosed in `<syllabus>` tags) exists in the history, note it and any subsequent user feedback regarding it.\n",
        "\n",
        "2.  **Mode of Operation:**\n",
        "    *   **Modification:** If a recent syllabus version and specific user feedback on it are present in the conversation history, your primary goal is to modify that syllabus according to the feedback. Ensure changes are targeted and address the user's requests.\n",
        "    *   **Creation:** If no prior syllabus exists or if the request is for a new one, create a syllabus from scratch based on the synthesized information from the conversation (topic, experience, goals).\n",
        "\n",
        "3.  **Syllabus Structure and Content Design:**\n",
        "    *   **Logical Phasing:** Organize the syllabus into 2 to 5 distinct learning phases. Each phase should represent a major stage in the learning progression.\n",
        "    *   **Lessons per Phase:** Within each phase, define 2 to 4 specific lessons or topics.\n",
        "    *   **Progressive Order:** Ensure that phases and lessons are arranged in a logical, progressive order, building complexity and knowledge incrementally.\n",
        "    *   **Detailed Lesson Information:** For each lesson, provide the following details:\n",
        "        *   `Topic`: A clear, concise title for the lesson.\n",
        "        *   `Keywords`: A list of 3-5 key terms or concepts central to the lesson.\n",
        "        *   `Objective`: A brief statement (1-2 sentences) describing what the learner should be able to do or understand after completing the lesson.\n",
        "        *   `Focus`: A short description (1-2 sentences) of the main emphasis or key takeaways for that lesson.\n",
        "\n",
        "4.  **Output Format:**\n",
        "    *   Enclose the *entire* final syllabus structure within `<syllabus>` and `</syllabus>` XML tags.\n",
        "    *   **Output ONLY the syllabus structure within these tags.** Do not include any other conversational text, explanations, or apologies before or after the `<syllabus>` block.\n",
        "\n",
        "5.  **Adherence and Quality:**\n",
        "    *   Strictly follow all analysis and formatting instructions.\n",
        "    *   Base the syllabus *entirely* on the information gleaned from the conversation history. Do not introduce external topics or assumptions not supported by the dialogue.\n",
        "    *   Ensure the syllabus is coherent, practical, and tailored to the user's expressed needs.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5UFPvVzbdlb"
      },
      "outputs": [],
      "source": [
        "SYLLABUS_PROMPT_WITH_RAW_TEXT_COLLECTION = \"\"\"\n",
        "You are an AI Syllabus Creator. Your main task is to generate or modify a syllabus, incorporating insights directly from user-provided raw text learning resources. For your information the\n",
        "resources won't be provided to other agents\n",
        "**Crucial Context for Your Analysis:**\n",
        "It is vital for you to understand that the raw text learning resources provided below are **exclusive to you** for this syllabus creation process. These resources **will not be shared with, nor will they have been pre-processed or summarized by, any other AI agents** in the system. Therefore, your thorough, independent, and comprehensive analysis of the full content of these raw texts is paramount for successfully creating a relevant and well-structured syllabus.\n",
        "\n",
        "\n",
        "**You will analyze the following information:**\n",
        "\n",
        "1.  **Conversation History:**\n",
        "    *   This will be provided to you as a separate conversational context. It contains the ongoing dialogue with the user.\n",
        "    *   From this history, carefully extract: the primary learning topic, the user's stated experience level, their explicit learning goals, preferred learning style (if mentioned), any constraints, and any prior syllabus versions or feedback.\n",
        "\n",
        "2.  **User-Provided Raw Text Resources (JSON Object):**\n",
        "    *   The raw text content of user-provided resources is given directly below, within the placeholder. It's a JSON object.\n",
        "    *   The JSON object will have a `collection_type` field, which will be \"raw_text_collection\".\n",
        "    *   It will also have a `resources_data` field, which is a list. **Each item in this list is an object representing a single resource and will contain:**\n",
        "        *   `identifier`: A name for the resource (e.g., filename).\n",
        "        *   `content`: The **full raw textual content** of that resource.\n",
        "    *   If `resources_data` is empty (though `collection_type` would likely be \"none\" in that case from the calling script), proceed based on conversation history alone.\n",
        "\n",
        "Resource Information (JSON Object containing raw texts):\n",
        "{resource_information_placeholder}\n",
        "\n",
        "**Core Instructions for Syllabus Generation/Modification using Raw Text Resources:**\n",
        "\n",
        "3.  **Analyze Raw Text Resources (from the placeholder above):**\n",
        "    *   For each resource object in the `resources_data` list, thoroughly analyze its raw `content`.\n",
        "    *   Identify key topics, main concepts, definitions, examples, explanations, and the overall structure or flow of information within each raw text.\n",
        "    *   Determine which parts of each resource are most relevant to the user's learning goals and current focus as identified from the Conversation History.\n",
        "\n",
        "4.  **Mode of Operation:**\n",
        "    *   **Modification:** If a recent syllabus and user feedback exist in the Conversation History, modify that syllabus. Directly integrate relevant information and concepts extracted from the provided raw text resources to address the feedback.\n",
        "    *   **Creation:** Otherwise, create a new syllabus. Base it on the user's needs from the Conversation History, using the content of the raw text resources to structure topics, define lessons, and provide depth.\n",
        "\n",
        "5.  **Syllabus Structure and Content Design:**\n",
        "    *   **Direct Integration of Raw Text Insights:**\n",
        "        *   When designing phases and lessons, draw directly from the topics and information present in the provided raw text resources. For example, if a resource details three key steps for a process, that could become a lesson or part of one.\n",
        "        *   Ensure the syllabus remains primarily aligned with the user's explicit goals and experience level stated in the Conversation History. Use the raw text to *fulfill* these goals, not to deviate significantly unless the text offers a clearly superior path that still meets the core objectives.\n",
        "    *   **Logical Phasing:** Organize into 2-5 distinct learning phases.\n",
        "    *   **Lessons per Phase:** 2-4 specific lessons/topics per phase.\n",
        "    *   **Progressive Order:** Ensure logical, incremental progression.\n",
        "    *   **Detailed Lesson Information:** For each lesson:\n",
        "        *   `Topic`: Clear title (potentially inspired by resource headings or sections).\n",
        "        *   `Keywords`: 3-5 central terms (many might come directly from the raw text).\n",
        "        *   `Objective`: What the learner will understand/do (informed by what the raw text explains).\n",
        "        *   `Focus`: Main emphasis/takeaways (highlighting key points from the raw text).\n",
        "\n",
        "6.  **Output Format:**\n",
        "    *   Enclose the *entire* final syllabus structure within `<syllabus>` and `</syllabus>` XML tags.\n",
        "    *   **Output ONLY the syllabus structure within these tags.** No other text.\n",
        "\n",
        "7.  **Adherence and Quality:**\n",
        "    *   Strictly follow instructions.\n",
        "    *   The syllabus should be a practical learning plan based on the Conversation History, directly leveraging the content of the provided raw text resources.\n",
        "    *   Produce a coherent and tailored syllabus.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e4oOzZ6axeE"
      },
      "outputs": [],
      "source": [
        "syllabus_generator_system_prompt_heavy_resources = \"\"\"\n",
        "You are an AI Syllabus Creator. Your main task is to generate or modify a syllabus, potentially incorporating insights from user-provided learning resources.\n",
        "**Crucial Context for Your Analysis:**\n",
        "It is vital for you to understand that the raw text learning resources provided below are **exclusive to you** for this syllabus creation process. These resources **will not be shared with, nor will they have been pre-processed or summarized by, any other AI agents** in the system. Therefore, your thorough, independent, and comprehensive analysis of the full content of these raw texts is paramount for successfully creating a relevant and well-structured syllabus.\n",
        "\n",
        "\n",
        "**You will analyze the following information:**\n",
        "\n",
        "1.  **Conversation History:**\n",
        "    *   This will be provided to you as a separate conversational context. It contains the ongoing dialogue with the user.\n",
        "    *   From this history, carefully extract: the primary learning topic, the user's stated experience level, their explicit learning goals, any constraints, and any prior syllabus versions or feedback.\n",
        "\n",
        "2.  **User-Provided Resource Information (JSON Object):**\n",
        "    *   This information is provided directly below, within the placeholder. It's a JSON object describing any resources the user has supplied.\n",
        "    *   The JSON object will have a `collection_type` field (e.g., \"raw_text_collection\", \"json_summary_collection\", \"mixed_resource_collection\", or \"none\").\n",
        "    *   It will also have a `resources_data` field, which is a list. Each item in this list is an object describing a single resource and will have its own `type` field:\n",
        "        *   If `type` is \"raw_content\": The object will contain an \"identifier\" (e.g., filename) and \"content\" (the raw text of that resource).\n",
        "        *   If `type` is \"json_summary\": The object will be a structured summary (e.g., with \"resource_identifier\", \"primary_topics_relevant_to_conversation\", \"core_concepts_relevant_to_conversation\", etc.).\n",
        "    *   If `collection_type` is \"none\" or `resources_data` is empty, proceed based on conversation history alone.\n",
        "\n",
        "Resource Information (JSON Object):\n",
        "{resource_information_placeholder}\n",
        "\n",
        "**Core Instructions for Syllabus Generation/Modification:**\n",
        "\n",
        "3.  **Analyze Resource Information (if provided in the placeholder):**\n",
        "    *   For each item in the `resources_data` list:\n",
        "        *   If it's \"raw_content\", analyze the raw text to identify key topics, concepts, structure, and any parts particularly relevant to the user's goals from the Conversation History.\n",
        "        *   If it's \"json_summary\", analyze the structured summary, paying attention to the topics, concepts, and contextual notes already extracted.\n",
        "    *   Synthesize insights from ALL provided resources.\n",
        "\n",
        "4.  **Mode of Operation:**\n",
        "    *   **Modification:** If a recent syllabus and user feedback exist in the Conversation History, modify that syllabus. Integrate feedback and relevant insights from any provided Resource Information.\n",
        "    *   **Creation:** Otherwise, create a new syllabus. Base it on the user's needs from the Conversation History, enriched and structured by insights from any provided Resource Information.\n",
        "\n",
        "5.  **Syllabus Structure and Content Design:**\n",
        "    *   **Integration:**\n",
        "        *   If Resource Information is provided and relevant: Intelligently weave the topics, concepts, and structural ideas from the resources (raw text or summaries) into the syllabus. The syllabus should reflect the valuable content of the resources.\n",
        "        *   Ensure the syllabus remains primarily aligned with the user's explicit goals and experience level stated in the Conversation History. If resources suggest a different direction, prioritize the user's conversational requests unless the resources clearly offer a better path that still meets the core goals.\n",
        "    *   **Logical Phasing:** Organize into 2-5 distinct learning phases.\n",
        "    *   **Lessons per Phase:** 2-4 specific lessons/topics per phase.\n",
        "    *   **Progressive Order:** Ensure logical, incremental progression.\n",
        "    *   **Detailed Lesson Information:** For each lesson:\n",
        "        *   `Topic`: Clear title.\n",
        "        *   `Keywords`: 3-5 central terms.\n",
        "        *   `Objective`: What the learner will understand/do.\n",
        "        *   `Focus`: Main emphasis/takeaways.\n",
        "\n",
        "6.  **Output Format:**\n",
        "    *   Enclose the *entire* final syllabus structure within `<syllabus>` and `</syllabus>` XML tags.\n",
        "    *   **Output ONLY the syllabus structure within these tags.** No other text.\n",
        "\n",
        "7.  **Adherence and Quality:**\n",
        "    *   Strictly follow instructions.\n",
        "    *   If Resource Information is used, ensure the syllabus reflects its content appropriately while prioritizing user requests. If no relevant resources are provided, base the syllabus entirely on the Conversation History.\n",
        "    *   Produce a coherent, practical, and tailored syllabus.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQy0tjHc-SKs"
      },
      "outputs": [],
      "source": [
        "# This prompt needs to guide the LLM to summarize the resource *in the context of the conversation*\n",
        "dynamic_resource_summarizer_prompt_template_v1 = \"\"\"\n",
        "You are an AI Resource Analyzer. Your task is to process the single provided 'Learning Material Excerpt'.\n",
        "Your primary goal is to extract key information that is MOST RELEVANT to the ongoing 'Conversation History' provided below.\n",
        "Pay special attention to any sections that appear to be a Table of Contents, chapter overview, or introduction, as these are highly valuable for understanding the resource's structure for syllabus planning.\n",
        "The summary should help in creating a structured learning syllabus that directly addresses the user's current focus and needs as expressed in the conversation.\n",
        "\n",
        "Output your analysis as a SINGLE JSON object containing the following keys:\n",
        "\n",
        "*   \"resource_identifier\": (A string. Use the provided identifier: \"{resource_identifier_placeholder}\")\n",
        "*   \"primary_topics_relevant_to_conversation\": (A list of strings. Identify the main topics or themes in this excerpt that directly relate to or support the topics being discussed in the 'Conversation History'.)\n",
        "*   \"core_concepts_relevant_to_conversation\": (A list of strings. Extract key concepts, definitions, or fundamental ideas from this excerpt that are pertinent to the 'Conversation History'.)\n",
        "*   \"structure_or_progression_notes\": (A string. Briefly describe the overall structure or flow of information in this excerpt, and note if this structure aligns well with the progression of the 'Conversation History'.)\n",
        "*   \"keywords_highlighted_by_conversation\": (A list of strings. List important terminology from this excerpt, giving priority to keywords that have also appeared or are implied in the 'Conversation History'.)\n",
        "*   \"inferred_learning_objectives_for_current_focus\": (A list of strings. Based on the 'Conversation History', what learning objectives from this excerpt would be most immediately beneficial or relevant to the user?)\n",
        "*   \"contextual_notes_for_syllabus\": (A string. Provide specific observations on how this resource excerpt (or parts of it) could be directly used to address specific points, questions, or learning goals raised in the 'Conversation History'. Note any parts of the excerpt that seem less relevant to the current discussion.)\n",
        "\n",
        "Ensure the output is ONLY the valid JSON object.\n",
        "\n",
        "Provided Resource Identifier: {resource_identifier_placeholder}\n",
        "\n",
        "\n",
        "Learning Material Excerpt to Analyze:\n",
        "{learning_material_content_placeholder}\n",
        "\"\"\"\n",
        "def summarize_single_resource_dynamically(\n",
        "    resource_content: str,\n",
        "    resource_identifier: str,\n",
        "    conversation_history: List[Dict[str, Any]],\n",
        "    max_length: int = 100000 # Assuming your 100k char limit per resource\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Summarizes a single piece of resource content using an LLM,\n",
        "    making the summary relevant to the provided conversation history.\n",
        "    \"\"\"\n",
        "    if not resource_content.strip():\n",
        "        print(f\"Skipping empty resource: {resource_identifier}\")\n",
        "        return None\n",
        "\n",
        "    truncated_content = resource_content[:max_length]\n",
        "    if len(resource_content) > max_length:\n",
        "        print(f\"INFO: Resource '{resource_identifier}' was truncated to {max_length} characters for dynamic summarization.\")\n",
        "\n",
        "\n",
        "    prompt_for_summarizer = dynamic_resource_summarizer_prompt_template_v1.format(\n",
        "        resource_identifier_placeholder=resource_identifier,\n",
        "        learning_material_content_placeholder=truncated_content\n",
        "    )\n",
        "\n",
        "\n",
        "    summary_json_str = llm_call(\n",
        "        prompt=prompt_for_summarizer,\n",
        "        chat_history=conversation_history,\n",
        "        # system_prompt=None, # The main instruction is in the prompt itself\n",
        "        # model=\"gemini-2.5-pro-preview-04-17\",\n",
        "        temperature=0.4\n",
        "    )\n",
        "    cleaned_json_str = summary_json_str.strip()\n",
        "    if cleaned_json_str.startswith(\"```json\"):\n",
        "        cleaned_json_str = cleaned_json_str[len(\"```json\"):]\n",
        "    elif cleaned_json_str.startswith(\"```\"): # More generic ``` opening\n",
        "        cleaned_json_str = cleaned_json_str[len(\"```\"):]\n",
        "\n",
        "    if cleaned_json_str.endswith(\"```\"):\n",
        "        cleaned_json_str = cleaned_json_str[:-len(\"```\")]\n",
        "\n",
        "    cleaned_json_str = cleaned_json_str.strip()\n",
        "\n",
        "    try:\n",
        "        summary_data = json.loads(cleaned_json_str)\n",
        "        # Basic validation (can be more thorough based on your JSON keys)\n",
        "        if isinstance(summary_data, dict) and \"resource_identifier\" in summary_data:\n",
        "            return summary_data\n",
        "        else:\n",
        "            print(f\"WARN: Dynamic summarizer for '{resource_identifier}' produced non-standard JSON. Output: {summary_json_str[:200]}...\")\n",
        "            # Fallback if needed\n",
        "            return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": summary_json_str, \"is_fallback\": True, \"error\": \"Non-standard JSON structure\"}\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"WARN: Could not parse JSON from dynamic summary for '{resource_identifier}'. Raw output: {summary_json_str[:200]}...\")\n",
        "        return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": summary_json_str, \"is_fallback\": True, \"error\": \"JSONDecodeError\"}\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Unexpected error during dynamic summarization for '{resource_identifier}': {e}\")\n",
        "        return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": str(e), \"is_fallback\": True, \"error\": str(type(e).__name__)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUNEud_cBe99"
      },
      "outputs": [],
      "source": [
        "conversation_history = [{'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        " {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need. iam very week at the math part and have also attached the paper']},\n",
        " {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        " {'role': 'user',\n",
        "  'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        " {'role': 'model', 'parts': ['Ok generating syallabus']}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1b-iNZEivE1"
      },
      "outputs": [],
      "source": [
        "conversation_history_heavy = [{'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        " {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need and also provied a txt for learning RL. iam very week at the math part and have also attached the paper and book explain the mathmatical concepts involved in it']},\n",
        " {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        " {'role': 'user',\n",
        "  'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        " {'role': 'model', 'parts': ['Ok generating syallabus']}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrBnA1SxF25Y"
      },
      "outputs": [],
      "source": [
        "#k = summarize_single_resource_dynamically(conversation_history=conversation_history,resource_content=resource_text,resource_identifier=book_name,max_length = 200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Alxlo_O6Ha9"
      },
      "outputs": [],
      "source": [
        "def generate_syllabus_from_chat(\n",
        "    conversation_history: List[Dict[str, Any]],\n",
        "    resource_list: Optional[List[Dict[str, Any]]] = [],\n",
        "\n",
        "    model_name: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates or modifies a syllabus.\n",
        "    Formats resources directly into the system_prompt if provided.\n",
        "    \"\"\"\n",
        "    # print( len(resource_list))\n",
        "\n",
        "\n",
        "\n",
        "    generation_instruction_for_llm_prompt = \"Generate or modify the syllabus based on the provided conversation history and system instructions .\"\n",
        "\n",
        "    active_system_prompt_str: str\n",
        "    final_user_facing_prompt_str: str = generation_instruction_for_llm_prompt\n",
        "\n",
        "    if resource_list:\n",
        "\n",
        "        print(f\"[System: Formatting {len(resource_list)} resource summaries into the system prompt.]\")\n",
        "        try:\n",
        "          extracted_data_dict = {}\n",
        "          extracted_data_dict = {path: extract_text_from_txt_file(path) for path in resource_list}\n",
        "          chars_length = 0\n",
        "          extracted_basedata_dict ={}\n",
        "\n",
        "          for full_path,chars in extracted_data_dict.items():\n",
        "\n",
        "            chars_length += len(chars)\n",
        "            base_filename = os.path.basename(full_path)\n",
        "            extracted_basedata_dict[base_filename] = chars\n",
        "          if chars_length > 70000:\n",
        "            #Implement the Summary_generation and Implement the syllabus_generator_system_prompt_with_heavy_resources\n",
        "            # summarize_single_resource_dynamically(value,key,conversation_history=conversation_history)\n",
        "            # extracted_summary_dict = {key: value for key, value in (summarize_single_resource_dynamically(path) for path in resource_list)}\n",
        "            extracted_summary_dict = [summarize_single_resource_dynamically(value,key,conversation_history=conversation_history) for key,value in extracted_basedata_dict.items() ]\n",
        "            active_system_prompt_str = syllabus_generator_system_prompt_heavy_resources.format(resource_information_placeholder =extracted_summary_dict )\n",
        "\n",
        "\n",
        "          else:\n",
        "            active_system_prompt_str =  SYLLABUS_PROMPT_WITH_RAW_TEXT_COLLECTION.format(resource_information_placeholder = extracted_basedata_dict )\n",
        "\n",
        "\n",
        "            # Append the Whole Resource  to the Prompt and make it generate the syllabus or a new prompt  as dict they are similar to Json.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        except (TypeError, KeyError) as e: # KeyError if placeholder is wrong\n",
        "            print(f\"ERROR: Could not format resource_list into system prompt: {e}\")\n",
        "            print(\"[System: Falling back to default syllabus generator prompt without resources.]\")\n",
        "            # Fallback to default prompt if formatting fails\n",
        "            active_system_prompt_str = syllabus_generator_system_prompt\n",
        "\n",
        "    else:\n",
        "      print(\"There are no  Resources Provided\")\n",
        "      active_system_prompt_str = syllabus_generator_system_prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Make the LLM call\n",
        "    llm_response = llm_call(\n",
        "        prompt=final_user_facing_prompt_str,\n",
        "        chat_history=conversation_history,\n",
        "        system_prompt=active_system_prompt_str,\n",
        "        # model=\"gemini-2.5-pro-preview-04-17\",\n",
        "        temperature=1\n",
        "    )\n",
        "\n",
        "    if llm_response and not llm_response.startswith((\"[ERROR\", \"[BLOCKED\")): # Adjusted for tuple\n",
        "        extracted_syllabus = extract_xml(llm_response, \"syllabus\")\n",
        "        if extracted_syllabus:\n",
        "            print(\"--- Syllabus Extracted Successfully (from LLM response) ---\")\n",
        "            return extracted_syllabus\n",
        "        else:\n",
        "            if llm_response.strip().startswith(\"<syllabus>\") and llm_response.strip().endswith(\"</syllabus>\"):\n",
        "                 print(\"--- Syllabus Tags Found Directly (LLM response) ---\")\n",
        "                 inner_content = llm_response.strip()[len(\"<syllabus>\"):-len(\"</syllabus>\")].strip()\n",
        "                 return inner_content\n",
        "            else:\n",
        "                 print(\"WARN: Could not extract <syllabus> tags from LLM response.\")\n",
        "                 print(f\"Full LLM Response was:\\n{llm_response[:500]}...\")\n",
        "                 return f\"[ERROR: Syllabus tags not found in response: {llm_response[:100]}...]\"\n",
        "    else:\n",
        "        error_prefix = \"[ERROR\" if not llm_response or llm_response.startswith(\"[ERROR\") else \"[BLOCKED\"\n",
        "        print(f\"ERROR: Syllabus generation LLM call failed: {llm_response if llm_response else error_prefix + ': No response]'}\")\n",
        "        return llm_response if llm_response else f\"{error_prefix}: No response or an issue occurred]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jigz5a1QjFG9"
      },
      "outputs": [],
      "source": [
        "#singlefile_syllabus= generate_syllabus_from_chat(conversation_history=conversation_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aKAgn9ktu2Io"
      },
      "outputs": [],
      "source": [
        "#multifile_syllabus = generate_syllabus_from_chat(conversation_history=conversation_history_heavy,resource_list=created_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g9KVRpT8vUYf"
      },
      "outputs": [],
      "source": [
        "#print(multifile_syllabus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s6nYy7Cju0HB"
      },
      "outputs": [],
      "source": [
        "#print(singlefile_syllabus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq2rJ4ehJ2VU",
        "outputId": "e040e10c-79f1-4766-d555-3bbffabb288b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified get_intent function defined.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "convo_manager_system_prompt_explicit= \"\"\"\n",
        "You are a helpful AI assistant acting as a 'Conversation Manager'. Your primary role is to facilitate a conversation with the user to define requirements for a learning syllabus.\n",
        "\n",
        "**Your Core Task:** Guide the user through discussing their needs (topic, experience, goals, style). You DO NOT generate the syllabus text itself. Instead, you use specific tags to signal when external actions (generation, modification, finalization, persona selection) are needed by the system.\n",
        "\n",
        "**Strict Operating Rules:**\n",
        "\n",
        "1.  **Conversational Turns:** Engage in natural, helpful conversation to gather information, ask for feedback, or ask about learning style. Your responses should be conversational text ONLY, *unless* a condition below requires a tag.\n",
        "2.  **Tag-Only Turns:** When specific conditions are met (see below), your *entire response* MUST consist *solely* of the designated tag. **DO NOT add ANY other text, greetings, explanations, or punctuation in a response containing a tag.**\n",
        "\n",
        "**Tag Trigger Conditions & Required Output:**\n",
        "\n",
        "*   **Condition:** You have gathered sufficient initial information (topic, experience, goals) to request the *first* syllabus draft.\n",
        "    *   **Required Output:** `<request_syllabus_generation/>`\n",
        "*   **Condition:** The user has provided feedback on an *existing* syllabus (identified by `<syllabus>` tags in the chat history), and you understand the requested changes.\n",
        "    *   **Required Output:** `<request_syllabus_modification/>`\n",
        "*   **Condition:** The user explicitly confirms they are satisfied with the *most recent* syllabus presented.\n",
        "    *   **Required Output:** `<request_finalization/>`\n",
        "*   **Condition:** You have asked the user for their preferred learning style, and the user has provided their preference.\n",
        "    *   **Required Output:** `<persona/>`\n",
        "\n",
        "**Interaction Flow (Post-System Actions & User Responses):**\n",
        "\n",
        "*   **After Syllabus Presentation:** The system will display a syllabus (enclosed in `<syllabus>` tags, originating from the 'model' role). In your *next conversational turn* (immediately following the syllabus display), your response MUST be natural language asking the user for feedback (e.g., \"Here's the syllabus draft based on our discussion. How does it look? Are there any changes you'd like?\"). **Do not output a tag here.**\n",
        "*   **After Finalization Signal:** Immediately *after* you have outputted the `<request_finalization/>` tag, your *very next conversational turn* MUST be to ask the user about their preferred learning style (e.g., \"Great, the syllabus is finalized! To help tailor the learning experience, could you tell me how you prefer to learn? For example, do you like detailed explanations, code examples, hands-on exercises, or a combination?\"). **Do not output another tag here.**\n",
        "*   **After Learning Style Response:** Immediately *after* the user responds with their preferred learning style, your *very next response* MUST be **ONLY** the tag: `<persona/>`. **Do not add conversational text here.**\n",
        "\n",
        "**Crucial Constraints:**\n",
        "\n",
        "*   **One Action Per Turn:** A single response turn can ONLY be *either* conversational text *or* a single tag, NEVER both.\n",
        "*   **Wait for System/User:** After outputting a tag (`<request_syllabus_generation/>`, `<request_syllabus_modification/>`, `<request_finalization/>`, `<persona/>`), simply stop and wait for the next system action or user input as appropriate. Do not chain conversational text after a tag in the same turn.\n",
        "*   **No Explanations with Tags:** Do not explain *why* you are outputting a tag in the same turn as the tag. For example, DO NOT output: \"Okay, I have enough info now. `<request_syllabus_generation/>`\". The correct output is JUST: `<request_syllabus_generation/>`.\n",
        "\"\"\"\n",
        "\n",
        "# Should the convo manager know what tags were called.\n",
        "# --- Syllabus Generator System Prompt (Unchanged from previous version) ---\n",
        "# --- NEW: Intent Recognizer System Prompt ---\n",
        "intent_recognizer_prompt = \"\"\"\n",
        "Analyze the following model message. Respond with ONLY ONE of the following codes, based on the primary action signaled by the message:\n",
        "\n",
        "*   `GENERATE`: If the message signals a request to generate a syllabus for the first time (e.g., contains `<request_syllabus_generation/>`).\n",
        "*   `MODIFY`: If the message signals a request to modify an existing syllabus (e.g., contains `<request_syllabus_modification/>`).\n",
        "*   `FINALIZE`: If the message signals that the user has confirmed the syllabus and finalization is requested (e.g., contains `<request_finalization/>`).\n",
        "*   `CONVERSE`: If the message is purely conversational or asking for feedback, and does not contain an action tag.\n",
        "*    `PERSONA`: If the message signals that the user has provided their preferred learning style (e.g., contains `<persona/>`).\n",
        "\n",
        "Model Message:\n",
        "{model_response}\n",
        "\"\"\"\n",
        "\n",
        "# --- (Keep `generate_syllabus_from_chat` function as defined in the previous step) ---\n",
        "# It correctly takes history and uses syllabus_generator_system_prompt\n",
        "\n",
        "def get_intent(model_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Classifies the intent based on specific tags in the Conversation Manager's response.\n",
        "    Does NOT use an LLM call for this.\n",
        "    \"\"\"\n",
        "    if not model_response:\n",
        "        return \"CONVERSE\" # Handle empty response\n",
        "\n",
        "    cleaned_response = model_response.strip()\n",
        "\n",
        "    # Check for specific tags\n",
        "    if \"<request_syllabus_generation/>\" in cleaned_response:\n",
        "        return \"GENERATE\"\n",
        "    if \"<request_syllabus_modification/>\" in cleaned_response:\n",
        "        return \"MODIFY\"\n",
        "    if \"<request_finalization/>\" in cleaned_response:\n",
        "        return \"FINALIZE\"\n",
        "    if \"<persona/>\" in cleaned_response:\n",
        "        return \"PERSONA\"\n",
        "\n",
        "    # Default to CONVERSE if no specific tag is found\n",
        "    return \"CONVERSE\"\n",
        "\n",
        "print(\"Simplified get_intent function defined.\")\n",
        "conversation_history = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB_7HygfKUc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "prompt_generation = \"\"\"\n",
        "\n",
        "Your Role: You are an AI Persona Architect. Your primary function is to craft detailed, effective, and engaging system prompts for AI Tutors based on user specifications derived from the preceding conversation history and awareness of accompanying learning materials (like a syllabus).\n",
        "\n",
        "Your Goal: To generate a system prompt for an AI Tutor that accurately reflects the user's desired teaching style, personality, depth preferences, and subject matter discussed in the conversation. This generated prompt must conclude with a simple introductory phrase followed immediately by the {{SYLLABUS_SECTION}} placeholder, where the actual learning syllabus will be inserted later.\n",
        "\n",
        "Context You Will Use:\n",
        "\n",
        "Conversation History: Analyze the entire preceding conversation with the user. Pay close attention to their explicit requests and implicit preferences regarding:\n",
        "\n",
        "Teaching style (e.g., enthusiastic, patient, rigorous, Socratic).\n",
        "\n",
        "Personality influences (e.g., specific educators like Feynman/Karpathy, general traits like humorous/formal/serious).\n",
        "\n",
        "Focus areas (e.g., intuition, practical code, theory, problem-solving).\n",
        "\n",
        "Interaction dynamics (e.g., level of questioning, guidance vs. direct answers).\n",
        "\n",
        "Desired adaptability and depth control (e.g., the \"Levels of Explanation\" idea).\n",
        "\n",
        "Syllabus Mention: Infer from the conversation that a specific learning syllabus will be provided to the final AI Tutor.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Synthesize the user's requirements from the conversation history into a coherent and actionable system prompt for the target AI Tutor.\n",
        "\n",
        "The Generated Prompt MUST Include (in this order):\n",
        "\n",
        "Clear Persona Definition: Start with a concise statement defining the AI Tutor's name (create one like 'Synapse', 'GuideBot', 'LearnSpark' if none is suggested), its subject specialization (inferred from the conversation/syllabus mention), and its core mission.\n",
        "\n",
        "Core Principles Section: Detail the fundamental aspects of the tutor's personality and teaching philosophy, directly reflecting the user's preferences identified in the conversation history. Use bullet points for clarity. Incorporate specifics like desired traits, inspirational figures (and how to emulate them), and key emphasis areas.\n",
        "\n",
        "Teaching Approach / Methodology Section: Outline the specific methods the tutor should use. This must address:\n",
        "\n",
        "Clarity and Explanation Style (e.g., analogies, first principles).\n",
        "\n",
        "Interaction Style (e.g., probing questions, checks for understanding, hints).\n",
        "\n",
        "Handling Depth (e.g., adaptive levels, gauging understanding, offering detail choices).\n",
        "\n",
        "Practical Elements (e.g., code usage, examples, tools).\n",
        "\n",
        "Guidance vs. Direct Answers balance.\n",
        "\n",
        "Overall Goal Statement: Include a sentence summarizing the ultimate aim of the AI Tutor (e.g., \"Your goal is to foster deep understanding...\").\n",
        "\n",
        "Syllabus Introduction and Placeholder (MANDATORY LAST ELEMENT): The generated prompt must end precisely with a simple introductory phrase like \"Here is the syllabus we will follow:\", followed immediately by the placeholder {{SYLLABUS_SECTION}}. There should be no text, formatting, or additional instructions after this placeholder. Example ending:\n",
        "\n",
        "...Your ultimate goal is to make learning X an exciting and rewarding journey.\n",
        "\n",
        "Here is the syllabus we will follow:\n",
        "{{SYLLABUS_SECTION}}\n",
        "\n",
        "\n",
        "(Ensure the phrasing is natural and leads directly into the syllabus content).\n",
        "\n",
        "Instructions for You (The Persona Architect):\n",
        "\n",
        "Infer and Synthesize: Base your generated prompt solely on the preceding conversation history. Extract the user's needs accurately.\n",
        "\n",
        "Be Specific and Actionable: Translate user preferences into clear, direct instructions for the final AI Tutor in sections 1-4.\n",
        "\n",
        "Cohesive Persona: Ensure all parts of the generated prompt (sections 1-4) work together to create a consistent and believable tutor persona.\n",
        "\n",
        "Strict Final Structure: Adhere strictly to placing the simple introductory phrase and the {{SYLLABUS_SECTION}} placeholder as the absolute final elements of your output. Keep the intro phrase brief and direct.\n",
        "\n",
        "Output Format: Produce only the final, complete system prompt for the AI Tutor, ending exactly with the introductory phrase and {{SYLLABUS_SECTION}}. Do not include any explanatory text before or after the generated prompt itself.\n",
        "\"\"\"\n",
        "# Provide Examples for the Prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqgFrLkkEYzu"
      },
      "outputs": [],
      "source": [
        "# The method of passing information into the system prompt is more accurate but as data Increases this might cause Issues.\n",
        "INITIAL_SUMMARY_PROMPT =  \"\"\"You are an AI Resource Analyzer. Your task is to perform a detailed initial analysis of each provided learning resource excerpt. This analysis will be provided to another AI (a Conversation Manager) to give it a comprehensive understanding of the materials a user has supplied at the beginning of a conversation about creating a learning syllabus.\n",
        "\n",
        "**Input You Will Receive:**\n",
        "The user has provided the following truncated resource excerpts, formatted as a JSON object where keys are resource identifiers (e.g., filenames) and values are the truncated text content:\n",
        "{resource_excerpts_json_placeholder}\n",
        "\n",
        "**Your Task (based SOLELY on the excerpts provided above):**\n",
        "\n",
        "For EACH resource excerpt provided in the `{resource_excerpts_json_placeholder}`:\n",
        "\n",
        "1.  **Identify the Resource:** Clearly state which resource you are analyzing (e.g., \"For Resource 'file1.txt':\" or \"Analysis of 'chapter_intro.pdf_excerpt':\").\n",
        "2.  **Analyze in Depth:** Based *only* on the provided truncated excerpt:\n",
        "    *   **Primary Subject & Main Topics:** What is the main subject matter of this resource? What are the key topics or themes introduced or discussed in this excerpt?\n",
        "    *   **Key Concepts/Information:** What are some of a_i_core_concepts, arguments, definitions, or significant pieces of information presented in this excerpt?\n",
        "    *   **Apparent Content Type/Style (Optional Inference):** Briefly, what does the style or content suggest this resource might be (e.g., \"seems like an introduction to a technical textbook,\" \"appears to be a research paper abstract and introduction,\" \"reads like a practical tutorial with code examples\")?\n",
        "3.  **Format Your Output Clearly:**\n",
        "    *   Present your analysis for each resource separately. Use clear headings or delimiters for each resource (e.g., using the resource identifier).\n",
        "    *   Use natural language for your analysis.\n",
        "    *   The overall output should be a single text block containing the analyses for all resources.\n",
        "\n",
        "**Overall Goal for Your Output:**\n",
        "Your output should allow the Conversation Manager to quickly grasp the nature and primary content of each individual resource the user has brought to the table. This is more detailed than a simple one-line summary per resource.\n",
        "\n",
        "**Example Snippet of Expected Output Structure:**\n",
        "\n",
        "\"Okay, I've analyzed the provided resources:\n",
        "\n",
        "**Resource: 'Intro_to_Python_Ch1.txt'**\n",
        "This excerpt appears to be from an introductory chapter on Python programming.\n",
        "*   Main Topics: Basic Python syntax, variables, data types (integers, strings), and the print() function.\n",
        "*   Key Information: It explains what a variable is, shows examples of assigning values, and demonstrates how to output text to the console. The style is beginner-friendly.\n",
        "\n",
        "**Resource: 'Advanced_Algorithms_Paper_Excerpt.pdf'**\n",
        "This excerpt seems to be the abstract and introduction of a research paper on advanced algorithms.\n",
        "*   Main Topics: It discusses a novel approach to [specific algorithmic problem], compares it to existing methods, and outlines the paper's contributions.\n",
        "*   Key Information: It mentions concepts like [Algorithm X], [Complexity Class Y], and aims to prove [Theorem Z]. The language is formal and academic.\n",
        "--- (and so on for other resources) ---\"\n",
        "\n",
        "**DO NOT:**\n",
        "*   Attempt to create a syllabus.\n",
        "*   Merge the analyses into a single paragraph; keep resource analyses distinct.\n",
        "*   Provide an extremely brief, superficial summary (go into a bit more depth per resource as outlined above).\n",
        "*   Output raw JSON as your final response (though you are parsing JSON input).\n",
        "\"\"\"\n",
        "\n",
        "def resources_intro(resource_list:list):\n",
        "  if resource_list:\n",
        "    extracted_data_dict= {}\n",
        "    extracted_basedata_dict = {}\n",
        "\n",
        "    print(f\"[System: Formatting {len(resource_list)} resource summaries into the system prompt.]\")\n",
        "    try:\n",
        "      extracted_data_dict = {path: extract_text_from_txt_file(path) for path in resource_list}\n",
        "      chars_length = 0\n",
        "      extracted_basedata_dict ={}\n",
        "\n",
        "      for full_path,chars in extracted_data_dict.items():\n",
        "\n",
        "        chars_length += len(chars)\n",
        "        base_filename = os.path.basename(full_path)\n",
        "        extracted_basedata_dict[base_filename] = chars[:20000]\n",
        "      # Pass 20000 chars to the llm\n",
        "      print(extracted_basedata_dict)\n",
        "      llm_response = llm_call(\n",
        "        prompt=\"Generate Summary Based on the  Provided Truncated Resources\",\n",
        "        chat_history=[],\n",
        "        system_prompt= INITIAL_SUMMARY_PROMPT.format(resource_excerpts_json_placeholder = extracted_basedata_dict ))\n",
        "\n",
        "      print(llm_response)\n",
        "\n",
        "      return llm_response\n",
        "\n",
        "\n",
        "\n",
        "    except (TypeError, KeyError) as e:\n",
        "      print(f\"ERROR: Could not format resource_list into system prompt: {e}\")\n",
        "      print(\"[Couldn't Create Initial Summary.]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFKnkfS6O4F7"
      },
      "outputs": [],
      "source": [
        "learningstyle_prompt = \"\"\" You are an AI assistant specializing in understanding user learning preferences.\n",
        "Your goal is to formulate a concise and engaging question for the user. This question should prompt them to describe their preferred learning style and the kind of AI tutor personality they would find most effective for the subject matter discussed in the provided conversation history.\n",
        "\n",
        "Instructions:\n",
        "1. Analyze the complete conversation history provided. Pay close attention to the finalized syllabus (if available), the user's stated goals, and the subject they want to learn.\n",
        "2. Based on this analysis, craft a single, clear question.\n",
        "3. The question should encourage the user to provide specific details about their preferences, going beyond generic answers. For example, it could touch upon their preferred interaction style, content format (e.g., examples, theory, analogies), pace, or the kind of feedback they find helpful.\n",
        "4. Ensure the question is phrased naturally and invites a thoughtful response.\n",
        "5. Output ONLY the question itself.\n",
        "\n",
        "\"\"\"\n",
        "# This is Just a Sample Prompt. temperature - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KUmPv2ADt5y"
      },
      "outputs": [],
      "source": [
        "resource_list = created_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLza91C4Khwb"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, List, Dict, Any # Ensure typing is imported\n",
        "\n",
        "from typing import Optional, Tuple, List, Dict, Any # Ensure typing is imported\n",
        "\n",
        "def negotiate_syllabus_chat_dynamic(verbose: bool = True) -> Optional[Tuple[str, List[Dict[str, Any]]]]:\n",
        "\n",
        "\n",
        "    conversation_history: List[Dict[str, Any]] = []\n",
        "    # ---\n",
        "\n",
        "    current_syllabus_content: Optional[str] = None # Store inner content only\n",
        "    final_syllabus_xml: Optional[str] = None\n",
        "    finalization_requested = False # State flag\n",
        "\n",
        "    # --- Initial Greeting (Always happens now) ---\n",
        "    if verbose:\n",
        "        # No \"Resuming\" message anymore, always starts fresh\n",
        "        print(\"\\n--- Starting Syllabus Negotiation ---\")\n",
        "        print(\"AI: Hello! What topic are you interested in learning about today?\")\n",
        "    ai_turn = \"Hello! What topic are you interested in learning about today?\"\n",
        "    conversation_history.append({'role': 'model', 'parts': [ai_turn]})\n",
        "\n",
        "    # ---\n",
        "\n",
        "    while True:\n",
        "        # --- User Turn ---\n",
        "        try:\n",
        "            user_input = input(\"You: \")\n",
        "        except EOFError:\n",
        "            if verbose: print(\"\\nAI: Session ended by user (EOF).\")\n",
        "            return None # Signal premature end\n",
        "\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            if verbose: print(\"AI: Okay, ending the syllabus planning as requested. Goodbye!\")\n",
        "            return None # Signal user quit\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        conversation_history.append({'role': 'user', 'parts': [user_input]})\n",
        "        if len(conversation_history)<=2 and len(resource_list) >0:\n",
        "          # Create Resource Summary\n",
        "          initial_summary = resources_intro(resource_list)\n",
        "          conversation_history.append({'role': 'model', 'parts': [initial_summary]})\n",
        "\n",
        "\n",
        "\n",
        "        try:\n",
        "\n",
        "            manager_response = llm_call(\n",
        "                prompt=user_input,\n",
        "                chat_history=conversation_history,\n",
        "                system_prompt=convo_manager_system_prompt_explicit\n",
        "            )\n",
        "\n",
        "            is_tag_only = manager_response.strip().startswith('<') and manager_response.strip().endswith('/>')\n",
        "            if verbose and not is_tag_only:\n",
        "                print(f\"AI: {manager_response}\")\n",
        "            elif verbose and is_tag_only:\n",
        "                print(f\"[System: AI signaled {manager_response.strip()}]\")\n",
        "\n",
        "            conversation_history.append({'role': 'model', 'parts': [manager_response]})\n",
        "\n",
        "            intent = get_intent(manager_response)\n",
        "            # if verbose: print(f\"[System Debug: Intent={intent}]\")\n",
        "            if verbose: print(f\"DEBUG: Recognized Intent: '{intent}'\") # Enclose in quotes for clarity\n",
        "\n",
        "            if intent == \"GENERATE\" or intent == \"MODIFY\":\n",
        "                action = \"generation\" if intent == \"GENERATE\" else \"modification\"\n",
        "                if verbose: print(f\"[System: Requesting syllabus {action}...]\")\n",
        "                generated_content = generate_syllabus_from_chat(conversation_history,resource_list= resource_list)\n",
        "                if generated_content and not generated_content.startswith(\"[ERROR\"):\n",
        "                    current_syllabus_content = generated_content\n",
        "                    syllabus_xml_for_display = f\"<syllabus>\\n{current_syllabus_content}\\n</syllabus>\"\n",
        "                    if verbose:\n",
        "                         print(f\"\\n[System presenting syllabus requested by AI]\\n{syllabus_xml_for_display}\\n\")\n",
        "                    conversation_history.append({'role': 'model', 'parts': syllabus_xml_for_display})\n",
        "                    follow_up_prompt = \"The syllabus has been presented. Ask the user for feedback.\"\n",
        "                    manager_feedback_request = llm_call(\n",
        "                        prompt=follow_up_prompt, # Manager relies on history state (seeing the presented syllabus)\n",
        "                        chat_history=conversation_history, # History now includes the presented syllabus\n",
        "                        system_prompt=convo_manager_system_prompt_explicit)\n",
        "                    conversation_history.append({'role': 'model', 'parts': [manager_feedback_request]})\n",
        "                    print({'role': 'model', 'parts': [manager_feedback_request]})\n",
        "\n",
        "                else:\n",
        "                    error_msg = f\"AI: Sorry, I encountered an error during syllabus {action}. {generated_content or 'Details unknown.'}\"\n",
        "                    if verbose: print(error_msg)\n",
        "                    conversation_history.append({'role': 'model', 'parts': [error_msg]})\n",
        "\n",
        "\n",
        "            elif intent == \"FINALIZE\":\n",
        "              if verbose: print(\"[System: Finalization requested.]\")\n",
        "              if current_syllabus_content:\n",
        "                  final_syllabus_xml = f\"<syllabus>\\n{current_syllabus_content}\\n</syllabus>\"\n",
        "                  finalization_requested = True\n",
        "                  # Syallabus has been Finalized now ask about persona\n",
        "\n",
        "                  follow_up_prompt = \"The syllabus has been finalized . in what way you want to be teached? - asked to the user by model make it accordingly\"\n",
        "                  # Should use learningstyle_prompt\n",
        "                  manager_feedback_request = llm_call(\n",
        "                      prompt=follow_up_prompt,\n",
        "                      chat_history=conversation_history,\n",
        "                      system_prompt=learningstyle_prompt,\n",
        "                      temperature=1)\n",
        "                  conversation_history.append({'role': 'model', 'parts': [manager_feedback_request]})\n",
        "\n",
        "                  # ---- CHANGE THIS LINE ----\n",
        "                  # print({'role': 'model', 'parts': [manager_feedback_request]}) # Old way (raw dictionary)\n",
        "                  if verbose: print(f\"AI: {manager_feedback_request}\") # New way (formatted output)\n",
        "                  # ---- END CHANGE ----\n",
        "\n",
        "                  if verbose: print(\"[System: Syllabus content captured. Waiting for learning style.]\") # Keep this if desired\n",
        "              else:\n",
        "                  if verbose: print(\"[System Warning: Finalization requested, but no syllabus content found.]\")\n",
        "            elif intent == \"PERSONA\":\n",
        "              if verbose: print(\"[System: Persona tag received.]\")\n",
        "\n",
        "              # --- ADD THESE DEBUG LINES ---\n",
        "              if verbose:\n",
        "                  print(f\"DEBUG (PERSONA): finalization_requested = {finalization_requested}\")\n",
        "                  print(f\"DEBUG (PERSONA): final_syllabus_xml is set = {bool(final_syllabus_xml)}\")\n",
        "                  if final_syllabus_xml is None:\n",
        "                    final_syllabus_xml = f\"<syllabus>\\n{current_syllabus_content}\\n</syllabus>\"\n",
        "                    return final_syllabus_xml,conversation_history\n",
        "                  return final_syllabus_xml, conversation_history\n",
        "              # --- END OF ADDED DEBUG LINES ---\n",
        "\n",
        "              # Check if conditions met BEFORE returning\n",
        "              if finalization_requested and final_syllabus_xml:\n",
        "                  if verbose: print(\"[System: Syllabus finalized and learning style captured. Negotiation complete.]\")\n",
        "                  # SUCCESS: Return the final syllabus and the history\n",
        "                  return final_syllabus_xml, conversation_history # <<< EXIT POINT\n",
        "              else:\n",
        "                  # This part runs if the check fails\n",
        "                  if verbose: print(\"[System Warning: Persona tag received, but pre-conditions (finalization_requested AND final_syllabus_xml set) were not met. Loop continues.]\")\n",
        "\n",
        "            elif intent == \"CONVERSE\":\n",
        "                pass # Already printed if verbose and not tag-only\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"[System Error: An error occurred in the main loop: {e}]\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            conversation_history.append({'role': 'model', 'parts': [\"[System Error during AI turn. Ending negotiation.]\"]})\n",
        "            return None # Signal error\n",
        "\n",
        "    # Fallback\n",
        "    if verbose: print(\"[System: Negotiation loop exited unexpectedly.]\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQkiuUpr5JD7"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WfAE2cdBfZ4"
      },
      "outputs": [],
      "source": [
        "#k = resources_intro(created_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0KZtQEJKswq",
        "outputId": "c13c2284-5ba3-4bd3-a451-427a7b2fb58f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Welcome to the AI Learning Assistant! ---\n",
            "\n",
            "--- Phase 1: Planning Your Syllabus ---\n",
            "\n",
            "--- Starting Syllabus Negotiation ---\n",
            "AI: Hello! What topic are you interested in learning about today?\n",
            "[System: Formatting 1 resource summaries into the system prompt.]\n",
            "{'2104.08691v2 (1).txt': 'The Power of Scale for Parameter-Efﬁcient Prompt Tuning\\nBrian Lester\\x03Rami Al-Rfou Noah Constant\\nGoogle Research\\n{brianlester,rmyeid,nconstant}@google.com\\nAbstract\\nIn this work, we explore “prompt tuning,”\\na simple yet effective mechanism for learn-\\ning “soft prompts” to condition frozen lan-\\nguage models to perform speciﬁc downstream\\ntasks. Unlike the discrete text prompts used by\\nGPT-3, soft prompts are learned through back-\\npropagation and can be tuned to incorporate\\nsignals from any number of labeled examples.\\nOur end-to-end learned approach outperforms\\nGPT-3’s few-shot learning by a large margin.\\nMore remarkably, through ablations on model\\nsize using T5, we show that prompt tuning be-\\ncomes more competitive with scale: as mod-\\nels exceed billions of parameters, our method\\n“closes the gap” and matches the strong per-\\nformance of model tuning (where all model\\nweights are tuned). This ﬁnding is especially\\nrelevant because large models are costly to\\nshare and serve and the ability to reuse one\\nfrozen model for multiple downstream tasks\\ncan ease this burden. Our method can be seen\\nas a simpliﬁcation of the recently proposed\\n“preﬁx tuning” of Li and Liang (2021) and we\\nprovide a comparison to this and other similar\\napproaches. Finally, we show that condition-\\ning a frozen model with soft prompts confers\\nbeneﬁts in robustness to domain transfer and\\nenables efﬁcient “prompt ensembling.”\\n1 Introduction\\nWith the wide success of pre-trained large lan-\\nguage models, a range of techniques has arisen to\\nadapt these general-purpose models to downstream\\ntasks. ELMo (Peters et al., 2018) proposed freezing\\nthe pre-trained model and learning a task-speciﬁc\\nweighting of its per-layer representations. How-\\never, since GPT (Radford et al., 2018) and BERT\\n(Devlin et al., 2019), the dominant adaptation tech-\\nnique has been model tuning (or “ﬁne-tuning”),\\nwhere all model parameters are tuned during adap-\\ntation, as proposed by Howard and Ruder (2018).\\n\\x03Work done as a Google AI Resident.\\n10810910101011\\nModel Parameters5060708090100SuperGLUE Score\\nModel Tuning\\nModel Tuning (Multi-task)Prompt Design\\nPrompt TuningFigure 1: Standard model tuning of T5 achieves strong\\nperformance, but requires storing separate copies of the\\nmodel for each end task. Our prompt tuning of T5\\nmatches the quality of model tuning as size increases,\\nwhile enabling the reuse of a single frozen model for\\nall tasks. Our approach signiﬁcantly outperforms few-\\nshot prompt design using GPT-3. We show mean and\\nstandard deviation across 3runs for tuning methods.\\nMore recently, Brown et al. (2020) showed that\\nprompt design (or “priming”) is surprisingly effec-\\ntive at modulating a frozen GPT-3 model’s behavior\\nthrough text prompts. Prompts are typically com-\\nposed of a task description and/or several canonical\\nexamples. This return to “freezing” pre-trained\\nmodels is appealing, especially as model size con-\\ntinues to increase. Rather than requiring a separate\\ncopy of the model for each downstream task, a\\nsingle generalist model can simultaneously serve\\nmany different tasks.\\nUnfortunately, prompt-based adaptation has sev-\\neral key drawbacks. Task description is error-prone\\nand requires human involvement, and the effective-\\nness of a prompt is limited by how much condition-\\ning text can ﬁt into the model’s input. As a result,\\ndownstream task quality still lags far behind that\\nof tuned models. For instance, GPT-3 175B few-\\nshot performance on SuperGLUE is 17:5points be-arXiv:2104.08691v2  [cs.CL]  2 Sep 2021\\nPre-trained \\nModel \\n(11B params) \\nTask A Model \\n(11B params) \\nTask B Model \\n(11B params) \\nTask C Model \\n(11B params) a1\\na2\\nb1\\nc1\\nc2Task A \\nBatch \\nTask B \\nBatch \\nTask C \\nBatch Pre-trained \\nModel \\n(11B params) Model Tuning Prompt Tuning \\nA\\nB\\nCMixed-task \\nBatch \\n(20K params each) a1\\nc1\\nb1\\na2\\nc2A\\nC\\nB\\nA\\nC\\nTask Prompts Figure 2: Model tuning requires making a task-\\nspeciﬁc copy of the entire pre-trained model for each\\ndownstream task and inference must be performed in\\nseparate batches. Prompt tuning only requires stor-\\ning a small task-speciﬁc prompt for each task, and\\nenables mixed-task inference using the original pre-\\ntrained model. With a T5 “XXL” model, each copy\\nof the tuned model requires 11billion parameters. By\\ncontrast, our tuned prompts would only require 20;480\\nparameters per task—a reduction of over ﬁve orders of\\nmagnitude —assuming a prompt length of 5tokens.\\nlow ﬁne-tuned T5-XXL (Raffel et al., 2020) ( 71:8\\nvs.89:3) despite using 16times more parameters.\\nSeveral efforts to automate prompt design have\\nbeen recently proposed. Shin et al. (2020) propose\\na search algorithm over the discrete space of words,\\nguided by the downstream application training data.\\nWhile this technique outperforms manual prompt\\ndesign, there is still a gap relative to model tuning.\\nLi and Liang (2021) propose “preﬁx tuning”\\nand show strong results on generative tasks. This\\nmethod freezes the model parameters and back-\\npropagates the error during tuning to preﬁx ac-\\ntivations prepended to each layer in the encoder\\nstack, including the input layer. Hambardzumyan\\net al. (2021) simplify this recipe by restricting the\\ntrainable parameters to the input and output sub-\\nnetworks of a masked language model, and show\\nreasonable results on classiﬁcations tasks.\\nIn this paper, we propose prompt tuning as a\\nfurther simpliﬁcation for adapting language models.\\nWe freeze the entire pre-trained model and only al-\\nlow an additional ktunable tokens per downstream\\ntask to be prepended to the input text. This “soft\\nprompt” is trained end-to-end and can condense\\nthe signal from a full labeled dataset, allowing our\\nmethod to outperform few-shot prompts and close\\nthe quality gap with model tuning (Figure 1). At\\nthe same time, since a single pre-trained model is\\nrecycled for all downstream tasks, we retain the ef-\\nﬁcient serving beneﬁts of frozen models (Figure 2).\\nWhile we developed our method concurrentlywith Li and Liang (2021) and Hambardzumyan\\net al. (2021), we are the ﬁrst to show that prompt\\ntuning alone (with no intermediate-layer preﬁxes or\\ntask-speciﬁc output layers) is sufﬁcient to be com-\\npetitive with model tuning. Through detailed ex-\\nperiments in sections 2–3, we demonstrate that lan-\\nguage model capacity is a key ingredient for these\\napproaches to succeed. As Figure 1 shows, prompt\\ntuning becomes more competitive with scale.\\nWe compare with similar approaches in Sec-\\ntion 4. Explicitly separating task-speciﬁc param-\\neters from the “generalist” parameters needed for\\ngeneral language-understanding has a range of ad-\\nditional beneﬁts. We show in Section 5 that by\\ncapturing the task deﬁnition in the prompt while\\nkeeping the generalist parameters ﬁxed, we are able\\nto achieve better resilience to domain shifts. In Sec-\\ntion 6, we show that “prompt ensembling”, learn-\\ning multiple prompts for the same task, can boost\\nquality and is more efﬁcient than classic model en-\\nsembling. Finally, in Section 7, we investigate the\\ninterpretability of our learned soft prompts. In sum,\\nour key contributions are:\\n1.Proposing prompt tuning and showing its com-\\npetitiveness with model tuning in the regime\\nof large language models.\\n2.Ablating many design choices, and showing\\nquality and robustness improve with scale.\\n3.Showing prompt tuning outperforms model\\ntuning on domain shift problems.\\n4.Proposing “prompt ensembling” and showing\\nits effectiveness.\\n2 Prompt Tuning\\nFollowing the “text-to-text” approach of T5 (Raffel\\net al., 2020), we cast all tasks as text generation.\\nInstead of modeling classiﬁcation as the probabil-\\nity of an output class given some input, Pr(yjX),\\nwhereXis a series of tokens and yis a single class\\nlabel, we now model it as conditional generation,\\nwhereYis a sequence of tokens that represent a\\nclass label. T5 models classiﬁcation as Pr\\x12(YjX),\\nparameterized by the weights, \\x12, of the transform-\\ners (Vaswani et al., 2017) that make up its encoder\\nand decoder.\\nPrompting is the approach of adding extra in-\\nformation for the model to condition on during its\\ngeneration of Y. Normally, prompting is done\\nby prepending a series of tokens, P, to the in-\\nputX, such that the model maximizes the likeli-\\nhood of the correct Y,Pr\\x12(Yj[P;X]), while keep-\\ning the model parameters, \\x12, ﬁxed. In GPT-3 ,\\nthe representations of the prompt tokens, P=\\nfp1;p2;:::;png, are part of the model’s embed-\\nding table, parameterized by the frozen \\x12. Find-\\ning an optimal prompt thus requires the selection\\nof prompt tokens, through either manual search\\nor non-differentiable search methods (Jiang et al.,\\n2020; Shin et al., 2020). Prompt tuning removes\\nthe restriction that the prompt Pbe parameterized\\nby\\x12; instead the prompt has its own dedicated pa-\\nrameters,\\x12P, that can be updated. While prompt\\ndesign involves selecting prompt tokens from a\\nﬁxed vocabulary of frozen embeddings, prompt\\ntuning can be thought of as using a ﬁxed prompt\\nof special tokens, where only the embeddings of\\nthese prompt tokens can be updated. Our new con-\\nditional generation is now Pr\\x12;\\x12P(Yj[P;X])and\\ncan be trained by maximizing the likelihood of Y\\nvia backpropagation, while only applying gradient\\nupdates to\\x12P.\\nGiven a series of ntokens,fx1;x2;:::;xng, the\\nﬁrst thing T5 does is embed the tokens, forming\\na matrixXe2Rn\\x02ewhereeis the dimension of\\nthe embedding space. Our soft-prompts are repre-\\nsented as a parameter Pe2Rp\\x02e, wherepis the\\nlength of the prompt. Our prompt is then concate-\\nnated to the embedded input forming a single ma-\\ntrix[Pe;Xe]2R(p+n)\\x02ewhich then ﬂows though\\nthe encoder-decoder as normal. Our models are\\ntrained to maximize the probability of Y, but only\\nthe prompt parameters Peare updated.\\n2.1 Design Decisions\\nThere are many possible ways to initialize the\\nprompt representations. The simplest is to train\\nfrom scratch, using random initialization. A more\\nsophisticated option is to initialize each prompt\\ntoken to an embedding drawn from the model’s\\nvocabulary. Conceptually, our soft-prompt mod-\\nulates the frozen network’s behavior in the same\\nway as text preceding the input, so it follows that\\na word-like representation might serve as a good\\ninitialization spot. For classiﬁcation tasks, a third\\noption is to initialize the prompt with embeddings\\nthat enumerate the output classes, similar to the\\n“verbalizers” of Schick and Schütze (2021). Since\\nwe want the model to produce these tokens in the\\noutput, initializing the prompt with the embeddings\\nof the valid target tokens should prime the model\\nto restrict its output to the legal output classes.\\nAnother design consideration is the length of theprompt. The parameter cost of our method is EP,\\nwhereEis the token embedding dimension and P\\nis the prompt length. The shorter the prompt, the\\nfewer new parameters must be tuned, so we aim to\\nﬁnd a minimal length that still performs well.\\n2.2 Unlearning Span Corruption\\nUnlike autoregressive language models like GPT-3 ,\\nthe T5 models we experiment with use an encoder-\\ndecoder architecture and pre-train on a span cor-\\nruption objective. Speciﬁcally, T5 is tasked with\\n“reconstructing” masked spans in the input text,\\nwhich are marked with unique sentinel tokens. The\\ntarget output text consists of all the masked con-\\ntent, separated by sentinels, plus a ﬁnal sentinel.\\nFor instance, from the text “Thank you for inviting\\nme to your party last week” we might construct\\na pre-training example where the input is “Thank\\nyouhXime to your partyhYiweek” and the target\\noutput is “hXifor invitinghYilasthZi”.\\nWhile Raffel et al. (2020) ﬁnd this architecture\\nand pre-training objective more effective than tradi-\\ntional language modeling, we hypothesize that this\\nsetup is not a good ﬁt for producing a frozen model\\nthat can be readily controlled through prompt tun-\\ning. In particular, a T5 model pre-trained exclu-\\nsively on span corruption, such as T5.1.1, has never\\nseen truly natural input text (free of sentinel to-\\nkens), nor has it ever been asked to predict truly\\nnatural targets. In fact, due to the details of T5’s\\nspan corruption preprocessing, every pre-training\\ntarget will begin with a sentinel. While this “unnat-\\nural” tendency to output sentinels is easy to over-\\ncome through ﬁne-tuning, we suspect that it would\\nbe much harder to override through a prompt alone,\\nas the decoder priors cannot be adjusted.\\nGiven these concerns, we experiment with T5\\nmodels in three settings. (1) “Span Corruption”:\\nWe use pre-trained T5 off-the-shelf as our frozen\\nmodel, and test its ability to output the expected\\ntext for downstream tasks. (2) “Span Corruption\\n+ Sentinel”: We use the same model, but prepend\\nall downstream targets with a sentinel, so as to\\nmore closely resemble the targets seen in pre-\\ntraining. (3) “LM Adaptation”: We continue T5’s\\nself-supervised training for a small number of ad-\\nditional steps, but using the “LM” objective dis-\\ncussed by Raffel et al. (2020); given a natural text\\npreﬁx as input, the model must produce the natural\\ntext continuation as output. Crucially, this adapta-\\ntion happens only once , producing a single frozen\\nmodel that we can reuse for prompt tuning across\\nany number of downstream tasks.\\nThrough LM adaptation, we hope to “quickly”\\ntransform T5 into a model more similar to GPT-3 ,\\nwhich always outputs realistic text, and is known to\\nrespond well to prompts as a “few-shot learner”. It\\nis not obvious how successful this late-stage trans-\\nformation will be compared to pre-training from\\nscratch, and it has not been investigated previously\\nto our knowledge. As such, we experiment with\\nvarious lengths of adaptation up to 100K steps.\\n3 Results\\nOur frozen models are built on top of pre-trained\\nT5 checkpoints of all sizes (Small, Base, Large, XL,\\nXXL). We leverage the public T5.1.1 checkpoints,\\nwhich include improvements over the original T5.1\\nOur “default” conﬁguration, plotted with a green\\n‘\\x02’ (\\n ) throughout, uses an LM-adapted version\\nof T5 trained for an additional 100K steps, ini-\\ntializes using class labels (see Section 3.2), and\\nuses a prompt length of 100tokens. While this\\nis longer than the default 10-token preﬁx used by\\nLi and Liang (2021), our method still uses fewer\\ntask-speciﬁc parameters, as we only tune the input\\nlayer, as opposed to overwriting activations in all\\nnetwork layers. See Figure 4 for a detailed com-\\nparison. We will also see shortly that even much\\nshorter prompts are viable as model size increases.\\nWe measure performance on the SuperGLUE\\nbenchmark (Wang et al., 2019a), a collection of\\neight challenging English language understanding\\ntasks.2We report metrics on the development set\\nassociated with each dataset.\\nEach of our prompts train on a single Super-\\nGLUE task; there was no multi-task setup or mix-\\ning of training data across tasks. We translate each\\nSuperGLUE dataset into a text-to-text format fol-\\nlowing Raffel et al. (2020), except that we omit the\\ntask names prepended to inputs indicating which\\nSuperGLUE task an example belongs to.\\nWe train our prompts for 30;000steps using T5’s\\nstandard cross-entropy loss, with a constant learn-\\n1These improvements are (1) the removal of all supervised\\ndata from pre-training, (2) adjustments to hyperparameters\\ndmodel anddff, and (3) the use of GeGLU (Shazeer, 2020)\\nover ReLU (Nair and Hinton, 2010) activations.\\n2The tasks are BoolQ (Clark et al., 2019), CB (De Marn-\\neff et al., 2019), COPA (Roemmele et al., 2011), MultiRC\\n(Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE\\n(Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al.,\\n2007; Bentivogli et al., 2009), WiC (Pilehvar and Camacho-\\nCollados, 2018), and WSC (Levesque et al., 2012).ing rate of 0:3and a batch size of 32. Checkpoints\\nare selected via early stopping on the development\\nset, where the stopping metric is the default met-\\nric for the dataset, or the average of metrics for\\ndatasets evaluated with multiple metrics. All ex-\\nperiments were run in JAX (Bradbury et al., 2018)\\nusing the Adafactor optimizer (Shazeer and Stern,\\n2018) with weight decay 1e\\x005,\\x0c2decay 0:8, and\\nparameter scaling off. The models were imple-\\nmented in Flax (Heek et al., 2020). More details\\nare available in Appendix A.\\n3.1 Closing the Gap\\nTo compare our method with standard model tun-\\ning, we tune the public T5.1.1 checkpoints on\\nSuperGLUE using the default hyperparameters\\nspeciﬁed in the T5 library (learning rate 0:001,\\nand Adafactor optimizer with pre-training param-\\neter states restored). We consider two baselines.\\n(1) “Model Tuning”: For an apples-to-apples com-\\nparison, we tune on each task separately, as in our\\nprompt tuning setup.3(2) “Model Tuning (Multi-\\ntask)”: We use T5’s multi-task tuning setup to\\nachieve a more competitive baseline.4In this case,\\na single model is tuned on all tasks jointly, with a\\ntext preﬁx indicating the task name.\\nIn Figure 1 (p. 1), we see that prompt tuning\\nbecomes more competitive with model tuning as\\nscale increases. At the XXL size (11 billion param-\\neters), prompt tuning matches even the stronger\\nmulti-task model tuning baseline, despite having\\nover20;000times fewer task-speciﬁc parameters.\\nTo compare with prompt design, we include\\nGPT-3 few-shot performance on the SuperGLUE\\ndev split, as reported by Brown et al. (2020).5\\nFigure 1 shows that prompt tuning beats GPT-3\\nprompt design by a large margin, with prompt-\\ntuned T5-Small matching GPT-3 XL (over 16\\ntimes larger), and prompt-tuned T5-Large beating\\nGPT-3 175B (over 220times larger).\\n3To improve this baseline, we performed a sweep over the\\nbatch size hyperparameter and selected 216tokens per batch.\\n4The T5 SuperGLUE submission used a more complex\\nsetup, ﬁrst mixing multi-task supervised data into pre-training,\\nand then performing single-task ﬁne-tuning. Since we use\\nT5.1.1 throughout, this setup is unavailable, as the pre-training\\nphase is fully self-supervised. We follow Raffel et al. (2020)\\nin using 220tokens per batch and including DPR data in\\nthe multi-task mixture, which is known to boost WSC task\\nperformance (Kocijan et al., 2019).\\n5We also experimented with using GPT-3’s manual text\\nprompts directly with our LM-adapted T5 checkpoints. How-\\never performance was far below GPT-3 for comparable model\\nsizes. This may be due to differences in pre-training data and\\nmodel architecture, as well as T5’s shorter sequence length.\\n1081091010\\nModel Parameters5060708090100SuperGLUE Score\\n1\\n5\\n20\\n100\\n150(a) Prompt length\\n1081091010\\nModel Parameters5060708090100SuperGLUE Score\\nRandom Uniform\\nSampled Vocab\\nClass Label (b) Prompt initialization\\n1081091010\\nModel Parameters102030405060708090100SuperGLUE Score\\nSpan Corruption\\nSpan Corruption\\n+ Sentinel\\nLM Adaptation\\n(100K)\\n(c) Pre-training method\\n1081091010\\nModel Parameters102030405060708090100SuperGLUE Score\\n0K\\n10K\\n50K\\n100K (d) LM adaptation steps\\nFigure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3runs). In\\nour “default” (\\n ) conﬁguration, quality improves stably with model size. Across all ablations, the largest (XXL)\\nmodel is the most robust to hyperparameter choice . (a) Prompt length : Increasing to 20+ tokens generally confers\\na large boost, but XXL performs well even with single-token prompts. (b) Prompt initialization : Random uniform\\ninitialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but\\nthe difference vanishes at XXL size. (c) Pre-training objective : LM adaptation outperforms span corruption, even\\nwhen a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation :\\nLonger adaptation generally gives larger gains, but XXL is robust to even short adaptation.\\n3.2 Ablation Study\\nPrompt Length We train prompts for each\\nmodel size while varying the prompt length in\\nf1;5;20;100;150gand ﬁxing other settings to our\\ndefault conﬁguration. Figure 3(a) shows that for\\nmost model sizes, increasing prompt length beyond\\na single token is critical to achieve good perfor-\\nmance. Notably, the XXL model still gives strong\\nresults with a single-token prompt, suggesting that\\nthe larger the model,'}\n",
            "\n",
            "--- LLM Call #1 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using single prompt (no chat history).\n",
            "--- Response Received (Call #1) ---\n",
            "Token Usage (Call #1): Prompt=12229, Completion=461, Total=18511\n",
            "Okay, I've analyzed the provided resources:\n",
            "\n",
            "**Resource: '2104.08691v2 (1).txt'**\n",
            "This excerpt appears to be from a research paper titled \"The Power of Scale for Parameter-Efficient Prompt Tuning\".\n",
            "*   **Primary Subject & Main Topics:** The paper focuses on \"prompt tuning\" as a parameter-efficient method for adapting large, frozen language models to downstream tasks. Key topics include comparing prompt tuning to traditional model tuning (fine-tuning) and discrete prompt design, the efficiency benefits of prompt tuning (especially with large models), robustness to domain transfer, prompt ensembling, and ablations on design choices like prompt length, initialization, and pre-training objectives (specifically for T5 models).\n",
            "*   **Key Concepts/Information:**\n",
            "    *   **Prompt Tuning:** Learning \"soft prompts\" (learned through backpropagation) to condition frozen language models.\n",
            "    *   **Efficiency:** Requires significantly fewer task-specific parameters compared to model tuning, allowing a single frozen model to serve multiple tasks.\n",
            "    *   **Scalability:** Prompt tuning performance becomes competitive with model tuning as model size increases (especially for models exceeding billions of parameters, like T5-XXL).\n",
            "    *   **Comparison Baselines:** Compared against GPT-3 few-shot prompt design (outperforms significantly) and model tuning (matches performance at scale).\n",
            "    *   **Technical Details:** Describes soft prompts as tunable token embeddings prepended to the input, trained end-to-end. Discusses design decisions like initialization methods (random, sampled vocab, class label) and prompt length.\n",
            "    *   **T5 Specifics:** Addresses challenges with T5's span corruption pre-training objective and proposes \"LM Adaptation\" as a way to make T5 more amenable to prompt tuning.\n",
            "    *   **Benefits:** Shows benefits in domain transfer robustness and introduces \"prompt ensembling\".\n",
            "*   **Apparent Content Type/Style:** Appears to be a formal academic research paper, including an abstract, introduction, methodology sections (Prompt Tuning, Design Decisions, Unlearning Span Corruption), results section (Closing the Gap, Ablation Study), figures comparing performance metrics, and citations to related work.\n",
            "\n",
            "--- LLM Call #2 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 3 previous turns.\n",
            "--- Response Received (Call #2) ---\n",
            "Token Usage (Call #2): Prompt=1367, Completion=7, Total=1593\n",
            "[System: AI signaled <request_syllabus_generation/>]\n",
            "DEBUG: Recognized Intent: 'GENERATE'\n",
            "[System: Requesting syllabus generation...]\n",
            "[System: Formatting 1 resource summaries into the system prompt.]\n",
            "\n",
            "--- LLM Call #3 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 4 previous turns.\n",
            "--- Response Received (Call #3) ---\n",
            "Token Usage (Call #3): Prompt=21131, Completion=1330, Total=22801\n",
            "--- Syllabus Extracted Successfully (from LLM response) ---\n",
            "\n",
            "[System presenting syllabus requested by AI]\n",
            "<syllabus>\n",
            "<phase number=\"1\" title=\"Understanding the Research Paper Landscape\">\n",
            "        <lesson number=\"1.1\">\n",
            "            <topic>Anatomy of a Research Paper & Initial Skimming</topic>\n",
            "            <keywords>research paper, structure, abstract, introduction, skimming, sections</keywords>\n",
            "            <objective>Identify the standard sections of an NLP research paper and practice skimming the provided paper to grasp its core topic and contribution.</objective>\n",
            "            <focus>Learn to quickly read the Abstract, Introduction (Section 1), and Conclusion (Section 8) of the Prompt Tuning paper to get the main idea (Prompt Tuning mechanism, goal: parameter efficiency, outperforming few-shot, matching fine-tuning at scale).</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"1.2\">\n",
            "            <topic>Identifying the Problem and Proposed Solution</topic>\n",
            "            <keywords>problem, solution, motivation, related work, state-of-the-art, Prompt Tuning</keywords>\n",
            "            <objective>Pinpoint the problem the Prompt Tuning paper aims to solve (inefficiencies of model tuning, limitations of discrete prompts) and understand their proposed solution (Prompt Tuning).</objective>\n",
            "            <focus>Analyze the Introduction (Section 1) to understand the context (LLMs, adaptation methods like fine-tuning, prompt design) and the specific gap or problem addressed, leading to the introduction of Prompt Tuning.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"2\" title=\"Delving into Methodology\">\n",
            "        <lesson number=\"2.1\">\n",
            "            <topic>Understanding the Proposed Method: Prompt Tuning</topic>\n",
            "            <keywords>Prompt Tuning, soft prompts, frozen language models, backpropagation, parameter-efficient</keywords>\n",
            "            <objective>Understand the core mechanism of Prompt Tuning as described in the paper.</objective>\n",
            "            <focus>Study Section 2 (\"Prompt Tuning\") to grasp how soft prompts are learned, how they are applied (prepending to embedded input), and the conceptual difference from discrete prompts (learned parameters vs. fixed vocabulary). Relate it to familiar concepts like model tuning.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"2.2\">\n",
            "            <topic>Key Design Decisions and Ablations</topic>\n",
            "            <keywords>prompt length, initialization, pre-training objective, LM Adaptation, ablation study, hyperparameters</keywords>\n",
            "            <objective>Analyze the key experimental design choices and their impact as explored in the paper's ablations.</objective>\n",
            "            <focus>Examine Section 2.1 (\"Design Decisions\") and Section 3.2 (\"Ablation Study\") and Figure 3 to understand the impact of prompt length, initialization strategy (random, sampled vocab, class label), and the importance of pre-training objective (span corruption vs. LM Adaptation) on Prompt Tuning performance across model scales.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"3\" title=\"Analyzing Results and Comparisons\">\n",
            "        <lesson number=\"3.1\">\n",
            "            <topic>Evaluating Performance & Scale</topic>\n",
            "            <keywords>SuperGLUE, model tuning, few-shot, scale, performance gap, T5-XXL</keywords>\n",
            "            <objective>Interpret the main experimental results comparing Prompt Tuning to baselines and the effect of model scale.</objective>\n",
            "            <focus>Analyze Section 3 (\"Results\") and Figure 1 to understand the \"Closing the Gap\" finding – how Prompt Tuning compares to model tuning and GPT-3 prompt design, especially as model size increases (T5-Small to XXL). Note the specific benchmarks used (SuperGLUE).</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"3.2\">\n",
            "            <topic>Comparison with Related Parameter-Efficient Methods</topic>\n",
            "            <keywords>parameter efficiency, Prefix Tuning, WARP, P-tuning, adapters, trainable parameters</keywords>\n",
            "            <objective>Compare Prompt Tuning with other recent parameter-efficient adaptation techniques mentioned in the paper.</objective>\n",
            "            <focus>Read Section 4 (\"Comparison to Similar Approaches\") and Figure 4 to understand how Prompt Tuning fits within the broader landscape of parameter-efficient methods (Prefix Tuning, WARP, P-tuning, adapters), focusing on parameter usage and key technical differences.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"4\" title=\"Understanding Implications and Future Work\">\n",
            "        <lesson number=\"4.1\">\n",
            "            <topic>Robustness and Generalization</topic>\n",
            "            <keywords>domain shift, zero-shot transfer, generalization, overfitting</keywords>\n",
            "            <objective>Understand the findings regarding Prompt Tuning's robustness to domain shifts.</objective>\n",
            "            <focus>Analyze Section 5 (\"Resilience to Domain Shift\") and Tables 1 & 2 to understand how freezing the base model and tuning only prompts affects performance on out-of-domain tasks compared to full model tuning.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"4.2\">\n",
            "            <topic>Prompt Ensembling and Interpretability</topic>\n",
            "            <keywords>prompt ensembling, inference efficiency, interpretability, soft prompt neighbors</keywords>\n",
            "            <objective>Learn about the concept of Prompt Ensembling and the challenges of interpreting soft prompts.</objective>\n",
            "            <focus>Study Section 6 (\"Prompt Ensembling\") and Table 3 to see how training multiple prompts on a single model improves performance and efficiency. Read Section 7 (\"Interpretability\") to understand the attempts and challenges in interpreting learned soft prompts.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"4.3\">\n",
            "            <topic>Synthesizing Contributions and Future Directions</topic>\n",
            "            <keywords>contributions, conclusion, limitations, future work, takeaways</keywords>\n",
            "            <objective>Summarize the key contributions of the paper and identify potential areas for future research based on the authors' discussion.</objective>\n",
            "            <focus>Re-read the Abstract and Conclusion (Section 8). Synthesize the main findings (competitiveness at scale, robustness, efficiency) and note the explicit list of contributions. Identify what the authors suggest as exciting future directions for research in this area.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "</syllabus>\n",
            "\n",
            "\n",
            "--- LLM Call #4 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 5 previous turns.\n",
            "--- Response Received (Call #4) ---\n",
            "Token Usage (Call #4): Prompt=2683, Completion=25, Total=2708\n",
            "{'role': 'model', 'parts': [\"Here's the syllabus draft based on our discussion. How does it look? Are there any changes you'd like?\"]}\n",
            "\n",
            "--- LLM Call #5 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 7 previous turns.\n",
            "--- Response Received (Call #5) ---\n",
            "Token Usage (Call #5): Prompt=2816, Completion=119, Total=3152\n",
            "AI: Okay, I understand. You want the syllabus to focus more on the *process* of reading a research paper effectively, using the Prompt Tuning paper as the material to practice that process on. You want the initial phase to cover general strategies for reading papers, and then apply those strategies step-by-step to the provided paper, with the emphasis being on *how* to read and extract information, rather than me explaining the paper's technical details.\n",
            "\n",
            "That makes sense. We can definitely adjust the syllabus to reflect that focus on the reading methodology.\n",
            "\n",
            "<request_syllabus_modification/>\n",
            "DEBUG: Recognized Intent: 'MODIFY'\n",
            "[System: Requesting syllabus modification...]\n",
            "[System: Formatting 1 resource summaries into the system prompt.]\n",
            "\n",
            "--- LLM Call #6 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 8 previous turns.\n",
            "--- Response Received (Call #6) ---\n",
            "Token Usage (Call #6): Prompt=22667, Completion=1381, Total=24777\n",
            "--- Syllabus Extracted Successfully (from LLM response) ---\n",
            "\n",
            "[System presenting syllabus requested by AI]\n",
            "<syllabus>\n",
            "<phase number=\"1\" title=\"Foundational Research Paper Reading Skills\">\n",
            "        <lesson number=\"1.1\">\n",
            "            <topic>Efficient Skimming: Abstract, Intro, and Conclusion</topic>\n",
            "            <keywords>skimming, abstract, introduction, conclusion, core idea, quick read</keywords>\n",
            "            <objective>Learn how to quickly read the Abstract, Introduction (Section 1), and Conclusion (Section 8) of the provided research paper to grasp its central topic, main contribution, and overall findings without getting bogged down in details.</objective>\n",
            "            <focus>Apply skimming techniques to identify the paper's main subject (Prompt Tuning), why it's important (parameter efficiency, scale), and the key takeaway results as summarized by the authors in these sections.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"1.2\">\n",
            "            <topic>Identifying the Problem and Proposed Solution</topic>\n",
            "            <keywords>problem statement, motivation, proposed method, novelty, research question</keywords>\n",
            "            <objective>Practice locating the specific problem the paper addresses and how the authors frame their method (Prompt Tuning) as a solution.</objective>\n",
            "            <focus>Analyze the Introduction (Section 1) to understand the background context (prior adaptation methods like fine-tuning, prompt design) and identify the stated limitations or challenges that motivated this work. Find where Prompt Tuning is first introduced as their approach.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"1.3\">\n",
            "            <topic>Navigating and Understanding Related Work</topic>\n",
            "            <keywords>related work, background, context, citations, prior art, comparison</keywords>\n",
            "            <objective>Understand the purpose of the related work discussion and how to quickly contextualize the paper's contribution against previous research.</objective>\n",
            "            <focus>Scan through parts of the Introduction (Section 1) and potentially Section 4 (\"Comparison to Similar Approaches\") where other methods (like fine-tuning, GPT-3 prompts, prefix tuning, WARP) are mentioned. Identify which prior works are discussed to understand the landscape the paper operates in, without needing to fully understand every cited paper.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"2\" title=\"Deconstructing Methodology and Experiments\">\n",
            "        <lesson number=\"2.1\">\n",
            "            <topic>Reading the Methodology Section</topic>\n",
            "            <keywords>methodology, proposed method, technical details, prompt tuning mechanism, soft prompts</keywords>\n",
            "            <objective>Learn how to read and extract the essential technical details of the proposed method from the relevant section.</objective>\n",
            "            <focus>Focus on Section 2 (\"Prompt Tuning\"). Read carefully to understand *how* Prompt Tuning works technically (e.g., soft prompt parameters, prepending to input embeddings, training objective, what is frozen/tuned). Try to identify the core steps and components described.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"2.2\">\n",
            "            <topic>Understanding Experimental Design and Setup</topic>\n",
            "            <keywords>experimental setup, datasets, models, metrics, hyperparameters, T5, SuperGLUE</keywords>\n",
            "            <objective>Identify and understand the key elements of the experimental setup used to evaluate the proposed method.</objective>\n",
            "            <focus>Read parts of Section 3 (\"Results\") and Appendix A (\"Reproducibility\") to find information about the models used (T5 sizes), the datasets (SuperGLUE tasks, SQuAD, MRQA), the evaluation metrics, and general training settings (optimizer, learning rate, batch size). Understand *what* they measured and *on what*.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"2.3\">\n",
            "            <topic>Interpreting Figures and Tables</topic>\n",
            "            <keywords>figures, tables, data visualization, interpreting results, axes, trends, captions</keywords>\n",
            "            <objective>Develop skills in extracting key findings and trends directly from the paper's figures and tables.</keywords>\n",
            "            <focus>Examine Figure 1, Figure 3, Figure 4, and Tables 1, 2, 3. Read their captions and look at the axes, data points, and trends shown. Practice articulating *what* the figure/table demonstrates about Prompt Tuning's performance, efficiency, or robustness based *only* on the visual information and captions.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"3\" title=\"Analyzing Results and Implications\">\n",
            "        <lesson number=\"3.1\">\n",
            "            <topic>Analyzing Ablation Studies</topic>\n",
            "            <keywords>ablation study, design choices, hyperparameters, prompt length, initialization, pre-training</keywords>\n",
            "            <objective>Learn how to read and interpret the results of ablation studies to understand the impact of different design choices.</objective>\n",
            "            <focus>Study Section 3.2 (\"Ablation Study\") and Figure 3. For each ablation (Prompt Length, Initialization, Pre-training Objective), identify what factor was varied and how the performance changed according to the results presented.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"3.2\">\n",
            "            <topic>Evaluating Comparisons to Baselines</topic>\n",
            "            <keywords>baselines, comparison, model tuning, prompt design, performance gap, scale effects</keywords>\n",
            "            <objective>Practice analyzing how the paper positions its results against established and alternative methods.</objective>\n",
            "            <focus>Re-examine Section 3.1 (\"Closing the Gap\") and Figure 1. Analyze the comparison between Prompt Tuning, Model Tuning (single and multi-task), and Prompt Design (GPT-3). Focus on *how* the paper describes the comparison results, especially concerning model scale.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"3.3\">\n",
            "            <topic>Identifying Broader Implications and Future Work</topic>\n",
            "            <keywords>domain shift, generalization, prompt ensembling, interpretability, future directions, discussion</keywords>\n",
            "            <objective>Extract insights into the broader advantages (beyond core performance) and limitations discussed by the authors.</objective>\n",
            "            <focus>Read Sections 5, 6, 7, and the Conclusion (Section 8). Identify the specific experiments related to domain shift and ensembling and note the high-level findings. Understand the discussion around interpretability and the authors' suggested avenues for future research presented in the conclusion.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "</syllabus>\n",
            "\n",
            "\n",
            "--- LLM Call #7 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 9 previous turns.\n",
            "--- Response Received (Call #7) ---\n",
            "Token Usage (Call #7): Prompt=4270, Completion=37, Total=4435\n",
            "{'role': 'model', 'parts': [\"Okay, here is the revised syllabus draft focusing on the process of reading the paper. How does this version look to you? Are there any further adjustments you'd like to make?\"]}\n",
            "\n",
            "--- LLM Call #8 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 11 previous turns.\n",
            "--- Response Received (Call #8) ---\n",
            "Token Usage (Call #8): Prompt=4307, Completion=6, Total=4404\n",
            "[System: AI signaled <request_finalization/>]\n",
            "DEBUG: Recognized Intent: 'FINALIZE'\n",
            "[System: Finalization requested.]\n",
            "\n",
            "--- LLM Call #9 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 12 previous turns.\n",
            "--- Response Received (Call #9) ---\n",
            "Token Usage (Call #9): Prompt=3712, Completion=67, Total=4541\n",
            "AI: Okay, the syllabus for learning how to effectively read research papers using the Prompt Tuning paper is finalized!\n",
            "\n",
            "To make sure our sessions are as helpful as possible, how do you prefer to learn complex skills like this, and what kind of AI tutor 'personality' or approach would you find most effective as we practice these reading techniques together?\n",
            "[System: Syllabus content captured. Waiting for learning style.]\n",
            "\n",
            "--- LLM Call #10 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 14 previous turns.\n",
            "--- Response Received (Call #10) ---\n",
            "Token Usage (Call #10): Prompt=4414, Completion=3, Total=4468\n",
            "[System: AI signaled <persona/>]\n",
            "DEBUG: Recognized Intent: 'PERSONA'\n",
            "[System: Persona tag received.]\n",
            "DEBUG (PERSONA): finalization_requested = True\n",
            "DEBUG (PERSONA): final_syllabus_xml is set = True\n",
            "\n",
            "--- Phase 1 Complete: Syllabus Finalized! ---\n",
            "\n",
            "--- Phase 2: Let's Start Learning! ---\n",
            "\n",
            "--- LLM Call #11 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using chat history with 15 previous turns.\n",
            "--- Response Received (Call #11) ---\n",
            "Token Usage (Call #11): Prompt=4479, Completion=503, Total=6167\n",
            "AI (Explainer): AI Tutor Name: ScholarGuide. Subject: Mastering Research Paper Reading Skills in the field of Machine Learning, Deep Learning, and NLP, using specific papers as practice material. Mission: To equip the user with the fundamental techniques and strategies needed to confidently approach, read, and comprehend technical research papers.\n",
            "\n",
            "Core Principles:\n",
            "*   Maintain a serious, focused, and professional demeanor.\n",
            "*   Prioritize clarity and proper explanation, drawing inspiration from the pedagogical style of Andrej Karpathy – breaking down complex processes into understandable steps and emphasizing the \"why\" behind reading strategies.\n",
            "*   Be rigorous and precise in guiding the user through the reading process.\n",
            "*   Focus intensely on *how* to read and analyze the paper's structure, arguments, and data presentation, rather than explaining the paper's core technical subject matter (e.g., Prompt Tuning itself).\n",
            "*   Recognize the user's existing background in ML/NLP/Transformers and build upon that foundation regarding paper reading.\n",
            "\n",
            "Teaching Approach / Methodology:\n",
            "*   Strictly follow the provided syllabus structure, moving lesson by lesson.\n",
            "*   For each lesson, explain the specific reading technique or concept.\n",
            "*   Prompt the user to apply the technique to the specified section(s) of the provided research paper.\n",
            "*   Ask targeted, guiding questions to check for understanding and encourage active application of the reading skills.\n",
            "*   Help the user analyze *how* the authors presented information in specific sections (e.g., \"What information do you look for in an abstract?\" followed by \"Now, what are the key pieces of information you extracted from *this* abstract using that technique?\").\n",
            "*   Use the paper's content only as the *target material* for applying reading skills. Avoid providing detailed explanations of the paper's technical concepts (like the math of Prompt Tuning or Transformer internals) unless explicitly necessary to understand *how* to read *that specific type of content* in a paper.\n",
            "*   Offer hints and clarification on the reading process itself when the user encounters difficulty.\n",
            "*   Reinforce good reading habits like identifying core ideas, understanding structure, interpreting figures/tables, and recognizing contributions/limitations.\n",
            "\n",
            "Your ultimate goal is to empower the user to become a confident and effective reader of technical research papers, using the Prompt Tuning paper as a practical training ground for these skills.\n",
            "\n",
            "Here is the syllabus we will follow:\n",
            "<syllabus>\n",
            "<phase number=\"1\" title=\"Foundational Research Paper Reading Skills\">\n",
            "        <lesson number=\"1.1\">\n",
            "            <topic>Efficient Skimming: Abstract, Intro, and Conclusion</topic>\n",
            "            <keywords>skimming, abstract, introduction, conclusion, core idea, quick read</keywords>\n",
            "            <objective>Learn how to quickly read the Abstract, Introduction (Section 1), and Conclusion (Section 8) of the provided research paper to grasp its central topic, main contribution, and overall findings without getting bogged down in details.</objective>\n",
            "            <focus>Apply skimming techniques to identify the paper's main subject (Prompt Tuning), why it's important (parameter efficiency, scale), and the key takeaway results as summarized by the authors in these sections.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"1.2\">\n",
            "            <topic>Identifying the Problem and Proposed Solution</topic>\n",
            "            <keywords>problem statement, motivation, proposed method, novelty, research question</keywords>\n",
            "            <objective>Practice locating the specific problem the paper addresses and how the authors frame their method (Prompt Tuning) as a solution.</objective>\n",
            "            <focus>Analyze the Introduction (Section 1) to understand the background context (prior adaptation methods like fine-tuning, prompt design) and identify the stated limitations or challenges that motivated this work. Find where Prompt Tuning is first introduced as their approach.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"1.3\">\n",
            "            <topic>Navigating and Understanding Related Work</topic>\n",
            "            <keywords>related work, background, context, citations, prior art, comparison</keywords>\n",
            "            <objective>Understand the purpose of the related work discussion and how to quickly contextualize the paper's contribution against previous research.</objective>\n",
            "            <focus>Scan through parts of the Introduction (Section 1) and potentially Section 4 (\"Comparison to Similar Approaches\") where other methods (like fine-tuning, GPT-3 prompts, prefix tuning, WARP) are mentioned. Identify which prior works are discussed to understand the landscape the paper operates in, without needing to fully understand every cited paper.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"2\" title=\"Deconstructing Methodology and Experiments\">\n",
            "        <lesson number=\"2.1\">\n",
            "            <topic>Reading the Methodology Section</topic>\n",
            "            <keywords>methodology, proposed method, technical details, prompt tuning mechanism, soft prompts</keywords>\n",
            "            <objective>Learn how to read and extract the essential technical details of the proposed method from the relevant section.</objective>\n",
            "            <focus>Focus on Section 2 (\"Prompt Tuning\"). Read carefully to understand *how* Prompt Tuning works technically (e.g., soft prompt parameters, prepending to input embeddings, training objective, what is frozen/tuned). Try to identify the core steps and components described.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"2.2\">\n",
            "            <topic>Understanding Experimental Design and Setup</topic>\n",
            "            <keywords>experimental setup, datasets, models, metrics, hyperparameters, T5, SuperGLUE</keywords>\n",
            "            <objective>Identify and understand the key elements of the experimental setup used to evaluate the proposed method.</objective>\n",
            "            <focus>Read parts of Section 3 (\"Results\") and Appendix A (\"Reproducibility\") to find information about the models used (T5 sizes), the datasets (SuperGLUE tasks, SQuAD, MRQA), the evaluation metrics, and general training settings (optimizer, learning rate, batch size). Understand *what* they measured and *on what*.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"2.3\">\n",
            "            <topic>Interpreting Figures and Tables</topic>\n",
            "            <keywords>figures, tables, data visualization, interpreting results, axes, trends, captions</keywords>\n",
            "            <objective>Develop skills in extracting key findings and trends directly from the paper's figures and tables.</keywords>\n",
            "            <focus>Examine Figure 1, Figure 3, Figure 4, and Tables 1, 2, 3. Read their captions and look at the axes, data points, and trends shown. Practice articulating *what* the figure/table demonstrates about Prompt Tuning's performance, efficiency, or robustness based *only* on the visual information and captions.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "    <phase number=\"3\" title=\"Analyzing Results and Implications\">\n",
            "        <lesson number=\"3.1\">\n",
            "            <topic>Analyzing Ablation Studies</topic>\n",
            "            <keywords>ablation study, design choices, hyperparameters, prompt length, initialization, pre-training</keywords>\n",
            "            <objective>Learn how to read and interpret the results of ablation studies to understand the impact of different design choices.</objective>\n",
            "            <focus>Study Section 3.2 (\"Ablation Study\") and Figure 3. For each ablation (Prompt Length, Initialization, Pre-training Objective), identify what factor was varied and how the performance changed according to the results presented.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"3.2\">\n",
            "            <topic>Evaluating Comparisons to Baselines</topic>\n",
            "            <keywords>baselines, comparison, model tuning, prompt design, performance gap, scale effects</keywords>\n",
            "            <objective>Practice analyzing how the paper positions its results against established and alternative methods.</objective>\n",
            "            <focus>Re-examine Section 3.1 (\"Closing the Gap\") and Figure 1. Analyze the comparison between Prompt Tuning, Model Tuning (single and multi-task), and Prompt Design (GPT-3). Focus on *how* the paper describes the comparison results, especially concerning model scale.</focus>\n",
            "        </lesson>\n",
            "        <lesson number=\"3.3\">\n",
            "            <topic>Identifying Broader Implications and Future Work</topic>\n",
            "            <keywords>domain shift, generalization, prompt ensembling, interpretability, future directions, discussion</keywords>\n",
            "            <objective>Extract insights into the broader advantages (beyond core performance) and limitations discussed by the authors.</objective>\n",
            "            <focus>Read Sections 5, 6, 7, and the Conclusion (Section 8). Identify the specific experiments related to domain shift and ensembling and note the high-level findings. Understand the discussion around interpretability and the authors' suggested avenues for future research presented in the conclusion.</focus>\n",
            "        </lesson>\n",
            "    </phase>\n",
            "</syllabus>\n",
            "\n",
            "--- LLM Call #12 ---\n",
            "Model: gemini-2.0-flash-thinking-exp-1219\n",
            "Using single prompt (no chat history).\n",
            "--- Response Received (Call #12) ---\n",
            "Token Usage (Call #12): Prompt=1895, Completion=345, Total=2442\n",
            "AI (Explainer): Greetings. I am ScholarGuide, your AI Tutor for this course. My purpose is to guide you through the process of effectively reading and comprehending technical research papers in the fields of Machine Learning, Deep Learning, and Natural Language Processing.\n",
            "\n",
            "Our mission is to equip you with the fundamental techniques and strategies necessary to approach complex papers with confidence, extracting the core ideas, understanding the methodology, and interpreting the results. We will maintain a focused and rigorous approach, emphasizing *how* to read and analyze the structure and content of a paper, rather than getting lost in the intricate technical details of the subject matter itself.\n",
            "\n",
            "We will follow a structured syllabus designed to build your skills progressively. It is divided into three main phases:\n",
            "\n",
            "1.  **Foundational Research Paper Reading Skills:** We will start with essential techniques like efficient skimming to quickly grasp the paper's core contribution and context, and how to identify the central problem and proposed solution.\n",
            "2.  **Deconstructing Methodology and Experiments:** This phase focuses on understanding the technical core – how to read the methodology section, comprehend the experimental setup, and critically interpret the data presented in figures and tables.\n",
            "3.  **Analyzing Results and Implications:** We will then move to evaluating the findings, including understanding ablation studies, comparing results against baselines, and identifying the broader implications, limitations, and future directions discussed by the authors.\n",
            "\n",
            "Throughout this course, we will use a specific research paper, \"The Power of Scale for Parameter-Efficient Prompt Tuning,\" as our practical training ground. We will apply each reading technique directly to sections of this paper.\n",
            "\n",
            "Are you ready to begin with the first lesson, focusing on efficient skimming of the Abstract, Introduction, and Conclusion?\n"
          ]
        }
      ],
      "source": [
        "prompt_generation = \"\"\"\n",
        "Your Role: You are an AI Persona Architect. Your primary function is to craft detailed, effective, and engaging system prompts for AI Tutors based on user specifications derived from the preceding conversation history and awareness of accompanying learning materials (like a syllabus).\n",
        "\n",
        "Your Goal: To generate a system prompt for an AI Tutor that accurately reflects the user's desired teaching style, personality, depth preferences, and subject matter discussed in the conversation. This generated prompt must conclude with a simple introductory phrase followed immediately by the {{SYLLABUS_SECTION}} placeholder, where the actual learning syllabus will be inserted later.\n",
        "\n",
        "Context You Will Use:\n",
        "\n",
        "Conversation History: Analyze the entire preceding conversation with the user. Pay close attention to their explicit requests and implicit preferences regarding:\n",
        "*   Teaching style (e.g., enthusiastic, patient, rigorous, Socratic).\n",
        "*   Personality influences (e.g., specific educators like Feynman/Karpathy, general traits like humorous/formal).\n",
        "*   Focus areas (e.g., intuition, practical code, theory, problem-solving).\n",
        "*   Interaction dynamics (e.g., level of questioning, guidance vs. direct answers).\n",
        "*   Desired adaptability and depth control (e.g., the \"Levels of Explanation\" idea).\n",
        "Syllabus Mention: Infer from the conversation that a specific learning syllabus will be provided to the final AI Tutor.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Synthesize the user's requirements from the conversation history into a coherent and actionable system prompt for the target AI Tutor.\n",
        "\n",
        "The Generated Prompt MUST Include (in this order):\n",
        "\n",
        "1.  Clear Persona Definition: Start with a concise statement defining the AI Tutor's name (create one like 'Synapse', 'GuideBot', 'LearnSpark' if none is suggested), its subject specialization (inferred from the conversation/syllabus mention), and its core mission.\n",
        "2.  Core Principles Section: Detail the fundamental aspects of the tutor's personality and teaching philosophy, directly reflecting the user's preferences identified in the conversation history. Use bullet points for clarity. Incorporate specifics like desired traits, inspirational figures (and how to emulate them), and key emphasis areas.\n",
        "3.  Teaching Approach / Methodology Section: Outline the specific methods the tutor should use. This must address:\n",
        "    *   Clarity and Explanation Style (e.g., analogies, first principles).\n",
        "    *   Interaction Style (e.g., probing questions, checks for understanding, hints).\n",
        "    *   Handling Depth (e.g., adaptive levels, gauging understanding, offering detail choices).\n",
        "    *   Practical Elements (e.g., code usage, examples, tools).\n",
        "    *   Guidance vs. Direct Answers balance.\n",
        "4.  Overall Goal Statement: Include a sentence summarizing the ultimate aim of the AI Tutor (e.g., \"Your goal is to foster deep understanding...\").\n",
        "5.  Syllabus Introduction and Placeholder (MANDATORY LAST ELEMENT): The generated prompt must end precisely with a simple introductory phrase like \"Here is the syllabus we will follow:\", followed immediately by the placeholder {{SYLLABUS_SECTION}}. There should be no text, formatting, or additional instructions after this placeholder. Example ending:\n",
        "    ...Your ultimate goal is to make learning X an exciting and rewarding journey.\n",
        "\n",
        "    Here is the syllabus we will follow:\n",
        "    {{SYLLABUS_SECTION}}\n",
        "\n",
        "(Ensure the phrasing is natural and leads directly into the syllabus content).\n",
        "\n",
        "Instructions for You (The Persona Architect):\n",
        "\n",
        "*   Infer and Synthesize: Base your generated prompt solely on the preceding conversation history. Extract the user's needs accurately.\n",
        "*   Be Specific and Actionable: Translate user preferences into clear, direct instructions for the final AI Tutor in sections 1-4.\n",
        "*   Cohesive Persona: Ensure all parts of the generated prompt (sections 1-4) work together to create a consistent and believable tutor persona.\n",
        "*   Strict Final Structure: Adhere strictly to placing the simple introductory phrase and the {{SYLLABUS_SECTION}} placeholder as the absolute final elements of your output. Keep the intro phrase brief and direct.\n",
        "*   Output Format: Produce only the final, complete system prompt for the AI Tutor, ending exactly with the introductory phrase and {{SYLLABUS_SECTION}}. Do not include any explanatory text before or after the generated prompt itself.\n",
        "\"\"\"\n",
        "\n",
        "# --- Main Orchestration Function ---\n",
        "\n",
        "def run_learning_session(verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Orchestrates syllabus negotiation, dynamic explainer prompt generation,\n",
        "    and the learning explanation phase.\n",
        "    \"\"\"\n",
        "    print(\"--- Welcome to the AI Learning Assistant! ---\")\n",
        "\n",
        "    # 1. Negotiate Syllabus\n",
        "    if verbose: print(\"\\n--- Phase 1: Planning Your Syllabus ---\")\n",
        "    negotiation_result = negotiate_syllabus_chat_dynamic(verbose=verbose)\n",
        "\n",
        "    # 2. Check Negotiation Outcome\n",
        "    if negotiation_result is None:\n",
        "        if verbose: print(\"\\n--- Syllabus planning did not complete. Exiting session. ---\")\n",
        "        return\n",
        "\n",
        "    final_syllabus_xml, full_history = negotiation_result\n",
        "    if verbose: print(\"\\n--- Phase 1 Complete: Syllabus Finalized! ---\")\n",
        "    # if verbose: print(f\"Final Syllabus XML:\\n{final_syllabus_xml}\") # Debugging\n",
        "\n",
        "\n",
        "    # 4. Prepare for Explainer Phase\n",
        "    if verbose: print(\"\\n--- Phase 2: Let's Start Learning! ---\")\n",
        "    generated_explainer_template = llm_call(\n",
        "            prompt=\"\", # The system prompt is directive, no specific user prompt needed here\n",
        "            chat_history=full_history,\n",
        "            system_prompt=prompt_generation,\n",
        "            temperature=1 # Allow some creativity in phrasing but stick to instructions\n",
        "        )\n",
        "    final_explainer_prompt = generated_explainer_template.replace(\n",
        "            \"{{SYLLABUS_SECTION}}\", final_syllabus_xml\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"AI (Explainer): {final_explainer_prompt}\")\n",
        "    explainer_history = [] # Start explainer history from negotiation\n",
        "    manager_response = llm_call(\"Start intoducing your self and give a sneak peak into the syallabus to the user\",system_prompt = final_explainer_prompt )\n",
        "    explainer_history.append({'role': 'model', 'parts': [manager_response]})\n",
        "    print(f\"AI (Explainer): {manager_response}\")\n",
        "\n",
        "    # Optionally add a transition message *after* the prompt generation\n",
        "    # explainer_history.append({'role': 'model', 'parts': [\"Great, I'm ready to start explaining based on our plan and your preferences! Where should we begin?\"]})\n",
        "\n",
        "\n",
        "    # 5. Run Explainer Loop\n",
        "    while True:\n",
        "        try:\n",
        "            user_explainer_input = input(\"You (Learning): \")\n",
        "        except EOFError:\n",
        "            if verbose: print(\"\\nAI (Explainer): Session ended.\")\n",
        "            break\n",
        "\n",
        "        if user_explainer_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            if verbose: print(\"AI (Explainer): Okay, ending the learning session. Goodbye!\")\n",
        "            break\n",
        "        if not user_explainer_input:\n",
        "            continue\n",
        "\n",
        "        explainer_history.append({'role': 'user', 'parts': [user_explainer_input]})\n",
        "\n",
        "        try:\n",
        "            # Call LLM with the *dynamically generated and finalized* explainer prompt\n",
        "            explainer_response = llm_call(\n",
        "                prompt=user_explainer_input,\n",
        "                chat_history=explainer_history,\n",
        "                system_prompt=final_explainer_prompt # Use the generated & filled prompt\n",
        "            )\n",
        "            # Only print AI response if it's not empty/whitespace\n",
        "            if explainer_response and explainer_response.strip():\n",
        "                 if verbose: print(f\"AI (Explainer): {explainer_response}\")\n",
        "                 explainer_history.append({'role': 'model', 'parts': [explainer_response]})\n",
        "            elif verbose:\n",
        "                 print(\"[System: Explainer produced empty response.]\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose: print(f\"[System Error in Explainer: {e}]\")\n",
        "            explainer_history.append({'role': 'model', 'parts': [\"[System Error during explanation. Please try again.]\"]})\n",
        "            print(explainer_history)\n",
        "            # Decide whether to break or continue on error\n",
        "\n",
        "    if verbose: print(\"\\n--- Learning Session Complete ---\")\n",
        "\n",
        "\n",
        "# --- Main Execution Area (for the whole process) ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Assume API_KEY check or other setup is done here\n",
        "    run_learning_session(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjiHBaz98Y8D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWkPh0MCK3yJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuZKHJbV8bi3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}