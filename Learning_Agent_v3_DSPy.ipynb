{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0DVZo2wsYI8"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "import dspy\n",
        "import litellm\n",
        "from typing import Optional, List, Dict, Any, Union,Tuple,Callable\n",
        "class CustomGeminiDspyLM(dspy.LM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str,\n",
        "        api_key: str,\n",
        "        rate_limiter_instance: Any,\n",
        "        safety_settings: Optional[List[Dict[str, str]]] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "\n",
        "        super().__init__(model)\n",
        "\n",
        "        # Store our custom parameters\n",
        "        self.model = model # The full LiteLLM model string\n",
        "        self.api_key = api_key\n",
        "        self.rate_limiter = rate_limiter_instance\n",
        "        self.safety_settings = safety_settings\n",
        "\n",
        "\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "\n",
        "        self.provider = \"custom_gemini_litellm\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            _ = litellm.model_cost # Access an attribute to check if litellm is basically working\n",
        "            print(f\"CustomGeminiDspyLM initialized for model: {self.model} via LiteLLM.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during LiteLLM check in CustomGeminiDspyLM init: {e}\")\n",
        "            # Depending on severity, you might raise an error here:\n",
        "            # raise RuntimeError(\"LiteLLM does not seem to be properly installed or available.\")\n",
        "\n",
        "\n",
        "\n",
        "    def _prepare_litellm_messages_from_dspy_inputs(\n",
        "        self, dspy_input: Union[str, List[Dict[str, str]]]\n",
        "    ) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Converts DSPy style input (string or list of messages)\n",
        "        to LiteLLM's expected messages format.\n",
        "        \"\"\"\n",
        "        if isinstance(dspy_input, str):\n",
        "            # Simple case: a single user prompt\n",
        "            return [{\"role\": \"user\", \"content\": dspy_input}]\n",
        "        elif isinstance(dspy_input, list):\n",
        "            #\n",
        "            return dspy_input\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                f\"Unsupported dspy_input type for message preparation: {type(dspy_input)}\"\n",
        "            )\n",
        "# (Keep your __init__ and _prepare_litellm_messages_from_dspy_inputs methods as they were)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Optional[str] = None,\n",
        "        messages: Optional[List[Dict[str, str]]] = None,\n",
        "        **kwargs,\n",
        "    ) -> List[str]:\n",
        "        if not prompt and not messages:\n",
        "            raise ValueError(\"Either 'prompt' or 'messages' must be provided.\")\n",
        "        if prompt and messages:\n",
        "            raise ValueError(\"Provide either 'prompt' or 'messages', not both.\")\n",
        "        if messages is not None:\n",
        "          dspy_input_content = messages\n",
        "        elif prompt is not None:\n",
        "            dspy_input_content = prompt\n",
        "        else:\n",
        "            raise ValueError(\"Either 'prompt' or 'messages' must be provided.\")\n",
        "\n",
        "        self.rate_limiter.wait_if_needed()\n",
        "\n",
        "        dspy_input_content = prompt if prompt is not None else messages\n",
        "        try:\n",
        "            messages_for_litellm = self._prepare_litellm_messages_from_dspy_inputs(dspy_input_content)\n",
        "        except TypeError as e:\n",
        "            print(f\"Error preparing messages: {e}\")\n",
        "            return [f\"[ERROR: Message preparation error - {e}]\"]\n",
        "\n",
        "        final_call_kwargs = self.kwargs.copy()\n",
        "        final_call_kwargs.update(kwargs)\n",
        "\n",
        "        current_safety_settings = self.safety_settings\n",
        "        if 'safety_settings' in final_call_kwargs:\n",
        "            current_safety_settings = final_call_kwargs.pop('safety_settings')\n",
        "\n",
        "        extra_body = {}\n",
        "        if current_safety_settings:\n",
        "            extra_body['safety_settings'] = current_safety_settings\n",
        "\n",
        "        try:\n",
        "            print(f\"[CustomGeminiDspyLM] Calling LiteLLM. Model: {self.model}\")\n",
        "            print(f\"Messages for LiteLLM: {messages_for_litellm}\")\n",
        "            print(f\"Final kwargs for LiteLLM: {final_call_kwargs}\")\n",
        "            if extra_body:\n",
        "                print(f\"Extra body for LiteLLM: {extra_body}\")\n",
        "\n",
        "            response_obj = litellm.completion(\n",
        "                model=self.model,\n",
        "                messages=messages_for_litellm,\n",
        "                api_key=self.api_key,\n",
        "                extra_body=extra_body if extra_body else None,\n",
        "                **final_call_kwargs,\n",
        "            )\n",
        "\n",
        "            # --- MODIFIED/ADDED SECTION FOR ROBUSTNESS ---\n",
        "            print(f\"[CustomGeminiDspyLM] Raw LiteLLM response object choices: {response_obj.choices}\")\n",
        "\n",
        "            completions = []\n",
        "            if response_obj.choices:\n",
        "                for choice in response_obj.choices:\n",
        "                    if choice.message and choice.message.content is not None:\n",
        "                        completions.append(choice.message.content)\n",
        "                    else:\n",
        "                        # Content is None, log this and potentially add a placeholder or error string\n",
        "                        finish_reason = choice.finish_reason if hasattr(choice, 'finish_reason') else \"N/A\"\n",
        "                        print(f\"[CustomGeminiDspyLM] Warning: Received a choice with None content. Finish reason: {finish_reason}\")\n",
        "                        completions.append(f\"[WARN: Content filtered or empty. Finish Reason: {finish_reason}]\")\n",
        "            else:\n",
        "                print(\"[CustomGeminiDspyLM] Warning: LiteLLM response object had no choices.\")\n",
        "                completions.append(\"[WARN: No choices in response]\")\n",
        "            # --- END OF MODIFIED/ADDED SECTION ---\n",
        "\n",
        "\n",
        "            if completions and completions[0] is not None and not completions[0].startswith(\"[WARN:\"): # Check if first completion is valid\n",
        "                print(f\"[CustomGeminiDspyLM] Response from LiteLLM (first choice snippet): {str(completions[0])[:100]}...\")\n",
        "            elif completions: # There are completions, but the first might be a warning\n",
        "                 print(f\"[CustomGeminiDspyLM] First completion (or warning): {completions[0]}\")\n",
        "            else: # No completions at all\n",
        "                print(\"[CustomGeminiDspyLM] No valid completions found.\")\n",
        "\n",
        "\n",
        "            return completions\n",
        "\n",
        "        except litellm.RateLimitError as rle:\n",
        "            print(f\"[CustomGeminiDspyLM] LiteLLM RateLimitError: {rle}.\")\n",
        "            return [f\"[ERROR: LiteLLM RateLimitError - {rle}]\"]\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"[CustomGeminiDspyLM] Error during LiteLLM completion: {type(e).__name__} - {e}\")\n",
        "            traceback.print_exc()\n",
        "            return [f\"[ERROR: {type(e).__name__} - {e}]\"]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTxLAky2Y94K",
        "outputId": "33f774c6-29f6-4eba-d590-4db453ac2eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate Limiter Initialized: Max 7 calls per 60 seconds.\n",
            "Global RateLimiter instance created.\n"
          ]
        }
      ],
      "source": [
        "class RateLimiter:\n",
        "    def __init__(self, max_calls=7, time_period=60):\n",
        "        self.max_calls = max_calls\n",
        "        self.time_period = time_period\n",
        "        self.call_timestamps = []\n",
        "        self.total_calls_made_through_limiter = 0\n",
        "        print(f\"Rate Limiter Initialized: Max {self.max_calls} calls per {self.time_period} seconds.\")\n",
        "\n",
        "    def wait_if_needed(self) -> int:\n",
        "        if not API_KEY:\n",
        "             raise ValueError(\"Cannot make API calls: API Key not configured.\")\n",
        "        current_time = datetime.now()\n",
        "        self.call_timestamps = [ts for ts in self.call_timestamps\n",
        "                               if current_time - ts < timedelta(seconds=self.time_period)]\n",
        "        if len(self.call_timestamps) >= self.max_calls:\n",
        "            oldest_call_in_window = min(self.call_timestamps)\n",
        "            wait_time = (oldest_call_in_window + timedelta(seconds=self.time_period) - current_time).total_seconds()\n",
        "            if wait_time > 0:\n",
        "                print(f\"\\n[Rate Limiter]: Limit reached. Waiting {wait_time:.1f} seconds...\")\n",
        "                time.sleep(wait_time + 0.5)\n",
        "            current_time = datetime.now()\n",
        "            self.call_timestamps = [ts for ts in self.call_timestamps\n",
        "                                   if current_time - ts < timedelta(seconds=self.time_period)]\n",
        "        self.call_timestamps.append(datetime.now())\n",
        "        self.total_calls_made_through_limiter += 1\n",
        "        return self.total_calls_made_through_limiter\n",
        "\n",
        "limiter = RateLimiter()\n",
        "print(\"Global RateLimiter instance created.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    print(\"Google Generative AI Configured Successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: Secret 'GEMINI_API_KEY' not found.\")\n",
        "    print(\"Please add your Gemini API Key to Colab Secrets.\")\n",
        "    # Optionally, raise an error or exit if the key is critical\n",
        "    API_KEY = None # Set API_KEY to None to indicate failure\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during genai configuration: {e}\")\n",
        "    API_KEY = None\n",
        "\n",
        "DEFAULT_MODEL_NAME= \"gemini-2.5-flash-preview-04-17\"\n",
        "\n",
        "\n",
        "DEFAULT_SAFETY_SETTINGS = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "if API_KEY and 'limiter' in globals():\n",
        "    try:\n",
        "      DEFAULT_GEMINI_VARIANT = \"gemini-2.5-flash-preview-04-17\"\n",
        "      LITELLM_MODEL_STRING = f\"gemini/{DEFAULT_GEMINI_VARIANT}\"\n",
        "      corrected_custom_lm = CustomGeminiDspyLM(\n",
        "          model=LITELLM_MODEL_STRING,\n",
        "          api_key=API_KEY,\n",
        "          rate_limiter_instance=limiter,\n",
        "          safety_settings=LITELLM_MODEL_STRING if 'DEFAULT_SAFETY_SETTINGS' in globals() else None,\n",
        "          temperature=1,\n",
        "      )\n",
        "      dspy.settings.configure(lm=corrected_custom_lm) # << THE IMPORTANT LINE\n",
        "      print(f\"DEBUG: DSPy configured globally with Corrected CustomGeminiDspyLM: {LITELLM_MODEL_STRING}\")\n",
        "      if dspy.settings.lm is None:\n",
        "            print(\"DEBUG ERROR: dspy.settings.lm is STILL NONE after corrected configuration!\")\n",
        "      else:\n",
        "            print(f\"DEBUG VERIFIED: dspy.settings.lm is configured to: {dspy.settings.lm.model}\")\n",
        "            print(f\"DEBUG LM type: {type(dspy.settings.lm)}\") # Add this line\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG Error RE-CONFIGURING DSPy: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7OFRiZbhQph",
        "outputId": "6e6f32fe-a0c6-4a57-a454-56b94e11307d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI Configured Successfully.\n",
            "CustomGeminiDspyLM initialized for model: gemini/gemini-2.5-flash-preview-04-17 via LiteLLM.\n",
            "DEBUG: DSPy configured globally with Corrected CustomGeminiDspyLM: gemini/gemini-2.5-flash-preview-04-17\n",
            "DEBUG VERIFIED: dspy.settings.lm is configured to: gemini/gemini-2.5-flash-preview-04-17\n",
            "DEBUG LM type: <class '__main__.CustomGeminiDspyLM'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Resources"
      ],
      "metadata": {
        "id": "jF6Ysdlzhn4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EPj2hzAhJDdf",
        "outputId": "eede19d0-5e26-405d-948c-50c098f1efdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dspy in /usr/local/lib/python3.11/dist-packages (2.6.24)\n",
            "Requirement already satisfied: backoff>=2.2 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.5.0)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.78.1)\n",
            "Requirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: ujson>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (5.10.0)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.6.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.32.3)\n",
            "Requirement already satisfied: optuna>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (4.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.11.4)\n",
            "Requirement already satisfied: magicattr>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.1.6)\n",
            "Requirement already satisfied: litellm>=1.60.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.71.1)\n",
            "Requirement already satisfied: diskcache>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (5.6.3)\n",
            "Requirement already satisfied: json-repair>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.46.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (9.1.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy) (4.9.0)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.0.8)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from dspy) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->dspy) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy) (6.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (8.2.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (0.28.1)\n",
            "Requirement already satisfied: httpx-aiohttp>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (0.1.4)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (1.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy) (0.21.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.28.1->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.28.1->dspy) (0.9.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.4.0->dspy) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=3.4.0->dspy) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.4.0->dspy) (2.0.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->dspy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->dspy) (2.19.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy) (1.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy) (1.20.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm>=1.60.3->dspy) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.60.3->dspy) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.60.3->dspy) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->dspy) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy) (3.2.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install PyPDF2\n",
        "!pip install dspy\n",
        "!pip install python-docx\n",
        "import PyPDF2\n",
        "import docx\n",
        "import csv\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time #Import the time module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekrzBxqfJJUY",
        "outputId": "cc34da0f-47e7-4c8d-a86b-2e0c39d29149",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory: '/content/drive/MyDrive/txt_files'\n",
            "\n",
            "Scanning directory: '/content/drive/MyDrive/Books'...\n",
            "Found 2 potential files to process.\n",
            "\n",
            "Processing file 1/2: BartoSutton (1).pdf\n",
            "    Attempting to write to: BartoSutton (1).txt\n",
            "    -> Extracted and saved to: BartoSutton (1).txt (Write operation completed)\n",
            "\n",
            "Processing file 2/2: Attention is all You need.pdf\n",
            "    Attempting to write to: Attention is all You need.txt\n",
            "    -> Extracted and saved to: Attention is all You need.txt (Write operation completed)\n",
            "\n",
            "--- Processing Complete ---\n",
            "Successfully processed files reported by script: 2\n",
            "Skipped/Failed files:       0\n",
            "---------------------------\n",
            "INFO: Waiting a few seconds for potential Drive sync...\n",
            "INFO: Wait finished.\n",
            "\n",
            "List of created/processed TXT files reported by script:\n",
            "Verifying file existence in output directory:\n",
            "- BartoSutton (1).txt (Exists: True)\n",
            "- Attention is all You need.txt (Exists: True)\n",
            "\n",
            "Verification complete: 2 files found in output directory.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import PyPDF2\n",
        "except ImportError:\n",
        "    print(\"WARNING: PyPDF2 library not found. PDF extraction will be disabled.\")\n",
        "    print(\"         Install it using: pip install PyPDF2\")\n",
        "    PyPDF2 = None\n",
        "\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    print(\"WARNING: python-docx library not found. DOCX extraction will be disabled.\")\n",
        "    print(\"         Install it using: pip install python-docx\")\n",
        "    docx = None\n",
        "\n",
        "def _extract_text_from_pdf(file_content: bytes) -> Optional[str]:\n",
        "    if not PyPDF2: return None\n",
        "    try:\n",
        "        pdf_file = io.BytesIO(file_content); reader = PyPDF2.PdfReader(pdf_file); text = \"\"\n",
        "        if reader.is_encrypted:\n",
        "            try:\n",
        "                if reader.decrypt('') == PyPDF2.PasswordType.NOT_DECRYPTED: return None\n",
        "            except Exception: return None\n",
        "        for page in reader.pages:\n",
        "            try:\n",
        "                page_text = page.extract_text();\n",
        "                if page_text: text += page_text + \"\\n\"\n",
        "            except Exception: continue\n",
        "        return text.strip() if text else None\n",
        "    except PyPDF2.errors.PdfReadError: return None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "def _extract_text_from_docx(file_content: bytes) -> Optional[str]:\n",
        "    if not docx: return None\n",
        "    try:\n",
        "        doc_file = io.BytesIO(file_content); document = docx.Document(doc_file)\n",
        "        text = \"\\n\".join([para.text for para in document.paragraphs if para.text])\n",
        "        return text.strip() if text else None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "def _read_text_from_txt(file_content: bytes, filename_for_log: str) -> Optional[str]:\n",
        "    try: text = file_content.decode('utf-8'); return text\n",
        "    except UnicodeDecodeError:\n",
        "        try: text = file_content.decode('latin-1'); return text\n",
        "        except Exception: return None\n",
        "    except Exception: return None\n",
        "\n",
        "\n",
        "\n",
        "def extract_and_copy_text_files(input_dir: str, output_dir: str) -> List[str]:\n",
        "    if not os.path.isdir(input_dir):\n",
        "        print(f\"Error: Input directory not found or is not a directory: '{input_dir}'\")\n",
        "        return []\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Output directory: '{os.path.abspath(output_dir)}'\")\n",
        "\n",
        "    successfully_processed_files = []\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    print(f\"\\nScanning directory: '{os.path.abspath(input_dir)}'...\")\n",
        "\n",
        "    files_to_process = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for filename in files:\n",
        "             files_to_process.append(os.path.join(root, filename))\n",
        "\n",
        "    total_files = len(files_to_process)\n",
        "    print(f\"Found {total_files} potential files to process.\")\n",
        "\n",
        "    # --- Process each file ---\n",
        "    for i, input_file_path in enumerate(files_to_process):\n",
        "        filename = os.path.basename(input_file_path)\n",
        "        base_name, file_extension = os.path.splitext(filename)\n",
        "        file_extension = file_extension.lower()\n",
        "\n",
        "        supported_extensions = [\".pdf\", \".docx\", \".txt\"]\n",
        "        if file_extension not in supported_extensions:\n",
        "            continue # Skip unsupported types early\n",
        "\n",
        "        # Use relative path for potentially deeply nested files for cleaner logs\n",
        "        try:\n",
        "            relative_path = os.path.relpath(input_file_path, input_dir)\n",
        "        except ValueError:\n",
        "            relative_path = input_file_path\n",
        "        print(f\"\\nProcessing file {i+1}/{total_files}: {relative_path}\")\n",
        "\n",
        "\n",
        "        if file_extension == \".pdf\" and not PyPDF2:\n",
        "            print(\"    - Skipping PDF: PyPDF2 library not available.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        if file_extension == \".docx\" and not docx:\n",
        "            print(\"    - Skipping DOCX: python-docx library not available.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(input_file_path, 'rb') as f:\n",
        "                file_content = f.read()\n",
        "        except IOError as e:\n",
        "            print(f\"    - Error reading file: {e}\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        except Exception as e:\n",
        "             print(f\"    - Unexpected error reading file: {e}\")\n",
        "             skipped_count += 1\n",
        "             continue\n",
        "\n",
        "        full_text = None\n",
        "        if file_extension == \".pdf\":\n",
        "            full_text = _extract_text_from_pdf(file_content)\n",
        "        elif file_extension == \".docx\":\n",
        "            full_text = _extract_text_from_docx(file_content)\n",
        "        elif file_extension == \".txt\":\n",
        "            full_text = _read_text_from_txt(file_content, filename)\n",
        "\n",
        "        if full_text is not None:\n",
        "            output_filename = base_name + \".txt\"\n",
        "            output_file_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            try:\n",
        "                print(f\"    Attempting to write to: {output_filename}\") # Add debug print\n",
        "                with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
        "                    f_out.write(full_text.strip())\n",
        "                    # --- Add these lines to force flush ---\n",
        "                    f_out.flush() # Flush Python's internal buffer\n",
        "                    os.fsync(f_out.fileno()) # Ask OS to sync to disk (Drive mount)\n",
        "                    # ----------------------------------------\n",
        "                action = \"Extracted and saved\" if file_extension != \".txt\" else \"Processed\"\n",
        "                print(f\"    -> {action} to: {output_filename} (Write operation completed)\")\n",
        "                successfully_processed_files.append(os.path.abspath(output_file_path))\n",
        "                processed_count += 1\n",
        "                # --- Optional: Add a small delay after each file ---\n",
        "                # time.sleep(0.5) # Pause for 0.5 seconds\n",
        "                # -------------------------------------------------\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"    - Error writing output file '{output_filename}': {e}\")\n",
        "                skipped_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"    - Unexpected error writing output file '{output_filename}': {e}\")\n",
        "                skipped_count += 1\n",
        "        else:\n",
        "            print(f\"    - Text extraction or reading failed for: {filename}\")\n",
        "            skipped_count += 1\n",
        "\n",
        "    print(f\"\\n--- Processing Complete ---\")\n",
        "    print(f\"Successfully processed files reported by script: {processed_count}\")\n",
        "    print(f\"Skipped/Failed files:       {skipped_count}\")\n",
        "    print(f\"---------------------------\")\n",
        "    print(\"INFO: Waiting a few seconds for potential Drive sync...\")\n",
        "    time.sleep(5) # Add a final delay to allow background sync\n",
        "    print(\"INFO: Wait finished.\")\n",
        "\n",
        "    return successfully_processed_files\n",
        "\n",
        "# --- Example Usage (Keep as is) ---\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_DIRECTORY = r\"/content/drive/MyDrive/Books\"\n",
        "    OUTPUT_DIRECTORY = r\"/content/drive/MyDrive/txt_files\"\n",
        "\n",
        "    if not os.path.isdir(INPUT_DIRECTORY):\n",
        "        print(f\"Error: Input directory not found: '{INPUT_DIRECTORY}'\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    created_files = extract_and_copy_text_files(INPUT_DIRECTORY, OUTPUT_DIRECTORY)\n",
        "\n",
        "    if created_files:\n",
        "        print(\"\\nList of created/processed TXT files reported by script:\")\n",
        "        # Check existence on disk *after* the final delay\n",
        "        print(\"Verifying file existence in output directory:\")\n",
        "        actual_files_found = 0\n",
        "        for f_path in created_files:\n",
        "             exists = os.path.exists(f_path)\n",
        "             print(f\"- {os.path.relpath(f_path, OUTPUT_DIRECTORY)} (Exists: {exists})\")\n",
        "             if exists:\n",
        "                 actual_files_found += 1\n",
        "        print(f\"\\nVerification complete: {actual_files_found} files found in output directory.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo TXT files were created or processed according to script logs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC7jV_drTuWH"
      },
      "outputs": [],
      "source": [
        "#created_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYuk6t3EfIWD"
      },
      "outputs": [],
      "source": [
        "# import dspy\n",
        "# help(dspy.LM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "52U0RfF7ii2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Mll8ZyRlUl"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_txt_file(file_path: str) -> Optional[str]:\n",
        "\n",
        "    try:\n",
        "        # Ensure the file exists before trying to open it\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"ERROR: File not found at path: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        # Open the file in read mode ('r') with UTF-8 encoding (common and robust)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except FileNotFoundError: # This is redundant if os.path.exists is used, but good practice\n",
        "        print(f\"ERROR: File not found: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not read file {file_path}. Reason: {e}\")\n",
        "        return None\n",
        "def format_history_for_dspy(history_list: List[Dict[str, Any]]) -> str:\n",
        "    formatted_history = []\n",
        "    for turn in history_list:\n",
        "        content = \"\"\n",
        "        if isinstance(turn.get('parts'), list) and turn['parts']:\n",
        "            content = turn['parts'][0]\n",
        "        elif isinstance(turn.get('parts'), str):\n",
        "            content = turn['parts']\n",
        "        formatted_history.append(f\"{turn.get('role', 'unknown')}: {content}\")\n",
        "    return \"\\n---\\n\".join(formatted_history) # Use a clear separator\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG5vTFjmW3wp"
      },
      "source": [
        "## Syllabus Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicSummarizationSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are an AI Resource Analyzer.\n",
        "    Process the provided 'learning_material_excerpt' in the context of the 'conversation_history' and its 'resource_identifier'.\n",
        "    Extract key information MOST RELEVANT to the ongoing conversation.\n",
        "    Pay special attention to Table of Contents, chapter overviews, or introductions.\n",
        "    The summary should help create a structured learning syllabus addressing user's current focus.\n",
        "    **The resource wont be passed to any other agent.**\n",
        "\n",
        "\n",
        "    Output your analysis as a SINGLE JSON object string with the following keys:\n",
        "    - \"resource_identifier\": (String, use the provided identifier)\n",
        "    - \"primary_topics_relevant_to_conversation\": (List of strings)\n",
        "    - \"core_concepts_relevant_to_conversation\": (List of strings)\n",
        "    - \"structure_or_progression_notes\": (String)\n",
        "    - \"keywords_highlighted_by_conversation\": (List of strings)\n",
        "    - \"inferred_learning_objectives_for_current_focus\": (List of strings)\n",
        "    - \"contextual_notes_for_syllabus\": (String)\n",
        "\n",
        "    Ensure the output is ONLY the valid JSON object string.\n",
        "    \"\"\"\n",
        "    conversation_history_str = dspy.InputField(desc=\"The ongoing conversation history as a formatted string.\")\n",
        "    resource_identifier_str = dspy.InputField(desc=\"The identifier (e.g., filename) of the learning material.\")\n",
        "    learning_material_excerpt_str = dspy.InputField(desc=\"The textual content of the learning material excerpt to be summarized and the format provided would be dict.\")\n",
        "\n",
        "    # The LLM's direct output will be a JSON string\n",
        "    json_summary_str = dspy.OutputField(desc=\"A string containing a single, valid JSON object with the summarized analysis.\")\n",
        "\n",
        "class DynamicResourceSummarizerModule(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Using Predict, as the task is to generate a structured string based on clear instructions.\n",
        "        # If formatting is tricky, ChainOfThought could be an alternative.\n",
        "        self.generate_json_summary = dspy.Predict(DynamicSummarizationSignature)\n",
        "\n",
        "    def forward(self,\n",
        "                resource_content: str,\n",
        "                resource_identifier: str,\n",
        "                conversation_history_str: str, # Takes the list of dicts\n",
        "                max_length: int = 100000 # Consistent with your original function\n",
        "               ) -> Optional[Dict[str, Any]]: # Returns a Python dict or None\n",
        "\n",
        "        if not resource_content.strip():\n",
        "            print(f\"[DynamicResourceSummarizerModule] Skipping empty resource: {resource_identifier}\")\n",
        "            return None\n",
        "\n",
        "        truncated_content = resource_content[:max_length]\n",
        "        if len(resource_content) > max_length:\n",
        "            print(f\"[DynamicResourceSummarizerModule] INFO: Resource '{resource_identifier}' truncated to {max_length} chars.\")\n",
        "\n",
        "        # Format conversation history for the signature's input field\n",
        "        # Ensure format_history_for_dspy is defined and accessible\n",
        "        if 'format_history_for_dspy' not in globals(): # Basic check\n",
        "            raise NameError(\"Helper function 'format_history_for_dspy' is not defined.\")\n",
        "        # formatted_history_str = format_history_for_dspy(formatted_history_str)\n",
        "\n",
        "        try:\n",
        "            # Call the DSPy Predictor\n",
        "            prediction = self.generate_json_summary(\n",
        "                conversation_history_str=conversation_history_str,\n",
        "                resource_identifier_str=resource_identifier,\n",
        "                learning_material_excerpt_str=truncated_content\n",
        "            )\n",
        "            raw_json_string_output = prediction.json_summary_str\n",
        "\n",
        "            # Parse the JSON string output from the LLM\n",
        "            # (Similar parsing logic as in your original summarize_single_resource_dynamically)\n",
        "            cleaned_json_str = raw_json_string_output.strip()\n",
        "            if cleaned_json_str.startswith(\"```json\"):\n",
        "                cleaned_json_str = cleaned_json_str[len(\"```json\"):]\n",
        "            elif cleaned_json_str.startswith(\"```\"):\n",
        "                cleaned_json_str = cleaned_json_str[len(\"```\"):]\n",
        "            if cleaned_json_str.endswith(\"```\"):\n",
        "                cleaned_json_str = cleaned_json_str[:-len(\"```\")]\n",
        "            cleaned_json_str = cleaned_json_str.strip()\n",
        "            print(\"1\")\n",
        "            print(cleaned_json_str)\n",
        "\n",
        "            if not cleaned_json_str:\n",
        "                print(f\"WARN [DynamicResourceSummarizerModule]: LLM returned empty string for JSON summary for '{resource_identifier}'.\")\n",
        "                return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": raw_json_string_output, \"is_fallback\": True, \"error\": \"Empty JSON string\"}\n",
        "\n",
        "            try:\n",
        "                summary_data_dict = json.loads(cleaned_json_str)\n",
        "                if isinstance(summary_data_dict, dict) and \"resource_identifier\" in summary_data_dict:\n",
        "                    return summary_data_dict # Success!\n",
        "                else:\n",
        "                    print(f\"WARN [DynamicResourceSummarizerModule]: For '{resource_identifier}', LLM produced non-standard JSON structure after cleaning. Output: {raw_json_string_output[:200]}...\")\n",
        "                    return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": raw_json_string_output, \"is_fallback\": True, \"error\": \"Non-standard JSON structure\"}\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"WARN [DynamicResourceSummarizerModule]: Could not parse JSON from LLM summary for '{resource_identifier}'. Raw output: {raw_json_string_output[:200]}...\")\n",
        "                return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": raw_json_string_output, \"is_fallback\": True, \"error\": \"JSONDecodeError\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR [DynamicResourceSummarizerModule]: Unexpected error during summarization for '{resource_identifier}': {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\"resource_identifier\": resource_identifier, \"raw_summary_text\": str(e), \"is_fallback\": True, \"error\": str(type(e).__name__)}\n",
        "\n",
        "print(\"DynamicResourceSummarizerModule defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n5-djYviHgl",
        "outputId": "56d483e3-b132-430b-b8bb-f3d75b27cebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DynamicResourceSummarizerModule defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxEMF16OWb6i",
        "outputId": "fb084ada-8140-4999-a31f-d2da89c144ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SyllabusGeneratorRouter defined.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import dspy\n",
        "from typing import Optional, List, Dict, Any # Ensure these are imported\n",
        "\n",
        "# --- Signature for NO Resources (Revised & Detailed) ---\n",
        "class SyllabusNoResourcesSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    **You are an expert AI Syllabus Creator.**\n",
        "    Your **sole task** is to generate or modify a learning syllabus based **exclusively** on the provided 'learning_conversation' history.\n",
        "    **No external resources, documents, or summaries are provided for this specific task, nor should any be assumed or hallucinated.** You must work only with the conversational context.\n",
        "\n",
        "    **Your Goal:** Produce a well-structured, practical, and coherent syllabus XML.\n",
        "\n",
        "    **Mode of Operation (Infer from 'learning_conversation'):**\n",
        "    1.  **Modification:** If the 'learning_conversation' contains a previously presented syllabus (typically in `<syllabus>...</syllabus>` tags from an 'assistant' or 'model' role) AND subsequent user messages clearly provide feedback or request changes to THAT specific syllabus, your primary goal is to **modify that most recent relevant syllabus**. Accurately incorporate all user feedback.\n",
        "    2.  **Generation:** If the 'learning_conversation' indicates a new learning topic, or if no prior syllabus for the current topic is evident, or if the user explicitly requests a fresh start, your goal is to **generate a new syllabus from scratch** based on the user's stated goals, experience level, and desired topic derived from the conversation.\n",
        "\n",
        "    **Syllabus Structure Requirements:**\n",
        "    *   Organize into 2 to 5 distinct learning phases.\n",
        "    *   Each phase must contain 2 to 4 specific lessons or topics.\n",
        "    *   Arrange phases and lessons in a logical, progressive order, building complexity incrementally.\n",
        "\n",
        "    **Lesson Detail Requirements (for each lesson):**\n",
        "    *   `Topic`: A clear, concise title.\n",
        "    *   `Keywords`: A list of 3-5 key terms or concepts.\n",
        "    *   `Objective`: 1-2 sentences describing what the learner should understand/do post-lesson.\n",
        "    *   `Focus`: 1-2 sentences on the main emphasis or key takeaways.\n",
        "\n",
        "    **Output Format: CRITICAL - Output ONLY the complete syllabus XML structure enclosed within `<syllabus>` and `</syllabus>` tags. Do not include any other conversational text, explanations, or apologies before or after the XML block.**\n",
        "    \"\"\"\n",
        "    learning_conversation = dspy.InputField(desc=\"The complete, ordered conversation history. This is your ONLY source of information for user needs, previous syllabi (if any, enclosed in <syllabus> tags), and feedback.\")\n",
        "    syllabus_xml = dspy.OutputField(desc=\"The complete generated or modified syllabus as a single XML string, starting with <syllabus> and ending with </syllabus>.\")\n",
        "\n",
        "# --- Signature for LIGHT/RAW Text Resources (Revised & Detailed) ---\n",
        "class SyllabusWithRawTextSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    **You are an expert AI Syllabus Creator.**\n",
        "    Your **sole task** is to generate or modify a learning syllabus using the 'learning_conversation' history AND the provided 'raw_resource_excerpts_json'.\n",
        "    **Crucial Context: The 'raw_resource_excerpts_json' you receive contains snippets of actual learning materials. This detailed content is exclusive to you for syllabus creation; no other AI agent has processed or summarized it for this purpose. Your thorough analysis and direct integration of this raw text are paramount.**\n",
        "\n",
        "    **Your Goal:** Produce a well-structured syllabus XML that is deeply informed by both the user's needs (from conversation) and the specific content of the raw text excerpts.\n",
        "\n",
        "    **'raw_resource_excerpts_json' Input:** This is a JSON string representing an object. Keys are resource identifiers (e.g., filenames), and values are the corresponding short raw text excerpts.\n",
        "\n",
        "    **Mode of Operation (Infer from 'learning_conversation', integrate 'raw_resource_excerpts_json'):**\n",
        "    1.  **Modification:** If the 'learning_conversation' contains a prior syllabus (in `<syllabus>` tags) and user feedback, **modify that syllabus**. Directly integrate relevant information, concepts, definitions, and examples from the 'raw_resource_excerpts_json' to address the feedback and enrich the syllabus.\n",
        "    2.  **Generation:** If generating anew, **use both the 'learning_conversation' and the 'raw_resource_excerpts_json' from scratch**. The raw text should heavily influence the topics, lesson objectives, keywords, and focus points. For instance, if an excerpt details three key steps for a process, that could become a lesson or part of one.\n",
        "\n",
        "    **Syllabus Structure & Lesson Detail Requirements:** (Same as SyllabusNoResourcesSignature: 2-5 phases, 2-4 lessons/phase; Topic, Keywords, Objective, Focus per lesson)\n",
        "    *   **Ensure keywords, objectives, and focus points directly reflect or are inspired by the content found in the 'raw_resource_excerpts_json'.**\n",
        "\n",
        "    **Output Format: CRITICAL - Output ONLY the complete syllabus XML structure enclosed within `<syllabus>` and `</syllabus>` tags. No other text.**\n",
        "    \"\"\"\n",
        "    learning_conversation = dspy.InputField(desc=\"Complete conversation history. May contain prior syllabi (in <syllabus> tags) and feedback. This defines user needs.\")\n",
        "    raw_resource_excerpts_json = dspy.InputField(desc=\"A JSON string: an object mapping resource IDs to their raw text snippets. This is your primary source for detailed content.\")\n",
        "    syllabus_xml = dspy.OutputField(desc=\"The complete syllabus XML, reflecting deep integration of raw text excerpts.\")\n",
        "\n",
        "# --- Signature for HEAVY Resources (using Summaries) (Revised & Detailed) ---\n",
        "class SyllabusWithSummariesSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    **You are an expert AI Syllabus Creator.**\n",
        "    Your **sole task** is to generate or modify a learning syllabus using the 'learning_conversation' history AND the provided 'resource_summaries_json'.\n",
        "    **Crucial Context: The 'resource_summaries_json' you receive contains structured analytical summaries of larger learning materials (e.g., identifying relevant topics, core concepts, structural notes). This summarized information is exclusive to you for syllabus creation. Your task is to synthesize these expert summaries with the user's conversational needs.**\n",
        "\n",
        "    **Your Goal:** Produce a well-structured syllabus XML that effectively translates the insights from the resource summaries into a practical learning plan aligned with user goals.\n",
        "\n",
        "    **'resource_summaries_json' Input:** This is a JSON string representing an object. Keys are resource identifiers, and values are individual JSON summary objects for each resource (each containing keys like 'primary_topics_relevant_to_conversation', 'core_concepts_relevant_to_conversation', 'contextual_notes_for_syllabus', etc.).\n",
        "\n",
        "    **Mode of Operation (Infer from 'learning_conversation', integrate 'resource_summaries_json'):**\n",
        "    1.  **Modification:** If 'learning_conversation' shows a prior syllabus (in `<syllabus>` tags) and user feedback, **modify that syllabus**. Intelligently weave the topics, concepts, and contextual notes from the 'resource_summaries_json' to address feedback and improve the syllabus.\n",
        "    2.  **Generation:** If generating anew, **use both 'learning_conversation' and 'resource_summaries_json' from scratch**. The summaries (especially 'primary_topics_relevant', 'core_concepts_relevant', 'contextual_notes_for_syllabus') should guide the choice of phases, lesson topics, keywords, objectives, and focus.\n",
        "\n",
        "    **Syllabus Structure & Lesson Detail Requirements:** (Same as SyllabusNoResourcesSignature: 2-5 phases, 2-4 lessons/phase; Topic, Keywords, Objective, Focus per lesson)\n",
        "    *   **Ensure lesson content is strongly guided by the insights presented in the 'resource_summaries_json'.**\n",
        "\n",
        "    **Output Format: CRITICAL - Output ONLY the complete syllabus XML structure enclosed within `<syllabus>` and `</syllabus>` tags. No other text.**\n",
        "    \"\"\"\n",
        "    learning_conversation = dspy.InputField(desc=\"Complete conversation history. Defines user needs and may contain prior syllabi/feedback.\")\n",
        "    resource_summaries_json = dspy.InputField(desc=\"A JSON string: an object mapping resource IDs to their structured summary objects. This provides high-level insights and content pointers.\")\n",
        "    syllabus_xml = dspy.OutputField(desc=\"The complete syllabus XML, reflecting effective use of resource summaries.\")\n",
        "\n",
        "\n",
        "class SyllabusGeneratorRouter(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use ChainOfThought for potentially better structured output for syllabus generation\n",
        "        self.gen_no_resources = dspy.Predict(SyllabusNoResourcesSignature)\n",
        "        self.gen_with_raw = dspy.Predict(SyllabusWithRawTextSignature)\n",
        "        self.gen_with_summaries = dspy.Predict(SyllabusWithSummariesSignature)\n",
        "\n",
        "    def forward(self,\n",
        "                conversation_str: str,\n",
        "                #task_description: str,\n",
        "                resource_type: str, # \"NONE\", \"RAW_TEXT\", \"SUMMARIES\"\n",
        "                resource_content: Optional[str] = None, # Actual raw text or JSON summaries string\n",
        "                # existing_syllabus_xml: Optional[str] = None Not needed\n",
        "               ) -> str: # Returns the syllabus_xml string\n",
        "\n",
        "        common_args = {\n",
        "            \"learning_conversation\": conversation_str,\n",
        "            #\"task_description\": #task_description,\n",
        "            # \"existing_syllabus_xml\": existing_syllabus_xml if existing_syllabus_xml else \"None\"\n",
        "        }\n",
        "\n",
        "        if resource_type == \"NONE\":\n",
        "            prediction = self.gen_no_resources(**common_args)\n",
        "        elif resource_type == \"RAW_TEXT\":\n",
        "            if not resource_content: raise ValueError(\"resource_content needed for RAW_TEXT type\")\n",
        "            prediction = self.gen_with_raw(raw_resource_excerpts=resource_content, **common_args)\n",
        "        elif resource_type == \"SUMMARIES\":\n",
        "            if not resource_content: raise ValueError(\"resource_content needed for SUMMARIES type (should be JSON string)\")\n",
        "            prediction = self.gen_with_summaries(resource_summaries_json=resource_content, **common_args)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown resource_type: {resource_type}\")\n",
        "\n",
        "        # Post-process to ensure <syllabus> tags, as in your previous SyllabusGenerator\n",
        "        content = prediction.syllabus_xml.strip()\n",
        "        if not content.lower().startswith(\"<syllabus>\"):\n",
        "            content = f\"<syllabus>\\n{content}\"\n",
        "        if not content.lower().endswith(\"</syllabus>\"):\n",
        "            content = f\"{content}\\n</syllabus>\"\n",
        "        return content\n",
        "\n",
        "print(\"SyllabusGeneratorRouter defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convo Manager, Prompt Generator"
      ],
      "metadata": {
        "id": "mP_GukTUjXlZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp19iHyzJqIL",
        "outputId": "1a6bcc31-c53c-4160-e6c0-cbd79587cfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Revised SyllabusNegotiationSignature with stricter display_text rules defined.\n",
            "ConversationManager module using revised signature defined.\n",
            "PersonaPromptBodyPredictSignature (for dspy.Predict, outputs only text) defined.\n",
            "PersonaPromptGenerator (using dspy.Predict and PersonaPromptBodyPredictSignature) defined.\n",
            "DSPy Signatures and Modules defined.\n"
          ]
        }
      ],
      "source": [
        "class InitialResourceSummarySignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are an AI Resource Analyzer.\n",
        "    Analyze the provided learning resource excerpts (JSON object with filenames as keys and text content as values).\n",
        "    For EACH resource, identify its primary subject, main topics, and key concepts/information.\n",
        "    Optionally infer the content type/style.\n",
        "    Present your analysis for each resource separately in a single text block.\n",
        "    Your goal is to allow a Conversation Manager to quickly grasp the nature and content of each resource.\n",
        "    Example of output format:\n",
        "    **Resource: 'filename.txt'**\n",
        "    This excerpt appears to be...\n",
        "    *   Main Topics: ...\n",
        "    *   Key Information: ...\n",
        "    ---\n",
        "    **Resource: 'another_file.pdf'**\n",
        "    ...\n",
        "    \"\"\"\n",
        "    # Input Field Which is Clearly Input\n",
        "    resource_excerpts_json = dspy.InputField(desc=\"A JSON string representing a dictionary where keys are resource identifiers (e.g., filenames) and values are the truncated text content of that resource.\")\n",
        "    summary_report = dspy.OutputField(desc=\"A formatted text report summarizing each resource excerpt as per the main instruction.\")\n",
        "\n",
        "class InitialResourceSummarizer(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.summarize = dspy.Predict(InitialResourceSummarySignature)\n",
        "\n",
        "    def forward(self, extracted_basedata_dict: Dict[str, str]):\n",
        "        # Convert dict to JSON string for the input field\n",
        "        json_input_str = json.dumps(extracted_basedata_dict, indent=2)\n",
        "        prediction = self.summarize(resource_excerpts_json=json_input_str)\n",
        "        return prediction.summary_report # Means Return Output and There is\n",
        "\n",
        "# --- Conversation Manager ---\n",
        "import dspy\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "class SyllabusNegotiationSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    **You are an expert AI Conversation Manager.**\n",
        "    Your primary role is to facilitate a conversation to define requirements for a learning syllabus by analyzing the inputs and determining the next system action.\n",
        "\n",
        "    **Inputs to Analyze:**\n",
        "    1.  `conversation_history`: The full record of previous turns.\n",
        "    2.  `current_syllabus_xml`: The latest syllabus draft (XML string or \"None\").\n",
        "    3.  `user_input`: The most recent message from the user.\n",
        "\n",
        "    **Your Task:** Based on the inputs, determine the single most appropriate `action_code` from the list below.\n",
        "    Additionally, if the action is purely conversational (`CONVERSE`), provide the `display_text` for the user.\n",
        "    For all other action codes (`GENERATE`, `MODIFY`, `FINALIZE`, `PERSONA`), the `display_text` **MUST be an empty string or a placeholder like \"[NO_DISPLAY_TEXT]\"** as the system will handle the next step non-conversationally or with a dedicated prompter.\n",
        "\n",
        "    **Action Codes & Conditions:**\n",
        "    *   `GENERATE`: Output this if sufficient initial information (topic, experience, goals) has been gathered from the conversation to request the *very first* syllabus draft.\n",
        "    *   `MODIFY`: Output this if a syllabus exists (indicated by a non-\"None\" `current_syllabus_xml` or visible in `conversation_history`) AND the `user_input` (or recent history) provides clear feedback or requests changes to that existing syllabus.\n",
        "    *   `FINALIZE`: Output this if the `user_input` (or recent history) explicitly confirms that the user is satisfied with the *most recent* syllabus presented and no further changes are needed.\n",
        "    *   `PERSONA`: Output this if the conversation indicates the user has just provided their preferred learning style (this action signals readiness for the system to generate the tutor's persona prompt). `display_text` can be a very brief acknowledgment like \"Got it, thanks!\" or empty.\n",
        "    *   `CONVERSE`: Output this for all other situations. This includes asking clarifying questions, acknowledging user statements, providing general responses, or when a previous action (like syllabus generation) has just completed and you need to prompt the user for feedback on that artifact (which would be visible in the updated `conversation_history`).\n",
        "\n",
        "    **Output Field Rules:**\n",
        "    - `action_code`: MUST be one of the specified codes.\n",
        "    - `display_text`:\n",
        "        - For `CONVERSE`: Provide the natural language response to the user.\n",
        "        - For `GENERATE`, `MODIFY`, `FINALIZE`: MUST be empty or \"[NO_DISPLAY_TEXT]\".\n",
        "        - For `PERSONA`: Can be empty, \"[NO_DISPLAY_TEXT]\", or a very brief acknowledgment.\n",
        "    \"\"\"\n",
        "    conversation_history = dspy.InputField(desc=\"Previous turns in the conversation, formatted as a multi-line string. This may contain previously presented syllabi.\")\n",
        "    current_syllabus_xml = dspy.InputField(desc=\"The current draft syllabus XML (<syllabus>...</syllabus>), or the string 'None' if no syllabus has been successfully generated or focused on yet.\")\n",
        "    user_input = dspy.InputField(desc=\"The user's latest message that needs processing.\")\n",
        "    # resource_summary = dspy.InputField(desc=\"A brief summary/overview of user-provided learning resources, or 'None' if no resources are relevant or provided.\")\n",
        "\n",
        "    action_code = dspy.OutputField(desc=\"One of: GENERATE, MODIFY, FINALIZE, PERSONA, CONVERSE.\")\n",
        "    display_text = dspy.OutputField(desc=\"The conversational text response for the user. MUST be empty or '[NO_DISPLAY_TEXT]' if action_code is GENERATE, MODIFY, or FINALIZE. Can be brief for PERSONA.\")\n",
        "\n",
        "print(\"Revised SyllabusNegotiationSignature with stricter display_text rules defined.\")\n",
        "\n",
        "class ConversationManager(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Using Predict as the Signature is now quite detailed.\n",
        "        # If the LLM struggles to follow the conditional logic for display_text,\n",
        "        # ChainOfThought might be needed, or more explicit examples in the Signature.\n",
        "        self.manage = dspy.Predict(SyllabusNegotiationSignature)\n",
        "\n",
        "    def forward(self, conversation_history: str, current_syllabus_xml: str, user_input: str):\n",
        "        # The user_input is the latest turn, but the full context is in conversation_history.\n",
        "        # The Signature is designed to look at the user_input in context of the whole history.\n",
        "        prediction = self.manage(\n",
        "            conversation_history=conversation_history,\n",
        "            current_syllabus_xml=current_syllabus_xml,\n",
        "            user_input=user_input, # Pass the latest user input specifically\n",
        "            # resource_summary=resource_summary\n",
        "        )\n",
        "\n",
        "        action = prediction.action_code.strip().upper()\n",
        "        text_to_display = prediction.display_text.strip()\n",
        "\n",
        "        # Enforce display_text rules based on the Signature's instructions\n",
        "        if action in [\"GENERATE\", \"MODIFY\", \"FINALIZE\"]:\n",
        "            if text_to_display and text_to_display.upper() != \"[NO_DISPLAY_TEXT]\":\n",
        "                print(f\"[ConversationManager WARNING] Action '{action}' returned with display_text: '{text_to_display}'. Forcing to empty as per rules.\")\n",
        "            text_to_display = \"\" # Enforce empty\n",
        "        elif text_to_display.upper() == \"[NO_DISPLAY_TEXT]\":\n",
        "            text_to_display = \"\"\n",
        "\n",
        "        # For PERSONA, allow brief confirmation or empty. If it's placeholder, make empty.\n",
        "        if action == \"PERSONA\" and text_to_display.upper() == \"[NO_DISPLAY_TEXT]\":\n",
        "            text_to_display = \"\"\n",
        "\n",
        "        return action, text_to_display\n",
        "\n",
        "print(\"ConversationManager module using revised signature defined.\")\n",
        "\n",
        "class LearningStyleSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are an AI assistant. The user has just finalized a learning syllabus.\n",
        "    Your goal is to formulate a concise and engaging question to prompt the user about their preferred learning style and the kind of AI tutor personality they'd find most effective for the subject matter (discernible from the history).\n",
        "    Encourage specific details beyond generic answers (e.g., interaction style, content format like examples/theory/analogies, pace, feedback type).\n",
        "    Output ONLY the question itself.\n",
        "    \"\"\"\n",
        "    conversation_history_with_final_syllabus = dspy.InputField(desc=\"Full conversation history, including the finalized syllabus (which might be the last model_artifact turn).\")\n",
        "    question_to_user = dspy.OutputField(desc=\"The single, clear question to ask the user about their learning preferences.\")\n",
        "\n",
        "class LearningStyleQuestioner(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ask = dspy.Predict(LearningStyleSignature)\n",
        "\n",
        "    def forward(self, conversation_history_str: str):\n",
        "        prediction = self.ask(conversation_history_with_final_syllabus=conversation_history_str)\n",
        "        return prediction.question_to_user\n",
        "\n",
        "# --- Persona Prompt Generator (Body only) ---\n",
        "import dspy\n",
        "import json # For parsing\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "import dspy\n",
        "import json\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "# --- Revised Signature for Persona Prompt Body (for dspy.Predict) ---\n",
        "class PersonaPromptBodyPredictSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    **You are an AI Persona Architect.**\n",
        "    Your goal is to generate the main body of a system prompt for an AI Tutor.\n",
        "    This prompt body should accurately reflect the user's desired teaching style, personality, depth preferences, and subject matter, all derived from the provided 'conversation_history_with_style_and_syllabus_context'.\n",
        "\n",
        "    **The prompt body MUST include:**\n",
        "    1.  **Clear Persona Definition:** (e.g., AI Tutor's name like 'Synapse', its subject specialization, and its core mission).\n",
        "    2.  **Core Principles Section:** (Detail the tutor's personality, teaching philosophy, desired traits, inspirational figures and how to emulate them, key emphasis areas. Use bullet points for clarity).\n",
        "    3.  **Teaching Approach / Methodology Section:** (Outline specific methods: clarity/explanation style, interaction style, handling depth, practical elements, guidance vs. direct answers balance).\n",
        "    4.  **Overall Goal Statement:** (A sentence summarizing the ultimate aim, e.g., \"Your goal is to foster deep understanding...\").\n",
        "\n",
        "    **CRITICAL: The generated text should be ONLY the prompt body itself, ready to have the syllabus appended to it externally. DO NOT include phrases like \"Here is the syllabus...\" or the {{SYLLABUS_SECTION}} placeholder.**\n",
        "    Focus solely on crafting the persona and teaching instructions for the tutor.\n",
        "    \"\"\"\n",
        "    conversation_history_with_style_and_syllabus_context = dspy.InputField(\n",
        "        desc=\"Full conversation history, including the finalized syllabus context (to understand the subject) and the user's stated learning style preferences (to inform persona and teaching approach).\"\n",
        "    )\n",
        "\n",
        "    # Only one output field: the prompt body text itself.\n",
        "    prompt_body_text = dspy.OutputField(\n",
        "        desc=\"The complete system prompt body for the AI Tutor, ending just before where the syllabus would be introduced by the calling system.\"\n",
        "    )\n",
        "\n",
        "print(\"PersonaPromptBodyPredictSignature (for dspy.Predict, outputs only text) defined.\")\n",
        "class PersonaPromptGenerator(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Switched to dspy.Predict with the new signature\n",
        "        self.generate_prompt_body = dspy.Predict(PersonaPromptBodyPredictSignature)\n",
        "\n",
        "    def forward(self,conversation_history_str: str):\n",
        "      try:\n",
        "        # Call the dspy.Predict instance\n",
        "        prediction_object = self.generate_prompt_body(\n",
        "            conversation_history_with_style_and_syllabus_context=conversation_history_str\n",
        "        )\n",
        "\n",
        "        prompt_body = prediction_object.prompt_body_text\n",
        "\n",
        "        if not prompt_body or not prompt_body.strip():\n",
        "            print(\"[PersonaPromptGenerator] Error: LLM returned an empty or whitespace-only prompt body.\")\n",
        "            return None # Or a default fallback string\n",
        "\n",
        "        return prompt_body.strip() # Return the generated text\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"[PersonaPromptGenerator] Error in forward pass: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None # Or a default fallback string\n",
        "\n",
        "print(\"PersonaPromptGenerator (using dspy.Predict and PersonaPromptBodyPredictSignature) defined.\")\n",
        "# --- Generic Explainer Interaction Signature ---\n",
        "class GenericInteractionSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Follow the comprehensive system_instructions provided, which define your role, persona, and current task (e.g., acting as an AI Tutor explaining a syllabus topic).\n",
        "    Respond to the user's query based on these instructions and the conversation history.\n",
        "    \"\"\"\n",
        "    system_instructions = dspy.InputField(desc=\"The full system prompt defining your current role, persona, how to interact, and often the learning material (like a syllabus).\")\n",
        "    history = dspy.InputField(desc=\"Recent conversation history relevant to the current interaction.\")\n",
        "    user_query = dspy.InputField(desc=\"The user's current question or statement.\")\n",
        "    response = dspy.OutputField(desc=\"Your response, adhering to the system_instructions.\")\n",
        "\n",
        "print(\"DSPy Signatures and Modules defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iukKZDyAMo4S"
      },
      "outputs": [],
      "source": [
        "def resource_summary_extractor(\n",
        "    extracted_basedata_dict: dict,\n",
        "    summary_router_instance, # Pass the summary_router object\n",
        "    conversation_history_str: str\n",
        ") -> str:\n",
        "    if not extracted_basedata_dict:\n",
        "        print(\"Warning: extracted_basedata_dict is empty. No summaries to process.\")\n",
        "        return \"{}\"\n",
        "\n",
        "    extracted_summary_list = [\n",
        "        summary_router_instance.forward(\n",
        "            resource_content=value,\n",
        "            resource_identifier=key,\n",
        "            conversation_history_str=conversation_history_str\n",
        "        )\n",
        "        for key, value in extracted_basedata_dict.items()\n",
        "    ]\n",
        "\n",
        "    #\n",
        "    aggregated_summaries_dict = {}\n",
        "    for summary_dict_item in extracted_summary_list:\n",
        "\n",
        "        if isinstance(summary_dict_item, dict) and \"resource_identifier\" in summary_dict_item:\n",
        "            aggregated_summaries_dict[summary_dict_item[\"resource_identifier\"]] = summary_dict_item\n",
        "        else:\n",
        "\n",
        "            print(f\"Warning: Skipping invalid or incomplete summary object: {summary_dict_item}\")\n",
        "\n",
        "    # 3. Convert the aggregated summaries to a JSON string\n",
        "    resource_summaries_json_string_for_signature: str\n",
        "    if aggregated_summaries_dict:\n",
        "        try:\n",
        "            resource_summaries_json_string_for_signature = json.dumps(aggregated_summaries_dict, indent=2)\n",
        "        except TypeError as e:\n",
        "            print(f\"Error: Could not serialize aggregated_summaries_dict to JSON: {e}\")\n",
        "            print(f\"Problematic data: {aggregated_summaries_dict}\")\n",
        "            resource_summaries_json_string_for_signature = \"{}\" # Fallback\n",
        "    else:\n",
        "        resource_summaries_json_string_for_signature = \"{}\"\n",
        "\n",
        "        print(\"Warning: No valid summaries were aggregated to create a JSON string.\")\n",
        "\n",
        "    return resource_summaries_json_string_for_signature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orchestrator"
      ],
      "metadata": {
        "id": "7fX4KsuHjtW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFQeA5FeNxOp"
      },
      "outputs": [],
      "source": [
        "import dspy\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Optional, Any, Tuple\n",
        "\n",
        "# --- Main Negotiation Function using DSPy (Revised for new Syllabus Signatures) ---\n",
        "def negotiate_syllabus_chat_dspy(\n",
        "    resource_paths: Optional[List[str]] = None,\n",
        "    verbose: bool = True\n",
        ") -> Optional[Tuple[str, List[Dict[str, Any]]]]:\n",
        "\n",
        "    if resource_paths is None: resource_paths = []\n",
        "    conversation_history: List[Dict[str, Any]] = []\n",
        "    current_syllabus_xml_content: Optional[str] = None # is it needed yep to store the Present Syllabus\n",
        "    finalization_requested = False\n",
        "\n",
        "    # To determine resource type and content for SyllabusGeneratorRouter\n",
        "    resource_type_for_syllabus_gen: str = \"NONE\"\n",
        "    resource_content_for_syllabus_gen: Optional[str] = \"No resources were processed or provided.\"\n",
        "\n",
        "    # --- Instantiate DSPy Modules ---\n",
        "    try:\n",
        "        if verbose: print(\"[System] Initializing DSPy modules for negotiation...\")\n",
        "        initial_summarizer = InitialResourceSummarizer()\n",
        "        convo_manager = ConversationManager() # Signature should guide it on actions\n",
        "        syllabus_router = SyllabusGeneratorRouter() # This now uses the refined internal signatures\n",
        "        style_asker = LearningStyleQuestioner()\n",
        "        dynamic_summarizer_module = DynamicResourceSummarizerModule()\n",
        "        if verbose: print(\"[System] Negotiation DSPy modules initialized.\")\n",
        "    except Exception as e:\n",
        "        if verbose: print(f\"[System Error] Failed to instantiate negotiation DSPy modules: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Initial Resource Processing ---\n",
        "    if resource_paths:\n",
        "        if verbose: print(f\"\\n[System] Processing {len(resource_paths)} provided resource paths for syllabus generation context...\")\n",
        "\n",
        "\n",
        "        total_chars = 0\n",
        "        all_raw_texts_dict = {} # For \"RAW_TEXT\" type\n",
        "        for path in resource_paths:\n",
        "            content = extract_text_from_txt_file(path)\n",
        "            if content:\n",
        "                total_chars += len(content)\n",
        "                filename = os.path.basename(path)\n",
        "\n",
        "                all_raw_texts_dict[filename] = content\n",
        "\n",
        "\n",
        "        if not all_raw_texts_dict:\n",
        "            if verbose: print(\"[System] No text could be extracted from provided resource paths.\")\n",
        "            resource_type_for_syllabus_gen = \"NONE\"\n",
        "            resource_content_for_syllabus_gen = \"No text extracted from provided paths.\"\n",
        "\n",
        "        elif total_chars > 70000: # Heuristic: if total raw text is large, summarize all\n",
        "            resource_type_for_syllabus_gen = \"SUMMARIES\"\n",
        "            if verbose: print(f\"[System] Total chars {total_chars} > threshold. Will generate dynamic summaries for ALL resources.\")\n",
        "            resource_content_for_syllabus_gen = \"Not None will be generated Dynamically\"\n",
        "            light_raw_excerpts_dict = {}\n",
        "            for filename, full_content in all_raw_texts_dict.items():\n",
        "                light_raw_excerpts_dict[filename] = full_content[:20000]\n",
        "\n",
        "\n",
        "\n",
        "        else: # Total chars is not too large, pass as \"RAW_TEXT\" (truncated snippets)\n",
        "            resource_type_for_syllabus_gen = \"RAW_TEXT\"\n",
        "            light_raw_excerpts_dict = {}\n",
        "            for filename, full_content in all_raw_texts_dict.items():\n",
        "                light_raw_excerpts_dict[filename] = full_content[:20000] # Truncate for \"light\" raw text\n",
        "            resource_content_for_syllabus_gen = json.dumps(light_raw_excerpts_dict, indent=2)\n",
        "            if verbose: print(f\"[System] Using truncated raw text snippets (JSON). Length: {len(resource_content_for_syllabus_gen)}\")\n",
        "    else:\n",
        "        resource_type_for_syllabus_gen = \"NONE\"\n",
        "        resource_content_for_syllabus_gen = \"No resources provided.\"\n",
        "\n",
        "\n",
        "    # --- Initial Greeting (can now use the processed resource info status) ---\n",
        "    ai_greeting_text = \"Hello! What topic are you interested in learning about today and also if you have any Resources Provide at the Start?\"\n",
        "    # conversation_history.append({'role': 'model', 'parts': [ai_turn_text]})\n",
        "    # print(conversation_history)\n",
        "    if resource_type_for_syllabus_gen != \"NONE\" and \"Error\" not in resource_content_for_syllabus_gen:\n",
        "        ai_turn_summarizer = initial_summarizer(extracted_basedata_dict=light_raw_excerpts_dict)\n",
        "        print(ai_turn_summarizer)\n",
        "    elif resource_paths: # Paths were given, but processing might have failed\n",
        "        ai_turn_text += \"\\nI see you provided some resources, but I had a bit of trouble processing them fully. Let's proceed with our conversation.\"\n",
        "        conversation_history.append({'role': 'model', 'parts': [ai_turn_text]})\n",
        "\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Starting Syllabus Negotiation (DSPy) ---\")\n",
        "    conversation_history.append({'role': 'model', 'parts': [ai_greeting_text]})\n",
        "    print(f\"AI: {ai_greeting_text}\")\n",
        "\n",
        "    print(resource_content_for_syllabus_gen)\n",
        "\n",
        "    while True:\n",
        "\n",
        "\n",
        "        try:\n",
        "            user_input = input(\"You: \").strip()\n",
        "        except EOFError: #\n",
        "            if verbose: print(\"\\nAI: Session ended by user (EOF).\")\n",
        "            return None\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]: # ... (same as before) ...\n",
        "            if verbose: print(\"AI: Okay, ending syllabus planning. Goodbye!\")\n",
        "            return None\n",
        "        if not user_input: continue\n",
        "        conversation_history.append({'role': 'user', 'parts': [user_input]})\n",
        "\n",
        "        if len(conversation_history) <=2 and resource_type_for_syllabus_gen != \"NONE\":\n",
        "            conversation_history.append({'role': 'model', 'parts': [ai_turn_summarizer]})\n",
        "            # Intial Summary is sent\n",
        "\n",
        "\n",
        "        try:\n",
        "            history_str = format_history_for_dspy(conversation_history)\n",
        "            syllabus_str_for_manager = current_syllabus_xml_content if current_syllabus_xml_content else \"None\"\n",
        "\n",
        "\n",
        "            if verbose: print(f\"[System Debug] Calling ConversationManager...\")\n",
        "            manager_prediction = convo_manager(\n",
        "                conversation_history=history_str,\n",
        "                current_syllabus_xml=syllabus_str_for_manager,\n",
        "                user_input=user_input, # User input is part of history_str\n",
        "                #resource_summary=resource_summary_for_convo_manager # i feel its not needed\n",
        "            )\n",
        "            action_code_str_from_manager, display_text_from_manager = manager_prediction\n",
        "            action_code = action_code_str_from_manager # Already stripped and uppercased\n",
        "            display_text = display_text_from_manager   # Already stripped\n",
        "\n",
        "\n",
        "\n",
        "            if verbose: # ... (same logging as before) ...\n",
        "                if display_text: print(f\"AI: {display_text}\")\n",
        "                else: print(f\"AI: [Received action '{action_code}' with no display text from manager]\")\n",
        "\n",
        "            if display_text: conversation_history.append({'role': 'model', 'parts': [display_text]})\n",
        "\n",
        "\n",
        "            if action_code == \"GENERATE\" or action_code == \"MODIFY\":\n",
        "                task_type_str = \"generation\" if action_code == \"GENERATE\" else \"modification\"\n",
        "                if verbose: print(f\"[System] Syllabus {task_type_str} requested by manager...\")\n",
        "\n",
        "\n",
        "                try:\n",
        "\n",
        "                    if resource_type_for_syllabus_gen == \"SUMMARIES\":\n",
        "                        resource_content_for_syllabus_gen = resource_summary_extractor(\n",
        "                            extracted_basedata_dict=light_raw_excerpts_dict,\n",
        "                            summary_router_instance=dynamic_summarizer_module,\n",
        "                            conversation_history_str=history_str\n",
        "                        )\n",
        "                    generated_xml = syllabus_router.forward(\n",
        "                        conversation_str=history_str, # Full history for LLM to infer task\n",
        "                        resource_type=resource_type_for_syllabus_gen,\n",
        "                        resource_content=resource_content_for_syllabus_gen if resource_type_for_syllabus_gen != \"NONE\" else None\n",
        "                    ) # Works for every type of Syllabus Signature\n",
        "                    print({resource_content_for_syllabus_gen})\n",
        "\n",
        "\n",
        "                    if generated_xml and not generated_xml.upper().startswith((\"[ERROR\", \"[BLOCKED\")):\n",
        "                        current_syllabus_xml_content = generated_xml # Store the full XML block\n",
        "                        if verbose: print(f\"\\n[System presenting syllabus]\\n{current_syllabus_xml_content}\\n\")\n",
        "                        conversation_history.append({'role': 'model_artifact', 'parts': [current_syllabus_xml_content]})\n",
        "                    else:\n",
        "                        err_msg = f\"AI: Sorry, I encountered an issue during syllabus {task_type_str}. {(generated_xml or 'No details.')[:200]}\"\n",
        "                        if verbose: print(err_msg)\n",
        "                        conversation_history.append({'role': 'model', 'parts': [err_msg]})\n",
        "\n",
        "                except Exception as e_gen:\n",
        "                    if verbose: print(f\"[System Error] Syllabus Router call failed: {e_gen}\")\n",
        "                    conversation_history.append({'role': 'model', 'parts': [f\"Sorry, there was an error with syllabus {task_type_str}.\"]})\n",
        "\n",
        "\n",
        "            elif action_code == \"FINALIZE\":\n",
        "                if verbose: print(\"[System] Finalization requested by manager.\")\n",
        "                if current_syllabus_xml_content:\n",
        "                    finalization_requested = True\n",
        "                    if not (\"learning style\" in display_text.lower() or \\\n",
        "                            \"prefer to learn\" in display_text.lower() or \\\n",
        "                            \"teach\" in display_text.lower()):\n",
        "                        try:\n",
        "                            if verbose: print(\"[System] Manager didn't ask style question on FINALIZE, calling StyleAsker...\")\n",
        "                            style_question_prediction = style_asker(\n",
        "                                conversation_history_str=format_history_for_dspy(conversation_history)\n",
        "                            )\n",
        "                            style_question_text = style_question_prediction.strip()\n",
        "                            if verbose: print(f\"AI: {style_question_text}\")\n",
        "                            conversation_history.append({'role': 'model', 'parts': [style_question_text]})\n",
        "                        except Exception as e_ask:\n",
        "                            if verbose: print(f\"[System Error] LearningStyleQuestioner call failed: {e_ask}\")\n",
        "                            fallback_style_q = \"Great, the syllabus is set! To help tailor the learning experience, how do you prefer to learn?\"\n",
        "                            if verbose: print(f\"AI: {fallback_style_q}\")\n",
        "                            conversation_history.append({'role': 'model', 'parts': [fallback_style_q]})\n",
        "                else: # ... same fallback ...\n",
        "                    if verbose: print(\"[System Warning] Finalization requested by manager, but no syllabus exists.\")\n",
        "                    no_syllabus_msg = \"It seems we don't have a syllabus to finalize yet. Shall we create or refine one first?\"\n",
        "                    if display_text != no_syllabus_msg:\n",
        "                        conversation_history.append({'role': 'model', 'parts': [no_syllabus_msg]})\n",
        "                        if verbose: print(f\"AI: {no_syllabus_msg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            elif action_code == \"PERSONA\":\n",
        "                if verbose: print(\"[System] Persona prompt generation triggered by manager.\")\n",
        "                if finalization_requested and current_syllabus_xml_content:\n",
        "                    if verbose: print(\"[System] Syllabus finalized. Learning style assumed in history. Negotiation complete.]\")\n",
        "                    return current_syllabus_xml_content, conversation_history # EXIT\n",
        "                else:\n",
        "                    if verbose: print(\"[System Warning] Persona trigger from manager, but conditions not fully met.\")\n",
        "                    conditions_not_met_msg = \"Before we tailor the tutor, we need a finalized syllabus and your learning style. Let's make sure those are set.\"\n",
        "                    if display_text != conditions_not_met_msg:\n",
        "                        conversation_history.append({'role': 'model', 'parts': [conditions_not_met_msg]})\n",
        "                        if verbose: print(f\"AI: {conditions_not_met_msg}\")\n",
        "\n",
        "\n",
        "            elif action_code == \"CONVERSE\":\n",
        "                pass\n",
        "            else:\n",
        "                if verbose: print(f\"[System Warning] Unknown action_code '{action_code}' from ConversationManager. Treating as CONVERSE.\")\n",
        "                if not display_text:\n",
        "                    fallback_converse = \"I'm not sure how to proceed with that. Could you clarify?\"\n",
        "                    if verbose: print(f\"AI: {fallback_converse}\")\n",
        "                    conversation_history.append({'role': 'model', 'parts': [fallback_converse]})\n",
        "\n",
        "        except Exception as e_loop:\n",
        "            if verbose:\n",
        "                print(f\"[System Error] An error occurred in DSPy main negotiation loop: {e_loop}\")\n",
        "                import traceback; traceback.print_exc()\n",
        "            conversation_history.append({'role': 'model', 'parts': [\"[System Error during AI turn. Please try again or type 'quit' to exit.\"]})\n",
        "\n",
        "    if verbose: print(\"[System] Negotiation loop exited unexpectedly.\")\n",
        "    return None\n",
        "\n",
        "def run_learning_session_dspy(resource_paths: Optional[List[str]] = None, verbose: bool = True):\n",
        "    print(\"--- Welcome to the AI Learning Assistant (DSPy Version)! ---\")\n",
        "    persona_gen = PersonaPromptGenerator()\n",
        "\n",
        "\n",
        "\n",
        "    if dspy.settings.lm is None:\n",
        "        print(\"CRITICAL Error: DSPy LM (dspy.settings.lm) is not configured. Session cannot start.\")\n",
        "        return\n",
        "    if isinstance(dspy.settings.lm, dspy.utils.DummyLM) and 'API_KEY' in globals() and API_KEY:\n",
        "        print(\"WARNING: API_KEY seems available, but DSPy is using a DummyLM. Real LLM calls are disabled.\")\n",
        "    elif verbose:\n",
        "        print(f\"INFO: Using DSPy LM: {type(dspy.settings.lm).__name__}, Model: {getattr(dspy.settings.lm, 'model', 'N/A')}\")\n",
        "\n",
        "    # 2. SYLLABUS NEGOTIATION PHASE\n",
        "    #    This phase interacts with the user to create and finalize a syllabus.\n",
        "    #    It uses several DSPy modules internally: ConversationManager, SyllabusGeneratorRouter, etc.\n",
        "    if verbose: print(\"\\n--- Phase 1: Planning Your Syllabus ---\")\n",
        "    negotiation_result = None\n",
        "    try:\n",
        "        # `negotiate_syllabus_chat_dspy` handles the multi-turn conversation for syllabus planning.\n",
        "        # It returns the final syllabus XML and the complete conversation history of this phase.\n",
        "        negotiation_result = negotiate_syllabus_chat_dspy(resource_paths=resource_paths, verbose=verbose)\n",
        "    except NameError as ne: # Catch if core functions/classes are not defined\n",
        "        print(f\"ERROR: A required function or class for negotiation (e.g., 'negotiate_syllabus_chat_dspy') is undefined: {ne}\")\n",
        "        return\n",
        "    except Exception as e_neg: # Catch any other unexpected errors during negotiation\n",
        "        print(f\"ERROR: An unexpected error occurred during syllabus negotiation phase: {e_neg}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    if negotiation_result is None:\n",
        "        if verbose: print(\"\\n--- Syllabus planning did not complete. Exiting session. ---\")\n",
        "        return\n",
        "\n",
        "    final_syllabus_xml, final_negotiation_history = negotiation_result\n",
        "    # `final_syllabus_xml` is the string like \"<syllabus>...</syllabus>\"\n",
        "    # `final_negotiation_history` is the List[Dict] containing all turns from the negotiation.\n",
        "    if verbose: print(\"\\n--- Phase 1 Complete: Syllabus Finalized! ---\")\n",
        "\n",
        "\n",
        "    # 3. PERSONA PROMPT GENERATION FOR EXPLAINER AI\n",
        "    #    Uses the outcome of Phase 1 (history + final syllabus) to create instructions for the tutor AI.\n",
        "    if verbose: print(\"\\n--- Phase 2: Preparing Your Learning Tutor ---\")\n",
        "    explainer_system_prompt = None # Initialize to ensure it's always defined\n",
        "\n",
        "    try:\n",
        "        # Instantiate the PersonaPromptGenerator\n",
        "        # This module now uses dspy.Predict with PersonaPromptBodyPredictSignature\n",
        "        # and its forward method returns the prompt body string or None.\n",
        "        persona_gen = PersonaPromptGenerator() # Ensure this class is correctly defined\n",
        "\n",
        "        # Format the negotiation history for the PersonaPromptGenerator's input\n",
        "        history_for_persona_str = format_history_for_dspy(final_negotiation_history)\n",
        "\n",
        "        if verbose: print(\"[System] Generating persona prompt body for the AI Tutor...\")\n",
        "\n",
        "        # Call the PersonaPromptGenerator's forward method.\n",
        "        explainer_prompt_body_text = persona_gen.forward(\n",
        "            conversation_history_str=history_for_persona_str,\n",
        "            #final_syllabus_xml_str=final_syllabus_xml  - Not needed\n",
        "        )\n",
        "\n",
        "        if explainer_prompt_body_text is None: # Check if persona generation failed\n",
        "            print(\"[System Error] Failed to generate explainer prompt body from PersonaPromptGenerator. Using a generic fallback.\")\n",
        "            # Define a more complete fallback persona prompt body\n",
        "            explainer_prompt_body_text = (\n",
        "                \"You are a helpful and patient AI Tutor named 'GuideBot'.\\n\"\n",
        "                \"Your mission is to clearly explain the topics in the provided syllabus.\\n\\n\"\n",
        "                \"Core Principles:\\n\"\n",
        "                \"*   Be encouraging and supportive.\\n\"\n",
        "                \"*   Break down complex topics into simple, understandable parts.\\n\\n\"\n",
        "                \"Teaching Approach:\\n\"\n",
        "                \"*   Use clear language and examples.\\n\"\n",
        "                \"*   Check for understanding frequently.\\n\\n\"\n",
        "                \"Your overall goal is to make learning an engaging and effective experience.\"\n",
        "            )\n",
        "\n",
        "        # Construct the full system prompt for the explainer AI by appending the syllabus\n",
        "        # Ensure explainer_prompt_body_text is a string before stripping\n",
        "        explainer_system_prompt = f\"{str(explainer_prompt_body_text).strip()}\\n\\nHere is the syllabus we will follow:\\n{final_syllabus_xml}\"\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\n[System] Generated Full Explainer System Prompt (first 300 chars):\")\n",
        "            print(f\"{explainer_system_prompt[:300]}...\")\n",
        "            # To see the full prompt for debugging:\n",
        "            # print(f\"\\nDEBUG: FULL EXPLAINER SYSTEM PROMPT:\\n{explainer_system_prompt}\\n\")\n",
        "\n",
        "    except NameError as ne: # Catch if PersonaPromptGenerator or its dependencies are undefined\n",
        "        print(f\"ERROR: PersonaPromptGenerator or related class/function is undefined: {ne}\")\n",
        "        print(\"[System] Using a very generic fallback explainer prompt due to setup error.\")\n",
        "        explainer_system_prompt = f\"You are a helpful AI Tutor. Explain the topics in the following syllabus clearly.\\n\\nHere is the syllabus we will follow:\\n{final_syllabus_xml}\" # Basic fallback\n",
        "    except Exception as e_persona: # Catch other unexpected errors in this phase\n",
        "        if verbose: print(f\"[System Error] An unexpected error occurred during explainer prompt generation: {e_persona}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "        print(\"[System] Using a very generic fallback explainer prompt due to error.\")\n",
        "        explainer_system_prompt = f\"You are a helpful AI Tutor. Explain the topics in the following syllabus clearly.\\n\\nHere is the syllabus we will follow:\\n{final_syllabus_xml}\" # Basic fallback\n",
        "\n",
        "\n",
        "    # 4. EXPLAINER/LEARNING PHASE\n",
        "    #    The user interacts with the AI Tutor, which is guided by the `explainer_system_prompt`.\n",
        "    if verbose: print(\"\\n--- Phase 3: Let's Start Learning! ---\")\n",
        "\n",
        "    if explainer_system_prompt is None: # Should have been set by fallbacks if errors occurred\n",
        "        print(\"CRITICAL ERROR: Explainer system prompt was not generated or set. Cannot start learning phase.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # The explainer uses a GenericInteractionSignature with dspy.Predict.\n",
        "        # This signature takes the full system_instructions (our explainer_system_prompt) as input.\n",
        "        explainer_predictor = dspy.Predict(GenericInteractionSignature) # Ensure GenericInteractionSignature is defined\n",
        "    except NameError as ne:\n",
        "        print(f\"ERROR: GenericInteractionSignature or dspy.Predict is undefined for explainer: {ne}\")\n",
        "        return\n",
        "    except Exception as e_explainer_init:\n",
        "        if verbose: print(f\"[System Error] Failed to instantiate explainer predictor: {e_explainer_init}\")\n",
        "        return\n",
        "\n",
        "    explainer_history_list: List[Dict[str, Any]] = [] # History for this learning phase\n",
        "    try:\n",
        "        # Tutor's introduction\n",
        "        if verbose: print(\"[System] AI Tutor is preparing its introduction...\")\n",
        "        # Prompting the tutor to introduce itself based on its newly defined persona\n",
        "        initial_explainer_query = \"Based on your persona (defined in system_instructions) and the syllabus provided, please introduce yourself to the user. Briefly state what you'll be helping them with and adopt a welcoming tone consistent with your persona.\"\n",
        "\n",
        "        intro_prediction_obj = explainer_predictor(\n",
        "            system_instructions=explainer_system_prompt,\n",
        "            history=\"None\", # No prior explainer history for this first turn\n",
        "            user_query=initial_explainer_query # This query prompts the tutor for its intro\n",
        "        )\n",
        "        initial_tutor_response = intro_prediction_obj.response.strip() # .response for dspy.Predict\n",
        "        if verbose: print(f\"AI Tutor: {initial_tutor_response}\")\n",
        "        if initial_tutor_response: explainer_history_list.append({'role': 'model', 'parts': [initial_tutor_response]})\n",
        "    except Exception as e_intro: # Catch errors during the tutor's introduction\n",
        "        if verbose: print(f\"[System Error] AI Tutor intro generation failed: {e_intro}\")\n",
        "        fallback_intro = \"Hello! I'm ready to help you learn based on our plan. Which part of the syllabus would you like to start with?\"\n",
        "        explainer_history_list.append({'role': 'model', 'parts': [fallback_intro]})\n",
        "        if verbose: print(f\"AI Tutor (fallback): {fallback_intro}\")\n",
        "\n",
        "    # Main learning loop with the AI Tutor\n",
        "    while True:\n",
        "        try:\n",
        "            user_explainer_input = input(\"You (Learning): \").strip()\n",
        "        except EOFError:\n",
        "            if verbose: print(\"\\nAI Tutor: Session ended by user (EOF).\")\n",
        "            break\n",
        "        if user_explainer_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            if verbose: print(\"AI Tutor: Okay, ending the learning session. Goodbye!\")\n",
        "            break\n",
        "        if not user_explainer_input: continue\n",
        "\n",
        "        explainer_history_list.append({'role': 'user', 'parts': [user_explainer_input]})\n",
        "        history_str_for_explainer = format_history_for_dspy(explainer_history_list)\n",
        "\n",
        "        try:\n",
        "            if verbose: print(f\"[System Debug] Calling Explainer predictor...\")\n",
        "            tutor_prediction_obj = explainer_predictor(\n",
        "                system_instructions=explainer_system_prompt, # Full instructions for the tutor\n",
        "                history=history_str_for_explainer,           # Current learning phase history\n",
        "                user_query=user_explainer_input              # User's latest query\n",
        "            )\n",
        "            tutor_response_text = tutor_prediction_obj.response.strip() # .response for dspy.Predict\n",
        "\n",
        "            if tutor_response_text and not tutor_response_text.upper().startswith((\"[ERROR\", \"[BLOCKED\", \"[WARN:\")):\n",
        "                if verbose: print(f\"AI Tutor: {tutor_response_text}\")\n",
        "                explainer_history_list.append({'role': 'model', 'parts': [tutor_response_text]})\n",
        "            else: # Handle empty or error-like responses\n",
        "                err_msg_explainer = \"I seem to be having a little trouble formulating a response for that. Could you perhaps rephrase your question, or would you like to try a different part of the syllabus?\"\n",
        "                if verbose: print(f\"AI Tutor: {err_msg_explainer} (Original LLM response snippet: {tutor_response_text[:100]}...)\")\n",
        "                explainer_history_list.append({'role': 'model', 'parts': [err_msg_explainer]})\n",
        "        except Exception as e_explain_call: # Catch errors from the LLM call itself\n",
        "            if verbose: print(f\"[System Error] Explainer DSPy call failed: {e_explain_call}\")\n",
        "            explainer_history_list.append({'role': 'model', 'parts': [\"[System Error during explanation. Please try again or type 'quit'.\"]})\n",
        "\n",
        "    if verbose: print(\"\\n--- Learning Session Complete ---\")\n",
        "\n",
        "\n",
        "# --- Example Execution Block (ensure it's at the end) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Main Script Execution Starting ---\")\n",
        "    # ... (Setup checks and resource path definitions as in the previous version) ...\n",
        "    # Ensure API_KEY, limiter, and DSPy LM config have happened from the top.\n",
        "    if 'API_KEY' not in globals() or not API_KEY:\n",
        "        print(\"CRITICAL: API_KEY not loaded.\")\n",
        "    if dspy.settings.lm is None:\n",
        "        print(\"CRITICAL: dspy.settings.lm is None. DSPy is not configured. Exiting.\")\n",
        "        sys.exit(1) # Critical if LM isn't set\n",
        "    else:\n",
        "        print(f\"Current DSPy LM: {type(dspy.settings.lm).__name__}, Model: {getattr(dspy.settings.lm, 'model', 'N/A')}\")\n",
        "        # Example: Test without resources first\n",
        "        # You'll need to ensure `DynamicResourceSummarizerModule` and its dependencies are defined\n",
        "        # for the resource processing part within `negotiate_syllabus_chat_dspy`.\n",
        "        run_learning_session_dspy(resource_paths = [], verbose=True) # Pass empty list for no resources\n",
        "\n",
        "        #    run_learning_session_dspy(resource_paths=created_files, verbose=True)\n",
        "\n",
        "    print(\"\\n--- Main Script Execution Finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2xptltiUR0j"
      },
      "source": [
        "### Testing/Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwZFOiLvejwI"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_raw_texts_dict[\"BartoSutton (1).txt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jfokw3hJyUI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKXdWR9Ek72n"
      },
      "outputs": [],
      "source": [
        "summary_router = DynamicResourceSummarizerModule()\n",
        "conversation_history_with_rawresource = [\n",
        "    {'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        "    {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need. iam very week at the math part and have also attached the paper']},\n",
        "    {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        "    {'role': 'user', 'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        "    {'role': 'model', 'parts': ['Ok generating syallabus']}\n",
        "]\n",
        "conversation_history_str_for_test = format_history_for_dspy(conversation_history_with_rawresource)\n",
        "resource_str = extract_text_from_txt_file(created_files[1])\n",
        "resource_summaries = summary_router.forward(\n",
        "                conversation_history_str= conversation_history_with_rawresource,\n",
        "                resource_content=resource_str,\n",
        "                resource_identifier=created_files[1],\n",
        "\n",
        "\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rho7y3ec2ZG"
      },
      "outputs": [],
      "source": [
        "# --- Test Script for SyllabusGeneratorRouter ---\n",
        "syllabus_l = []\n",
        "if 'dspy' not in globals() or dspy.settings.lm is None:\n",
        "    print(\"CRITICAL ERROR: DSPy LM not configured. Test cannot proceed.\")\n",
        "else:\n",
        "    print(f\"INFO: DSPy LM configured with: {type(dspy.settings.lm).__name__}, Model: {dspy.settings.lm.model}\")\n",
        "\n",
        "    # 1. Sample conversation history (same as your previous test)\n",
        "    sample_conversation_history_list = [\n",
        "        {'role': 'user', 'parts': [\"Hi, I want to create a learning plan to understand the mathematical details of the 'Attention Is All You Need' paper.\"]},\n",
        "        {'role': 'model', 'parts': [\"Okay! Your experience level with ML and math?\"]},\n",
        "        {'role': 'user', 'parts': [\"Intermediate ML, weak on specific math details.\"]},\n",
        "        {'role': 'model', 'parts': [\"Understood. Focus on math of attention.\"]}\n",
        "    ]\n",
        "    conversation_history_str_for_test = format_history_for_dspy(sample_conversation_history_list) # Ensure this helper is defined\n",
        "\n",
        "    # 2. Instantiate the router\n",
        "    try:\n",
        "        syllabus_router = SyllabusGeneratorRouter()\n",
        "        print(\"SyllabusGeneratorRouter instantiated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error instantiating SyllabusGeneratorRouter: {e}\")\n",
        "        syllabus_router = None\n",
        "\n",
        "    if syllabus_router:\n",
        "        # common_task_desc = \"Generate an initial syllabus for understanding 'Attention Is All You Need' math, for a learner intermediate in ML but weak in specific math.\"\n",
        "\n",
        "        # --- Test Case 1: NO RESOURCES ---\n",
        "        print(\"\\n--- Router Test Case 1: No Resources ---\")\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls before: {limiter.total_calls_made_through_limiter}\")\n",
        "        try:\n",
        "            syllabus_no_res = syllabus_router.forward(\n",
        "                conversation_str=conversation_history_str_for_test,\n",
        "                #task_description=common_task_desc + \" (Rely only on conversation).\",\n",
        "                resource_type=\"NONE\",\n",
        "                resource_content=None, # Explicitly None\n",
        "                # existing_syllabus_xml=None\n",
        "            )\n",
        "            syllabus_l.append(syllabus_no_res)\n",
        "            print(\"\\n--- Generated Syllabus (No Resources via Router) ---\")\n",
        "            print(syllabus_no_res)\n",
        "        except Exception as e_test: print(f\"ERROR in test case: {e_test}\")\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls after: {limiter.total_calls_made_through_limiter}\")\n",
        "\n",
        "\n",
        "        # --- Test Case 2: WITH SUMMARIZED RESOURCE INFO ---\n",
        "        extracted_data_dict = {}\n",
        "        extracted_data_dict = {path: extract_text_from_txt_file(path) for path in created_files}\n",
        "        chars_length = 0\n",
        "        extracted_basedata_dict ={}\n",
        "        for full_path,chars in extracted_data_dict.items():\n",
        "          chars_length += len(chars)\n",
        "          base_filename = os.path.basename(full_path)\n",
        "          extracted_basedata_dict[base_filename] = chars\n",
        "        extracted_summary_dict = [summary_router.forward(resource_content = value,resource_identifier = key,conversation_history_str=conversation_history_str_for_test) for key,value in extracted_basedata_dict.items() ]\n",
        "        aggregated_summaries_dict = {}\n",
        "        for summary_dict in extracted_summary_dict:\n",
        "            if summary_dict and \"resource_identifier\" in summary_dict:\n",
        "                aggregated_summaries_dict[summary_dict[\"resource_identifier\"]] = summary_dict\n",
        "            else:\n",
        "                print(f\"Warning: Skipping invalid summary object: {summary_dict}\")\n",
        "\n",
        "\n",
        "        if aggregated_summaries_dict:\n",
        "            resource_summaries_json_string_for_signature = json.dumps(aggregated_summaries_dict, indent=2)\n",
        "\n",
        "        else:\n",
        "            resource_summaries_json_string_for_signature = \"{}\"\n",
        "            print(\"Warning: No valid summaries to aggregate for SyllabusWithSummariesSignature.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls before: {limiter.total_calls_made_through_limiter}\")\n",
        "        try:\n",
        "            syllabus_with_summaries = syllabus_router.forward(\n",
        "                conversation_str=conversation_history_str_for_test,\n",
        "                #task_description=common_task_desc + \" (Use provided resource summaries).\",\n",
        "                resource_type=\"SUMMARIES\",\n",
        "                resource_content=aggregated_summaries_dict,\n",
        "\n",
        "            )\n",
        "            syllabus_l.append(syllabus_with_summaries)\n",
        "            print(\"\\n--- Generated Syllabus (With Summaries via Router) ---\")\n",
        "            print(syllabus_with_summaries)\n",
        "        except Exception as e_test: print(f\"ERROR in test case: {e_test}\")\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls after: {limiter.total_calls_made_through_limiter}\")\n",
        "\n",
        "\n",
        "        # --- Test Case 3: WITH RAW TEXT (LIGHT) ---\n",
        "        print(\"\\n\\n--- Router Test Case 3: With Light Raw Text ---\")\n",
        "        # Prepare some short raw text (e.g., from your created_files or dummy)\n",
        "        raw_text_input = \"\"\n",
        "        if 'created_files' in globals() and created_files and 'extract_text_from_txt_file' in globals():\n",
        "            # Let's use a short snippet from the first file if available\n",
        "            content_sample = extract_text_from_txt_file(created_files[1])\n",
        "            raw_text_input = {}\n",
        "            if content_sample:\n",
        "                raw_text_input[os.path.basename(created_files[1])] = content_sample\n",
        "\n",
        "            else:\n",
        "                raw_text_input = \"Attention(Q, K, V) = softmax( (QK^T) / sqrt(d_k) ) * V. This is scaled dot-product attention.\" # Fallback dummy\n",
        "        else: # Fallback if created_files or extractor not available\n",
        "            raw_text_input = \"Attention(Q, K, V) = softmax( (QK^T) / sqrt(d_k) ) * V. This is scaled dot-product attention. Q, K, V are matrices.\"\n",
        "        # print(f\"Using raw text for test: {raw_text_input[:200]}...\")\n",
        "\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls before: {limiter.total_calls_made_through_limiter}\")\n",
        "        try:\n",
        "            syllabus_with_raw = syllabus_router.forward(\n",
        "                conversation_str=conversation_history_str_for_test,\n",
        "                #task_description=common_task_desc + \" (Directly use these raw text snippets).\",\n",
        "                resource_type=\"RAW_TEXT\",\n",
        "                resource_content=raw_text_input,\n",
        "\n",
        "            )\n",
        "            syllabus_l.append(syllabus_with_raw)\n",
        "            print(\"\\n--- Generated Syllabus (With Raw Text via Router) ---\")\n",
        "            print(syllabus_with_raw)\n",
        "        except Exception as e_test: print(f\"ERROR in test case: {e_test}\")\n",
        "        if 'limiter' in globals(): print(f\"Limiter calls after: {limiter.total_calls_made_through_limiter}\")\n",
        "\n",
        "    else:\n",
        "        print(\"SyllabusGeneratorRouter not instantiated. Cannot run tests.\")\n",
        "\n",
        "print(\"\\n--- Test for SyllabusGeneratorRouter Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZhBslPeUtwy"
      },
      "outputs": [],
      "source": [
        "# print(syllabus_l[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykZxzSyEJ7xv"
      },
      "outputs": [],
      "source": [
        "extracted_data_dict = {}\n",
        "extracted_data_dict = {path: extract_text_from_txt_file(path) for path in created_files}\n",
        "chars_length = 0\n",
        "extracted_basedata_dict ={}\n",
        "for full_path,chars in extracted_data_dict.items():\n",
        "  chars_length += len(chars)\n",
        "  base_filename = os.path.basename(full_path)\n",
        "  extracted_basedata_dict[base_filename] = chars\n",
        "extracted_summary_dict = [summary_router.forward(resource_content = value,resource_identifier = key,conversation_history_str=conversation_history_str_for_test) for key,value in extracted_basedata_dict.items() ]\n",
        "aggregated_summaries_dict = {}\n",
        "for summary_dict in extracted_summary_dict:\n",
        "    if summary_dict and \"resource_identifier\" in summary_dict:\n",
        "        aggregated_summaries_dict[summary_dict[\"resource_identifier\"]] = summary_dict\n",
        "    else:\n",
        "        print(f\"Warning: Skipping invalid summary object: {summary_dict}\")\n",
        "\n",
        "\n",
        "if aggregated_summaries_dict:\n",
        "    resource_summaries_json_string_for_signature = json.dumps(aggregated_summaries_dict, indent=2)\n",
        "\n",
        "else:\n",
        "    resource_summaries_json_string_for_signature = \"{}\"\n",
        "    print(\"Warning: No valid summaries to aggregate for SyllabusWithSummariesSignature.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDCHbHTAe5-n"
      },
      "outputs": [],
      "source": [
        "conversation_history = [\n",
        "    {'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        "    {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need the Transformers paper. iam very week at the math part']},\n",
        "    {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        "    {'role': 'user', 'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        "    {'role': 'model', 'parts': ['Ok generating syallabus']}\n",
        "]\n",
        "\n",
        "conversation_history_witt_rawresource = [\n",
        "    {'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        "    {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need. iam very week at the math part and have also attached the paper']},\n",
        "    {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        "    {'role': 'user', 'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        "    {'role': 'model', 'parts': ['Ok generating syallabus']}\n",
        "]\n",
        "conversation_history_witt_heavyresource = [\n",
        "    {'role': 'model', 'parts': ['What Hard thing You want to learn Today']},\n",
        "    {'role': 'user', 'parts': ['I want to understand the math part of attention is all you need and the reinforcement learning math. iam very week at the math part and have also attached the resources']},\n",
        "    {'role': 'model', 'parts': ['Are you beginner or intermediate in Math and ML']},\n",
        "    {'role': 'user', 'parts': ['Iam ok with Basic math and im intermediate in ML/Transformers ']},\n",
        "    {'role': 'model', 'parts': ['Ok generating syallabus']}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "63MIPhcFkCaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}